{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# === EDIT THIS: your MIMIC-IV folder ===\n",
    "MIMIC_DIR = \"/kaggle/input/pfe-data\"  # e.g., \"/content/drive/MyDrive/mimic-iv-2.2\"\n",
    "\n",
    "# Core tables\n",
    "ADMISSIONS_CSV = '/kaggle/input/pfe-data/admissions.csv'\n",
    "PATIENTS_CSV   = '/kaggle/input/pfe-data/patients.csv'\n",
    "ICUSTAYS_CSV   = '/kaggle/input/pfe-data/icustays.csv'\n",
    "\n",
    "# Events + dictionaries\n",
    "CHARTEVENTS_CSV = '/kaggle/input/pfe-data/chartevents.csv'\n",
    "D_ITEMS_CSV     = '/kaggle/input/ditems/d_items.csv'\n",
    "\n",
    "LABEVENTS_CSV   = '/kaggle/input/pfe-data/labevents.csv'\n",
    "D_LABITEMS_CSV  = '/kaggle/input/hhhhhh/d_labitems.csv'\n",
    "\n",
    "import pandas as pd, numpy as np, gc, math, os, re\n",
    "from collections import defaultdict\n",
    "pd.set_option(\"display.max_colwidth\", 120)\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read minimal columns (faster)\n",
    "adm_cols = [\"subject_id\",\"hadm_id\",\"admittime\",\"dischtime\",\"deathtime\",\"hospital_expire_flag\"]\n",
    "icu_cols = [\"subject_id\",\"hadm_id\",\"stay_id\",\"first_careunit\",\"intime\",\"outtime\",\"los\"]\n",
    "pat_cols = [\"subject_id\",\"anchor_age\",\"anchor_year\"]\n",
    "\n",
    "adm = pd.read_csv(ADMISSIONS_CSV, usecols=adm_cols, parse_dates=[\"admittime\",\"dischtime\",\"deathtime\"],\n",
    "                  dtype={\"subject_id\":\"int32\",\"hadm_id\":\"int32\",\"hospital_expire_flag\":\"int8\"})\n",
    "icu = pd.read_csv(ICUSTAYS_CSV,  usecols=icu_cols, parse_dates=[\"intime\",\"outtime\"],\n",
    "                  dtype={\"subject_id\":\"int32\",\"hadm_id\":\"int32\",\"stay_id\":\"int32\",\"first_careunit\":\"category\"})\n",
    "pat = pd.read_csv(PATIENTS_CSV,  usecols=pat_cols, dtype={\"subject_id\":\"int32\",\"anchor_year\":\"int16\",\"anchor_age\":\"float32\"})\n",
    "\n",
    "# Age at admission (standard for MIMIC-IV)\n",
    "adm_year = adm[\"admittime\"].dt.year.astype(\"Int16\")\n",
    "tmp = adm.merge(pat, on=\"subject_id\", how=\"left\")\n",
    "tmp[\"age_at_admit\"] = tmp[\"anchor_age\"] + (adm_year.astype(\"float32\") - tmp[\"anchor_year\"].astype(\"float32\"))\n",
    "tmp[\"age_at_admit\"] = tmp[\"age_at_admit\"].clip(lower=0, upper=120)\n",
    "\n",
    "# Merge ICU with admission info\n",
    "cohort = icu.merge(\n",
    "    tmp[[\"subject_id\",\"hadm_id\",\"admittime\",\"dischtime\",\"deathtime\",\"hospital_expire_flag\",\"age_at_admit\"]],\n",
    "    on=[\"subject_id\",\"hadm_id\"], how=\"left\"\n",
    ")\n",
    "\n",
    "# Inclusion criteria\n",
    "cohort[\"icu_los_hours\"] = (cohort[\"outtime\"] - cohort[\"intime\"]).dt.total_seconds() / 3600.0\n",
    "cohort = cohort[cohort[\"age_at_admit\"] >= 18].copy()\n",
    "cohort = cohort[cohort[\"icu_los_hours\"] >= 24].copy()\n",
    "cohort = cohort.sort_values([\"subject_id\",\"hadm_id\",\"intime\"]).drop_duplicates([\"subject_id\",\"hadm_id\"], keep=\"first\")\n",
    "\n",
    "# Label: in-hospital death AFTER 24h from ICU intime\n",
    "delta_death = (cohort[\"deathtime\"] - cohort[\"intime\"]).dt.total_seconds() / 3600.0\n",
    "early_death_mask = (cohort[\"hospital_expire_flag\"] == 1) & (cohort[\"deathtime\"].notna()) & (delta_death <= 24)\n",
    "cohort = cohort[~early_death_mask].copy()  # drop deaths in first 24h\n",
    "\n",
    "delta_death = (cohort[\"deathtime\"] - cohort[\"intime\"]).dt.total_seconds() / 3600.0\n",
    "cohort[\"label\"] = ((cohort[\"hospital_expire_flag\"] == 1) & (cohort[\"deathtime\"].notna()) & (delta_death > 24)).astype(int)\n",
    "\n",
    "# Metadata for FL\n",
    "cohort[\"careunit\"] = cohort[\"first_careunit\"].astype(str)\n",
    "cohort[\"adm_year\"] = cohort[\"admittime\"].dt.year.astype(\"Int16\")\n",
    "\n",
    "print(\"Cohort size:\", len(cohort))\n",
    "print(\"Pos rate:\", cohort[\"label\"].mean().round(4))\n",
    "print(cohort[[\"careunit\"]].value_counts().head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort = cohort.reset_index(drop=True)\n",
    "stay_to_start = pd.Series(cohort[\"intime\"].values, index=cohort[\"stay_id\"].values)\n",
    "stay_to_end   = stay_to_start + pd.Timedelta(hours=24)\n",
    "\n",
    "stay_ids = set(cohort[\"stay_id\"].tolist())\n",
    "hadm_to_intime = cohort.set_index(\"hadm_id\")[\"intime\"]  # for labevents windowing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Vital signs from icu/d_items ---\n",
    "d_items = pd.read_csv(D_ITEMS_CSV, usecols=[\"itemid\",\"label\",\"category\"], dtype={\"itemid\":\"int32\",\"label\":\"string\",\"category\":\"string\"})\n",
    "d_items[\"label_lc\"] = d_items[\"label\"].str.lower()\n",
    "\n",
    "def find_itemids(patterns, in_category=None):\n",
    "    m = pd.Series(False, index=d_items.index)\n",
    "    for pat in patterns:\n",
    "        m |= d_items[\"label_lc\"].str.contains(pat, regex=True, na=False)\n",
    "    if in_category is not None:\n",
    "        m &= d_items[\"category\"].str.contains(in_category, case=False, na=False)\n",
    "    return set(d_items.loc[m, \"itemid\"].astype(int).tolist())\n",
    "\n",
    "vital_map = {\n",
    "    # var_name : regex patterns matched on d_items.label\n",
    "    \"hr\":   find_itemids([r\"\\bheart rate\\b\"], in_category=\"vital\"),            # Heart Rate\n",
    "    \"rr\":   find_itemids([r\"\\brespiratory rate\\b\"], in_category=\"vital\"),      # Respiratory Rate\n",
    "    \"spo2\": find_itemids([r\"\\bspo2\\b\", r\"oxygen saturation\", r\"pulse ox\"], in_category=\"vital\"),\n",
    "    \"temp\": find_itemids([r\"\\btemperature\\b\"], in_category=\"vital\"),\n",
    "    \"map\":  find_itemids([r\"\\bmean arterial pressure\\b\", r\"\\bmap\\b\", r\"non invasive blood pressure mean\", r\"arterial blood pressure mean\"]),\n",
    "    \"sbp\":  find_itemids([r\"\\bsystolic\\b\"], in_category=\"vital\"),\n",
    "    \"dbp\":  find_itemids([r\"\\bdiastolic\\b\"], in_category=\"vital\"),\n",
    "    \"gcs\":  find_itemids([r\"\\bglasgow coma scale total\\b\", r\"\\bgcs total\\b\"]),\n",
    "}\n",
    "# Remove empties\n",
    "vital_map = {k:v for k,v in vital_map.items() if len(v)>0}\n",
    "sum_ids_vital = set().union(*vital_map.values())\n",
    "print({k: len(v) for k,v in vital_map.items()})\n",
    "\n",
    "# --- Lab tests from hosp/d_labitems ---\n",
    "d_lab = pd.read_csv(D_LABITEMS_CSV, usecols=[\"itemid\",\"label\"], dtype={\"itemid\":\"int32\",\"label\":\"string\"})\n",
    "d_lab[\"label_lc\"] = d_lab[\"label\"].str.lower()\n",
    "\n",
    "def find_lab_itemids(patterns):\n",
    "    m = pd.Series(False, index=d_lab.index)\n",
    "    for pat in patterns:\n",
    "        m |= d_lab[\"label_lc\"].str.contains(pat, regex=True, na=False)\n",
    "    return set(d_lab.loc[m, \"itemid\"].astype(int).tolist())\n",
    "\n",
    "lab_map = {\n",
    "    \"wbc\":        find_lab_itemids([r\"\\bwbc\\b\", r\"white blood cell\"]),\n",
    "    \"hgb\":        find_lab_itemids([r\"\\bhemoglobin\\b\"]),\n",
    "    \"platelets\":  find_lab_itemids([r\"\\bplatelet\"]),\n",
    "    \"lactate\":    find_lab_itemids([r\"\\blactate\\b\"]),\n",
    "    \"sodium\":     find_lab_itemids([r\"\\bsodium\\b\"]),\n",
    "    \"potassium\":  find_lab_itemids([r\"\\bpotassium\\b\"]),\n",
    "    \"chloride\":   find_lab_itemids([r\"\\bchloride\\b\"]),\n",
    "    \"bicarb\":     find_lab_itemids([r\"\\bbicarbonate\\b\", r\"\\bHCO3\\b\"]),\n",
    "    \"creatinine\": find_lab_itemids([r\"\\bcreatinine\\b\"]),\n",
    "    \"bun\":        find_lab_itemids([r\"\\burea\\b\", r\"\\bbun\\b\"]),\n",
    "    \"glucose\":    find_lab_itemids([r\"\\bglucose\\b\"]),\n",
    "    \"bilirubin\":  find_lab_itemids([r\"\\bbilirubin\\b\"]),\n",
    "    \"inr\":        find_lab_itemids([r\"\\binr\\b\"]),\n",
    "}\n",
    "lab_map = {k:v for k,v in lab_map.items() if len(v)>0}\n",
    "sum_ids_lab = set().union(*lab_map.values())\n",
    "print({k: len(v) for k,v in lab_map.items()})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Online aggregator per (stay_id, var)\n",
    "class OnlineAgg:\n",
    "    __slots__ = (\"n\",\"sum\",\"sumsq\",\"min\",\"max\",\"last_t\",\"last_v\")\n",
    "    def __init__(self):\n",
    "        self.n=0; self.sum=0.0; self.sumsq=0.0\n",
    "        self.min=float(\"inf\"); self.max=float(\"-inf\")\n",
    "        self.last_t=pd.Timestamp.min; self.last_v=np.nan\n",
    "    def update(self, t, v):\n",
    "        if pd.isna(v): return\n",
    "        self.n += 1\n",
    "        self.sum += v\n",
    "        self.sumsq += v*v\n",
    "        if v < self.min: self.min = v\n",
    "        if v > self.max: self.max = v\n",
    "        if t > self.last_t: \n",
    "            self.last_t = t\n",
    "            self.last_v = v\n",
    "    def to_dict(self):\n",
    "        mean = self.sum/self.n if self.n>0 else np.nan\n",
    "        var = (self.sumsq/self.n - mean*mean) if self.n>1 else np.nan\n",
    "        std = math.sqrt(var) if var==var and var>=0 else np.nan\n",
    "        return {\"count\": self.n, \"mean\": mean, \"std\": std, \"min\": (self.min if self.n>0 else np.nan),\n",
    "                \"max\": (self.max if self.n>0 else np.nan), \"last\": self.last_v}\n",
    "\n",
    "# Build itemid -> varname map for fast lookup\n",
    "item_to_var = {}\n",
    "for var, ids in vital_map.items():\n",
    "    for iid in ids: item_to_var[iid] = var\n",
    "\n",
    "agg_vitals = defaultdict(OnlineAgg)\n",
    "\n",
    "usecols_ce = [\"stay_id\",\"itemid\",\"charttime\",\"valuenum\"]\n",
    "dtype_ce = {\"stay_id\":\"int32\",\"itemid\":\"int32\",\"valuenum\":\"float32\"}\n",
    "\n",
    "chunksize = 10_000_000  # tune for your RAM\n",
    "keep_ids = set(stay_ids)\n",
    "\n",
    "print(\"Streaming chartevents ...\")\n",
    "for chunk in pd.read_csv(CHARTEVENTS_CSV, usecols=usecols_ce, dtype=dtype_ce,\n",
    "                         parse_dates=[\"charttime\"], chunksize=chunksize):\n",
    "    # Filter to our stays & itemids of interest\n",
    "    chunk = chunk[chunk[\"stay_id\"].isin(keep_ids)]\n",
    "    chunk = chunk[chunk[\"itemid\"].isin(sum_ids_vital)]\n",
    "    if chunk.empty:\n",
    "        continue\n",
    "\n",
    "    # Map 24h window per stay\n",
    "    chunk[\"start\"] = chunk[\"stay_id\"].map(stay_to_start)\n",
    "    chunk[\"end\"]   = chunk[\"stay_id\"].map(stay_to_end)\n",
    "    m = (chunk[\"charttime\"] >= chunk[\"start\"]) & (chunk[\"charttime\"] <= chunk[\"end\"])\n",
    "    chunk = chunk[m]\n",
    "    if chunk.empty:\n",
    "        continue\n",
    "\n",
    "    # Update online aggregates\n",
    "    for iid, sid, t, v in zip(chunk[\"itemid\"].values, chunk[\"stay_id\"].values, chunk[\"charttime\"].values, chunk[\"valuenum\"].values):\n",
    "        var = item_to_var.get(int(iid))\n",
    "        if var is None: \n",
    "            continue\n",
    "        agg_vitals[(int(sid), var)].update(pd.Timestamp(t), float(v))\n",
    "\n",
    "    del chunk\n",
    "    gc.collect()\n",
    "\n",
    "# Convert aggregates to a wide feature table\n",
    "rows = []\n",
    "for (sid, var), A in agg_vitals.items():\n",
    "    d = A.to_dict()\n",
    "    row = {\"stay_id\": sid}\n",
    "    for k,v in d.items():\n",
    "        row[f\"{var}_{k}_0_24h\"] = v\n",
    "    rows.append(row)\n",
    "\n",
    "fe_vitals = pd.DataFrame(rows).groupby(\"stay_id\", as_index=False).first()  # one row per stay\n",
    "print(\"Vitals features:\", fe_vitals.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- ROBUST LABEVENTS STREAMING (handles NA hadm_id/itemid) ---\n",
    "from collections import defaultdict\n",
    "import math, gc\n",
    "\n",
    "# lab itemid -> varname (already built above as `labitem_to_var`)\n",
    "# hadm_to_intime (built earlier)\n",
    "# cohort, lab_map, sum_ids_lab (sets) already defined\n",
    "\n",
    "agg_labs = defaultdict(OnlineAgg)\n",
    "\n",
    "usecols_le = [\"hadm_id\",\"itemid\",\"charttime\",\"valuenum\"]\n",
    "# Only enforce dtype on valuenum to avoid NA casting errors for IDs\n",
    "dtype_le = {\"valuenum\":\"float32\"}\n",
    "\n",
    "keep_hadms = set(cohort[\"hadm_id\"].astype(int).tolist())\n",
    "sum_ids_lab_int = set(int(i) for i in sum_ids_lab)  # ensure ints\n",
    "\n",
    "chunksize = 1_000_000  # reduce if RAM is tight\n",
    "\n",
    "print(\"Streaming labevents (robust) ...\")\n",
    "for chunk in pd.read_csv(\n",
    "    LABEVENTS_CSV,\n",
    "    usecols=usecols_le,\n",
    "    dtype=dtype_le,                # don't force int dtypes here\n",
    "    parse_dates=[\"charttime\"],\n",
    "    chunksize=chunksize,\n",
    "    low_memory=True,\n",
    "):\n",
    "    # Coerce hadm_id/itemid to numeric with NA allowed\n",
    "    chunk[\"hadm_id\"] = pd.to_numeric(chunk[\"hadm_id\"], errors=\"coerce\")\n",
    "    chunk[\"itemid\"]  = pd.to_numeric(chunk[\"itemid\"], errors=\"coerce\")\n",
    "\n",
    "    # Drop rows missing the essentials\n",
    "    chunk = chunk.dropna(subset=[\"hadm_id\",\"itemid\",\"charttime\"])\n",
    "    if chunk.empty:\n",
    "        continue\n",
    "\n",
    "    # Now safe to cast to ints\n",
    "    chunk[\"hadm_id\"] = chunk[\"hadm_id\"].astype(\"int32\")\n",
    "    chunk[\"itemid\"]  = chunk[\"itemid\"].astype(\"int32\")\n",
    "\n",
    "    # Filter by our cohort's hadm_ids and selected lab itemids\n",
    "    chunk = chunk[chunk[\"hadm_id\"].isin(keep_hadms)]\n",
    "    if chunk.empty:\n",
    "        continue\n",
    "    chunk = chunk[chunk[\"itemid\"].isin(sum_ids_lab_int)]\n",
    "    if chunk.empty:\n",
    "        continue\n",
    "\n",
    "    # Map ICU intime for the hadm (first ICU stay we kept)\n",
    "    chunk[\"icuintime\"] = chunk[\"hadm_id\"].map(hadm_to_intime)\n",
    "\n",
    "    # Keep labs within 0â€“24h of ICU intime\n",
    "    end = chunk[\"icuintime\"] + pd.Timedelta(hours=24)\n",
    "    m = (chunk[\"charttime\"] >= chunk[\"icuintime\"]) & (chunk[\"charttime\"] <= end)\n",
    "    chunk = chunk[m]\n",
    "    if chunk.empty:\n",
    "        continue\n",
    "\n",
    "    # Map to stay_id via hadm_id (1:1 after we kept first ICU stay per hadm)\n",
    "    if \"hadm_to_stay\" not in globals():\n",
    "        hadm_to_stay = cohort.set_index(\"hadm_id\")[\"stay_id\"].to_dict()\n",
    "    chunk[\"stay_id\"] = chunk[\"hadm_id\"].map(hadm_to_stay)\n",
    "\n",
    "    # Drop any rows that still lack a stay_id (should be rare)\n",
    "    chunk = chunk.dropna(subset=[\"stay_id\"])\n",
    "    if chunk.empty:\n",
    "        continue\n",
    "\n",
    "    # Aggregate online\n",
    "    for iid, sid, t, v in zip(\n",
    "        chunk[\"itemid\"].values,\n",
    "        chunk[\"stay_id\"].astype(\"int32\").values,\n",
    "        chunk[\"charttime\"].values,\n",
    "        chunk[\"valuenum\"].values,\n",
    "    ):\n",
    "        # Skip non-numeric lab values (valuenum NA)\n",
    "        if pd.isna(v):\n",
    "            continue\n",
    "        var = labitem_to_var.get(int(iid))\n",
    "        if var is None:\n",
    "            continue\n",
    "        agg_labs[(int(sid), var)].update(pd.Timestamp(t), float(v))\n",
    "\n",
    "    del chunk\n",
    "    gc.collect()\n",
    "\n",
    "# Build wide lab feature table\n",
    "rows = []\n",
    "for (sid, var), A in agg_labs.items():\n",
    "    d = A.to_dict()\n",
    "    row = {\"stay_id\": sid}\n",
    "    for k,v in d.items():\n",
    "        row[f\"lab_{var}_{k}_0_24h\"] = v\n",
    "    rows.append(row)\n",
    "\n",
    "fe_labs = pd.DataFrame(rows).groupby(\"stay_id\", as_index=False).first()\n",
    "print(\"Lab features:\", fe_labs.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimal cohort columns for modeling/partitioning\n",
    "base_cols = [\n",
    "    \"subject_id\",\"hadm_id\",\"stay_id\",\n",
    "    \"careunit\",\"adm_year\",\n",
    "    \"intime\",\"outtime\",\"admittime\",\"dischtime\",\"deathtime\",\n",
    "    \"icu_los_hours\",\"age_at_admit\",\n",
    "    \"label\",\n",
    "]\n",
    "base = cohort[base_cols].copy()\n",
    "\n",
    "# Merge features\n",
    "final = base.merge(fe_vitals, on=\"stay_id\", how=\"left\").merge(fe_labs, on=\"stay_id\", how=\"left\")\n",
    "\n",
    "# Optional missingness flags (helps models + later imputation)\n",
    "for c in final.columns:\n",
    "    if c.endswith(\"_0_24h\") and (\"count\" not in c):\n",
    "        final[c.replace(\"_0_24h\",\"_missing_0_24h\")] = final[c].isna().astype(\"int8\")\n",
    "\n",
    "print(\"Final shape:\", final.shape)\n",
    "print(\"Label prevalence:\", final[\"label\"].mean().round(4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_PATH = \"/kaggle/working/mimic_iv_0_24h_features.parquet\"\n",
    "final.to_parquet(OUT_PATH, index=False)\n",
    "print(\"Saved:\", OUT_PATH)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "tpuV5e8",
   "dataSources": [
    {
     "datasetId": 8126524,
     "sourceId": 12915584,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8172892,
     "sourceId": 12916335,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8219498,
     "sourceId": 12986024,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8383840,
     "sourceId": 13226665,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31091,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
