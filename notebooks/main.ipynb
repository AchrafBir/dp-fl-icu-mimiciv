{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jtf423-PXutb"
   },
   "source": [
    "### Quick timer check\n",
    "Call this before/after long blocks to ensure we stay within the 12h budget.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZXCgLxHkXutc"
   },
   "outputs": [],
   "source": [
    "runtime_guard(start_time, budget_hours=12)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tgLspCtYXutd"
   },
   "source": [
    "# PART 0: Configuration & Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "id": "Uy_CMY7CXutd"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, List, Tuple, Optional\n",
    "from dataclasses import dataclass, field\n",
    "import json\n",
    "import pickle\n",
    "import os\n",
    "import random\n",
    "from datetime import datetime\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.feature_selection import mutual_info_classif, SelectKBest\n",
    "from sklearn.linear_model import LogisticRegression, Lasso\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.calibration import calibration_curve, CalibratedClassifierCV\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, average_precision_score, roc_curve, precision_recall_curve,\n",
    "    confusion_matrix, accuracy_score, precision_score, recall_score, f1_score,\n",
    "    brier_score_loss, log_loss\n",
    ")\n",
    "\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.stats import bootstrap, ttest_rel, chi2_contingency\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WIeiRuDyXute"
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from pathlib import Path\n",
    "from typing import Optional, Dict, Any, List, Tuple\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    RANDOM_STATE: int = 42\n",
    "\n",
    "    BASE_DIR: Path = field(default_factory=lambda: Path(\"/kaggle/working\"))\n",
    "    RAW_DATA_PATH: Optional[Path] = None\n",
    "    PROCESSED_DIR: Optional[Path] = None\n",
    "    RESULTS_DIR: Optional[Path] = None\n",
    "    MODELS_DIR: Optional[Path] = None\n",
    "    FIGURES_DIR: Optional[Path] = None\n",
    "\n",
    "    TRAIN_RATIO: float = 0.70\n",
    "    VAL_RATIO: float = 0.10\n",
    "    TEST_RATIO: float = 0.20\n",
    "    MISSINGNESS_THRESHOLD: float = 0.40\n",
    "    CORRELATION_THRESHOLD: float = 0.95\n",
    "    MICE_MAX_ITER: int = 50\n",
    "    MICE_TOL: float = 1e-3\n",
    "    OUTLIER_THRESHOLD: float = 5.0\n",
    "\n",
    "    TOP_VARIANCE_FEATURES: int = 10\n",
    "    TOP_INTERACTION_FEATURES: int = 5\n",
    "\n",
    "    CV_FOLDS: int = 5\n",
    "    CV_SHUFFLE: bool = True\n",
    "\n",
    "    OPTUNA_N_TRIALS: int = 150\n",
    "    OPTUNA_TIMEOUT: Optional[int] = 7200\n",
    "\n",
    "    EARLY_STOPPING_PATIENCE: int = 10\n",
    "    MAX_EPOCHS: int = 100\n",
    "    BATCH_SIZE: int = 64\n",
    "\n",
    "    N_BOOTSTRAP: int = 1000\n",
    "    CONFIDENCE_LEVEL: float = 0.95\n",
    "    ALPHA: float = 0.05\n",
    "\n",
    "    FL_ROUNDS: int = 50\n",
    "    FL_LOCAL_EPOCHS: int = 5\n",
    "    FL_CLIENT_FRACTION: float = 1.0\n",
    "    FEDPROX_MU_RANGE: Tuple[float, float] = (0.001, 0.1)\n",
    "    DITTO_LAMBDA_RANGE: Tuple[float, float] = (0.01, 1.0)\n",
    "\n",
    "    CALIBRATION_BINS: int = 10\n",
    "    CALIBRATION_METHOD: str = 'isotonic'\n",
    "\n",
    "    FIG_DPI: int = 300\n",
    "    FIG_FORMAT: List[str] = field(default_factory=lambda: ['png', 'pdf'])\n",
    "    FIG_SIZE_SMALL: Tuple[int, int] = (8, 6)\n",
    "    FIG_SIZE_LARGE: Tuple[int, int] = (12, 8)\n",
    "    FIG_SIZE_WIDE: Tuple[int, int] = (14, 6)\n",
    "\n",
    "    SAINT_PARAMS: Dict[str, Any] = field(default_factory=lambda: {\n",
    "        'd_model': 256,\n",
    "        'n_heads': 8,\n",
    "        'n_layers': 2,\n",
    "        'dropout': 0.42,\n",
    "        'lr': 1.67e-05,\n",
    "        'batch_size': 64\n",
    "    })\n",
    "\n",
    "    LGBM_PARAMS: Dict[str, Any] = field(default_factory=lambda: {\n",
    "        'n_estimators': 500,\n",
    "        'max_depth': 6,\n",
    "        'learning_rate': 0.05,\n",
    "        'colsample_bytree': 0.8,\n",
    "        'subsample': 0.8,\n",
    "        'random_state': 42,\n",
    "        'verbose': -1\n",
    "    })\n",
    "\n",
    "    XGB_PARAMS: Dict[str, Any] = field(default_factory=lambda: {\n",
    "        'n_estimators': 500,\n",
    "        'max_depth': 6,\n",
    "        'learning_rate': 0.05,\n",
    "        'colsample_bytree': 0.8,\n",
    "        'subsample': 0.8,\n",
    "        'random_state': 42,\n",
    "        'tree_method': 'hist',\n",
    "        'eval_metric': 'logloss'\n",
    "    })\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.PROCESSED_DIR = self.BASE_DIR / \"processed_data_final\"\n",
    "        self.RESULTS_DIR = self.BASE_DIR / \"results\"\n",
    "        self.MODELS_DIR = self.BASE_DIR / \"models\"\n",
    "        self.FIGURES_DIR = self.BASE_DIR / \"figures\"\n",
    "\n",
    "        for directory in [self.PROCESSED_DIR, self.RESULTS_DIR,\n",
    "                          self.MODELS_DIR, self.FIGURES_DIR]:\n",
    "            directory.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        possible_paths = [\n",
    "            Path(\"/kaggle/input/parquet/mimic_iv_0_24h_features.parquet\"),\n",
    "            Path(\"/kaggle/input/mimic-iv-features/mimic_iv_0_24h_features.parquet\"),\n",
    "            Path(\"/kaggle/input/your-dataset-name/mimic_iv_0_24h_features.parquet\"),\n",
    "            Path(\"/home/achraf/Documents/pfe/data/mimic_iv_0_24h_features.parquet\"),\n",
    "            Path(\"/home/achraf/Documents/pfe/mimic_iv_0_24h_features.parquet\"),\n",
    "        ]\n",
    "\n",
    "        for path in possible_paths:\n",
    "            if path.exists():\n",
    "                self.RAW_DATA_PATH = path\n",
    "                break\n",
    "\n",
    "    def save_config(self, filename: str = \"experiment_config.json\"):\n",
    "        config_dict = {\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'random_state': self.RANDOM_STATE,\n",
    "            'preprocessing': {\n",
    "                'train_ratio': self.TRAIN_RATIO,\n",
    "                'val_ratio': self.VAL_RATIO,\n",
    "                'test_ratio': self.TEST_RATIO,\n",
    "                'missingness_threshold': self.MISSINGNESS_THRESHOLD,\n",
    "                'correlation_threshold': self.CORRELATION_THRESHOLD,\n",
    "                'mice_max_iter': self.MICE_MAX_ITER,\n",
    "                'outlier_threshold': self.OUTLIER_THRESHOLD\n",
    "            },\n",
    "            'cross_validation': {\n",
    "                'cv_folds': self.CV_FOLDS,\n",
    "                'cv_shuffle': self.CV_SHUFFLE\n",
    "            },\n",
    "            'federated_learning': {\n",
    "                'fl_rounds': self.FL_ROUNDS,\n",
    "                'fl_local_epochs': self.FL_LOCAL_EPOCHS\n",
    "            }\n",
    "        }\n",
    "\n",
    "        filepath = self.RESULTS_DIR / filename\n",
    "        with open(filepath, 'w') as f:\n",
    "            json.dump(config_dict, f, indent=2)\n",
    "\n",
    "config = Config()\n",
    "config.save_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "04U6ijUAXutf"
   },
   "outputs": [],
   "source": [
    "def set_all_seeds(seed: int = 42) -> None:\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def worker_init_fn(worker_id: int) -> None:\n",
    "    worker_seed = config.RANDOM_STATE + worker_id\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "set_all_seeds(config.RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0UoWbLX3Xutg"
   },
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-tJ4hC2lXutg"
   },
   "outputs": [],
   "source": [
    "def compute_metrics(\n",
    "    y_true: np.ndarray,\n",
    "    y_pred: np.ndarray,\n",
    "    y_proba: np.ndarray,\n",
    "    prefix: str = \"\"\n",
    ") -> Dict[str, float]:\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "\n",
    "    metrics = {\n",
    "        f\"{prefix}auroc\": roc_auc_score(y_true, y_proba),\n",
    "        f\"{prefix}auprc\": average_precision_score(y_true, y_proba),\n",
    "        f\"{prefix}accuracy\": accuracy_score(y_true, y_pred),\n",
    "        f\"{prefix}precision\": precision_score(y_true, y_pred, zero_division=0),\n",
    "        f\"{prefix}recall\": recall_score(y_true, y_pred, zero_division=0),\n",
    "        f\"{prefix}f1\": f1_score(y_true, y_pred, zero_division=0),\n",
    "        f\"{prefix}specificity\": tn / (tn + fp) if (tn + fp) > 0 else 0,\n",
    "        f\"{prefix}brier\": brier_score_loss(y_true, y_proba),\n",
    "        f\"{prefix}log_loss\": log_loss(y_true, y_proba),\n",
    "        f\"{prefix}tp\": int(tp),\n",
    "        f\"{prefix}tn\": int(tn),\n",
    "        f\"{prefix}fp\": int(fp),\n",
    "        f\"{prefix}fn\": int(fn)\n",
    "    }\n",
    "\n",
    "    return metrics\n",
    "\n",
    "def bootstrap_metric(\n",
    "    y_true: np.ndarray,\n",
    "    y_pred: np.ndarray,\n",
    "    metric_fn: callable,\n",
    "    n_bootstraps: int = 1000,\n",
    "    confidence_level: float = 0.95,\n",
    "    random_state: int = 42\n",
    ") -> Tuple[float, float, float]:\n",
    "    rng = np.random.RandomState(random_state)\n",
    "    n_samples = len(y_true)\n",
    "\n",
    "    scores = []\n",
    "    for _ in range(n_bootstraps):\n",
    "        indices = rng.choice(n_samples, n_samples, replace=True)\n",
    "        try:\n",
    "            score = metric_fn(y_true[indices], y_pred[indices])\n",
    "            scores.append(score)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    scores = np.array(scores)\n",
    "    point_estimate = metric_fn(y_true, y_pred)\n",
    "\n",
    "    alpha = 1 - confidence_level\n",
    "    lower_percentile = (alpha / 2) * 100\n",
    "    upper_percentile = (1 - alpha / 2) * 100\n",
    "\n",
    "    lower = np.percentile(scores, lower_percentile)\n",
    "    upper = np.percentile(scores, upper_percentile)\n",
    "\n",
    "    return point_estimate, lower, upper\n",
    "\n",
    "def compute_metrics_with_ci(\n",
    "    y_true: np.ndarray,\n",
    "    y_pred: np.ndarray,\n",
    "    y_proba: np.ndarray,\n",
    "    n_bootstraps: int = 1000\n",
    ") -> Dict[str, Tuple[float, float, float]]:\n",
    "    metrics_with_ci = {}\n",
    "\n",
    "    metrics_with_ci['auroc'] = bootstrap_metric(\n",
    "        y_true, y_proba, roc_auc_score, n_bootstraps\n",
    "    )\n",
    "\n",
    "    metrics_with_ci['auprc'] = bootstrap_metric(\n",
    "        y_true, y_proba, average_precision_score, n_bootstraps\n",
    "    )\n",
    "\n",
    "    metrics_with_ci['accuracy'] = bootstrap_metric(\n",
    "        y_true, y_pred, accuracy_score, n_bootstraps\n",
    "    )\n",
    "\n",
    "    metrics_with_ci['f1'] = bootstrap_metric(\n",
    "        y_true, y_pred, f1_score, n_bootstraps\n",
    "    )\n",
    "\n",
    "    metrics_with_ci['brier'] = bootstrap_metric(\n",
    "        y_true, y_proba, brier_score_loss, n_bootstraps\n",
    "    )\n",
    "\n",
    "    return metrics_with_ci\n",
    "\n",
    "def delong_roc_variance(y_true: np.ndarray, y_score: np.ndarray) -> Tuple[float, np.ndarray]:\n",
    "    y_true = np.asarray(y_true).ravel()\n",
    "    y_score = np.asarray(y_score).ravel()\n",
    "    \n",
    "    pos_idx = np.where(y_true == 1)[0]\n",
    "    neg_idx = np.where(y_true == 0)[0]\n",
    "    \n",
    "    n_pos = len(pos_idx)\n",
    "    n_neg = len(neg_idx)\n",
    "    \n",
    "    if n_pos == 0 or n_neg == 0:\n",
    "        return 0.5, np.array([0.0])\n",
    "    \n",
    "    pos_scores = y_score[pos_idx]\n",
    "    neg_scores = y_score[neg_idx]\n",
    "    \n",
    "    \n",
    "    V10 = np.zeros(n_pos)\n",
    "    for i, ps in enumerate(pos_scores):\n",
    "        V10[i] = np.mean(ps > neg_scores) + 0.5 * np.mean(ps == neg_scores)\n",
    "    \n",
    "    V01 = np.zeros(n_neg)\n",
    "    for j, ns in enumerate(neg_scores):\n",
    "        V01[j] = np.mean(pos_scores > ns) + 0.5 * np.mean(pos_scores == ns)\n",
    "    \n",
    "    auc = np.mean(V10)\n",
    "    \n",
    "    S10 = np.var(V10, ddof=1) if n_pos > 1 else 0.0\n",
    "    S01 = np.var(V01, ddof=1) if n_neg > 1 else 0.0\n",
    "    \n",
    "    var_auc = S10 / n_pos + S01 / n_neg\n",
    "    \n",
    "    return auc, np.sqrt(var_auc) if var_auc > 0 else 0.0\n",
    "\n",
    "\n",
    "def delong_test(\n",
    "    y_true: np.ndarray,\n",
    "    y_pred1: np.ndarray,\n",
    "    y_pred2: np.ndarray\n",
    ") -> Tuple[float, float]:\n",
    "    from scipy import stats\n",
    "    \n",
    "    y_true = np.asarray(y_true).ravel()\n",
    "    y_pred1 = np.asarray(y_pred1).ravel()\n",
    "    y_pred2 = np.asarray(y_pred2).ravel()\n",
    "    \n",
    "    pos_idx = np.where(y_true == 1)[0]\n",
    "    neg_idx = np.where(y_true == 0)[0]\n",
    "    \n",
    "    n_pos = len(pos_idx)\n",
    "    n_neg = len(neg_idx)\n",
    "    \n",
    "    if n_pos < 2 or n_neg < 2:\n",
    "        return 0.0, 1.0\n",
    "    \n",
    "    # Compute placement values for both predictions\n",
    "    def compute_placements(y_score):\n",
    "        pos_scores = y_score[pos_idx]\n",
    "        neg_scores = y_score[neg_idx]\n",
    "        \n",
    "        V10 = np.array([\n",
    "            np.mean(ps > neg_scores) + 0.5 * np.mean(ps == neg_scores)\n",
    "            for ps in pos_scores\n",
    "        ])\n",
    "        V01 = np.array([\n",
    "            np.mean(pos_scores > ns) + 0.5 * np.mean(pos_scores == ns)\n",
    "            for ns in neg_scores\n",
    "        ])\n",
    "        return V10, V01\n",
    "    \n",
    "    V10_1, V01_1 = compute_placements(y_pred1)\n",
    "    V10_2, V01_2 = compute_placements(y_pred2)\n",
    "    \n",
    "    auc1 = np.mean(V10_1)\n",
    "    auc2 = np.mean(V10_2)\n",
    "    \n",
    "    D10 = V10_1 - V10_2\n",
    "    D01 = V01_1 - V01_2\n",
    "    \n",
    "    S10 = np.var(D10, ddof=1)\n",
    "    S01 = np.var(D01, ddof=1)\n",
    "    \n",
    "    var_diff = S10 / n_pos + S01 / n_neg\n",
    "    \n",
    "    if var_diff <= 0:\n",
    "        return 0.0, 1.0\n",
    "    \n",
    "    z_score = (auc1 - auc2) / np.sqrt(var_diff)\n",
    "    p_value = 2 * (1 - stats.norm.cdf(np.abs(z_score)))\n",
    "    \n",
    "    return float(z_score), float(p_value)\n",
    "def save_figure(\n",
    "    fig: plt.Figure,\n",
    "    filename: str,\n",
    "    tight: bool = True\n",
    ") -> None:\n",
    "    if tight:\n",
    "        fig.tight_layout()\n",
    "\n",
    "    for fmt in config.FIG_FORMAT:\n",
    "        filepath = config.FIGURES_DIR / f\"{filename}.{fmt}\"\n",
    "        fig.savefig(filepath, dpi=config.FIG_DPI, bbox_inches='tight')\n",
    "\n",
    "    plt.close(fig)\n",
    "\n",
    "def print_metrics_with_ci(\n",
    "    metrics: Dict[str, Tuple[float, float, float]],\n",
    "    title: str = \"Metrics\"\n",
    ") -> None:\n",
    "    print(f\"\\n{title}:\")\n",
    "    for metric_name, (point, lower, upper) in metrics.items():\n",
    "        print(f\"{metric_name:15s}: {point:.4f} (95% CI: {lower:.4f}-{upper:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Dict, Tuple, Optional\n",
    "from sklearn.metrics import roc_auc_score, brier_score_loss\n",
    "\n",
    "\n",
    "def compute_calibration_metrics(\n",
    "    y_true: np.ndarray,\n",
    "    y_prob: np.ndarray,\n",
    "    n_bins: int = 10,\n",
    "    strategy: str = 'uniform'\n",
    ") -> Dict[str, float]:\n",
    "    y_true = np.asarray(y_true).ravel()\n",
    "    y_prob = np.asarray(y_prob).ravel()\n",
    "    \n",
    "    if strategy == 'uniform':\n",
    "        bins = np.linspace(0.0, 1.0, n_bins + 1)\n",
    "    elif strategy == 'quantile':\n",
    "        bins = np.percentile(y_prob, np.linspace(0, 100, n_bins + 1))\n",
    "        bins[0] = 0.0\n",
    "        bins[-1] = 1.0\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown strategy: {strategy}\")\n",
    "    \n",
    "    bin_indices = np.digitize(y_prob, bins[1:-1])\n",
    "    \n",
    "    ece = 0.0\n",
    "    mce = 0.0\n",
    "    ace = 0.0\n",
    "    \n",
    "    bin_stats = []\n",
    "    n_samples = len(y_true)\n",
    "    n_nonempty_bins = 0\n",
    "    \n",
    "    for i in range(n_bins):\n",
    "        mask = bin_indices == i\n",
    "        bin_size = mask.sum()\n",
    "        \n",
    "        if bin_size > 0:\n",
    "            bin_acc = y_true[mask].mean()\n",
    "            bin_conf = y_prob[mask].mean()\n",
    "            bin_error = np.abs(bin_acc - bin_conf)\n",
    "            \n",
    "            ece += (bin_size / n_samples) * bin_error\n",
    "            mce = max(mce, bin_error)\n",
    "            ace += bin_error\n",
    "            n_nonempty_bins += 1\n",
    "            \n",
    "            bin_stats.append({\n",
    "                'bin_idx': i,\n",
    "                'bin_lower': bins[i],\n",
    "                'bin_upper': bins[i + 1],\n",
    "                'count': int(bin_size),\n",
    "                'accuracy': float(bin_acc),\n",
    "                'confidence': float(bin_conf),\n",
    "                'error': float(bin_error)\n",
    "            })\n",
    "    \n",
    "    ace = ace / max(n_nonempty_bins, 1)\n",
    "    \n",
    "    return {\n",
    "        'ece': float(ece),\n",
    "        'mce': float(mce),\n",
    "        'ace': float(ace),\n",
    "        'n_bins': n_bins,\n",
    "        'n_nonempty_bins': n_nonempty_bins,\n",
    "        'bin_stats': bin_stats\n",
    "    }\n",
    "\n",
    "\n",
    "def compute_full_metrics(\n",
    "    y_true: np.ndarray,\n",
    "    y_prob: np.ndarray,\n",
    "    n_bins: int = 10\n",
    ") -> Dict[str, float]:\n",
    "    y_true = np.asarray(y_true).ravel()\n",
    "    y_prob = np.asarray(y_prob).ravel()\n",
    "    \n",
    "    if len(np.unique(y_true)) < 2:\n",
    "        return {\n",
    "            'auroc': 0.5,\n",
    "            'brier': 0.25,\n",
    "            'ece': 0.0,\n",
    "            'mce': 0.0,\n",
    "            'ace': 0.0\n",
    "        }\n",
    "    \n",
    "    cal_metrics = compute_calibration_metrics(y_true, y_prob, n_bins)\n",
    "    \n",
    "    return {\n",
    "        'auroc': float(roc_auc_score(y_true, y_prob)),\n",
    "        'brier': float(brier_score_loss(y_true, y_prob)),\n",
    "        'ece': cal_metrics['ece'],\n",
    "        'mce': cal_metrics['mce'],\n",
    "        'ace': cal_metrics['ace']\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-QU21n2tXuth"
   },
   "outputs": [],
   "source": [
    "def get_optimal_threshold(y_true, y_probs):\n",
    "    from sklearn.metrics import precision_recall_curve\n",
    "    precision, recall, thresholds = precision_recall_curve(y_true, y_probs)\n",
    "    f1_scores = 2 * (precision * recall) / (precision + recall + 1e-10)\n",
    "    ix = np.argmax(f1_scores)\n",
    "    return thresholds[ix], f1_scores[ix]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VVTQ_uNAXuth"
   },
   "source": [
    "# PART 1: Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3M6BgIJRXuth"
   },
   "outputs": [],
   "source": [
    "class BestPracticeFeatureSelector:\n",
    "    def __init__(\n",
    "        self,\n",
    "        missingness_threshold: float = 0.50,\n",
    "        correlation_threshold: float = 0.98,\n",
    "        random_state: int = 42\n",
    "    ):\n",
    "        self.missingness_threshold = missingness_threshold\n",
    "        self.correlation_threshold = correlation_threshold\n",
    "        self.random_state = random_state\n",
    "\n",
    "        self.selected_features = None\n",
    "        self.feature_importance = None\n",
    "        self.selection_info = {}\n",
    "\n",
    "    def fit(self, X_train: pd.DataFrame, y_train: np.ndarray) -> 'BestPracticeFeatureSelector':\n",
    "        missing_pct = X_train.isnull().sum() / len(X_train)\n",
    "        features_step1 = missing_pct[missing_pct <= self.missingness_threshold].index.tolist()\n",
    "        self.selection_info['after_missingness'] = features_step1\n",
    "\n",
    "        X_step1 = X_train[features_step1].copy()\n",
    "\n",
    "        X_for_corr = X_step1.fillna(X_step1.median())\n",
    "        corr_matrix = X_for_corr.corr().abs()\n",
    "\n",
    "        upper_tri = corr_matrix.where(\n",
    "            np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)\n",
    "        )\n",
    "\n",
    "        to_drop = [col for col in upper_tri.columns\n",
    "                   if any(upper_tri[col] > self.correlation_threshold)]\n",
    "\n",
    "        features_step2 = [f for f in features_step1 if f not in to_drop]\n",
    "        self.selection_info['after_correlation'] = features_step2\n",
    "\n",
    "        X_step2 = X_train[features_step2].copy()\n",
    "\n",
    "        X_for_mi = X_step2.copy()\n",
    "        for col in X_for_mi.columns:\n",
    "            if X_for_mi[col].isnull().any():\n",
    "                X_for_mi[col] = X_for_mi[col].fillna(X_for_mi[col].median())\n",
    "\n",
    "        mi_scores = mutual_info_classif(\n",
    "            X_for_mi, y_train,\n",
    "            random_state=self.random_state\n",
    "        )\n",
    "        mi_threshold = np.percentile(mi_scores, 10)\n",
    "        features_step3 = X_step2.columns[mi_scores > mi_threshold].tolist()\n",
    "        self.selection_info['after_mi'] = features_step3\n",
    "\n",
    "        X_step3 = X_train[features_step3].copy()\n",
    "\n",
    "        X_for_l1 = X_step3.copy()\n",
    "        for col in X_for_l1.columns:\n",
    "            if X_for_l1[col].isnull().any():\n",
    "                X_for_l1[col] = X_for_l1[col].fillna(X_for_l1[col].median())\n",
    "\n",
    "        scaler_temp = StandardScaler()\n",
    "        X_scaled = scaler_temp.fit_transform(X_for_l1)\n",
    "\n",
    "        lasso = Lasso(alpha=0.001, random_state=self.random_state, max_iter=5000)\n",
    "        lasso.fit(X_scaled, y_train)\n",
    "\n",
    "        features_step4 = X_step3.columns[np.abs(lasso.coef_) > 0].tolist()\n",
    "        self.selection_info['after_l1'] = features_step4\n",
    "\n",
    "        X_for_lgb = X_train[features_step4].copy()\n",
    "        for col in X_for_lgb.columns:\n",
    "            if X_for_lgb[col].isnull().any():\n",
    "                X_for_lgb[col] = X_for_lgb[col].fillna(X_for_lgb[col].median())\n",
    "\n",
    "        lgb_model = lgb.LGBMClassifier(\n",
    "            n_estimators=500,\n",
    "            random_state=self.random_state,\n",
    "            verbose=-1\n",
    "        )\n",
    "        lgb_model.fit(X_for_lgb, y_train)\n",
    "\n",
    "        importance_scores = lgb_model.feature_importances_\n",
    "        importance_threshold = np.percentile(importance_scores, 5)\n",
    "        features_step5 = X_for_lgb.columns[importance_scores > importance_threshold].tolist()\n",
    "\n",
    "        self.selected_features = features_step5\n",
    "        self.selection_info['final'] = features_step5\n",
    "\n",
    "        self.feature_importance = pd.DataFrame({\n",
    "            'feature': features_step5,\n",
    "            'importance': importance_scores[X_for_lgb.columns.isin(features_step5)]\n",
    "        }).sort_values('importance', ascending=False)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        if self.selected_features is None:\n",
    "            raise ValueError(\"FeatureSelector has not been fitted yet.\")\n",
    "\n",
    "        return X[self.selected_features].copy()\n",
    "\n",
    "    def fit_transform(self, X: pd.DataFrame, y: np.ndarray) -> pd.DataFrame:\n",
    "        return self.fit(X, y).transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J_cbI05JXuth"
   },
   "outputs": [],
   "source": [
    "class BestPracticeImputer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        max_iter: int = 50,\n",
    "        tol: float = 1e-3,\n",
    "        random_state: int = 42\n",
    "    ):\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        self.random_state = random_state\n",
    "\n",
    "        self.imputer = IterativeImputer(\n",
    "            max_iter=max_iter,\n",
    "            tol=tol,\n",
    "            random_state=random_state,\n",
    "            verbose=0\n",
    "        )\n",
    "\n",
    "        self.feature_names = None\n",
    "        self.convergence_info = {}\n",
    "\n",
    "    def fit(self, X_train: pd.DataFrame) -> 'BestPracticeImputer':\n",
    "        self.feature_names = X_train.columns.tolist()\n",
    "\n",
    "        self.imputer.fit(X_train)\n",
    "\n",
    "        if hasattr(self.imputer, 'n_iter_'):\n",
    "            actual_iters = self.imputer.n_iter_\n",
    "            self.convergence_info['iterations'] = actual_iters\n",
    "\n",
    "            if actual_iters >= self.max_iter:\n",
    "                self.convergence_info['converged'] = False\n",
    "            else:\n",
    "                self.convergence_info['converged'] = True\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        if self.feature_names is None:\n",
    "            raise ValueError(\"Imputer has not been fitted yet.\")\n",
    "\n",
    "        X_imputed = self.imputer.transform(X)\n",
    "        X_imputed = pd.DataFrame(X_imputed, columns=self.feature_names, index=X.index)\n",
    "\n",
    "        return X_imputed\n",
    "\n",
    "    def fit_transform(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        return self.fit(X).transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JuQC5Dk5Xuti"
   },
   "outputs": [],
   "source": [
    "class OutlierHandler:\n",
    "    def __init__(self, iqr_multiplier: float = 5.0):\n",
    "        self.iqr_multiplier = iqr_multiplier\n",
    "        self.lower_bounds = {}\n",
    "        self.upper_bounds = {}\n",
    "        self.outlier_info = {}\n",
    "\n",
    "    def fit(self, X_train: pd.DataFrame) -> 'OutlierHandler':\n",
    "        for col in X_train.columns:\n",
    "            if X_train[col].dtype in [np.float64, np.float32, np.int64, np.int32]:\n",
    "                Q1 = X_train[col].quantile(0.25)\n",
    "                Q3 = X_train[col].quantile(0.75)\n",
    "                IQR = Q3 - Q1\n",
    "\n",
    "                self.lower_bounds[col] = Q1 - self.iqr_multiplier * IQR\n",
    "                self.upper_bounds[col] = Q3 + self.iqr_multiplier * IQR\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        X_clipped = X.copy()\n",
    "\n",
    "        for col in X_clipped.columns:\n",
    "            if col in self.lower_bounds:\n",
    "                lower = self.lower_bounds[col]\n",
    "                upper = self.upper_bounds[col]\n",
    "                X_clipped[col] = X_clipped[col].clip(lower, upper)\n",
    "\n",
    "        return X_clipped\n",
    "\n",
    "    def fit_transform(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        return self.fit(X).transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S59KTx0iXuti"
   },
   "outputs": [],
   "source": [
    "class BestPracticeScaler:\n",
    "    def __init__(self, method: str = 'standard'):\n",
    "        self.method = method\n",
    "\n",
    "        if method == 'standard':\n",
    "            self.scaler = StandardScaler()\n",
    "        elif method == 'robust':\n",
    "            self.scaler = RobustScaler()\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown scaling method: {method}\")\n",
    "\n",
    "        self.feature_names = None\n",
    "\n",
    "    def fit(self, X_train: pd.DataFrame) -> 'BestPracticeScaler':\n",
    "        self.feature_names = X_train.columns.tolist()\n",
    "        self.scaler.fit(X_train)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        if self.feature_names is None:\n",
    "            raise ValueError(\"Scaler has not been fitted yet.\")\n",
    "\n",
    "        X_scaled = self.scaler.transform(X)\n",
    "        X_scaled = pd.DataFrame(X_scaled, columns=self.feature_names, index=X.index)\n",
    "\n",
    "        return X_scaled\n",
    "\n",
    "    def fit_transform(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        return self.fit(X).transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RB6Q02vGXuti"
   },
   "source": [
    "## Load and Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gAEGg5rbXutj"
   },
   "outputs": [],
   "source": [
    "if config.RAW_DATA_PATH is None or not config.RAW_DATA_PATH.exists():\n",
    "    print(\"ERROR: Data file not found!\")\n",
    "    print(\"Please update config.RAW_DATA_PATH to point to your data file.\")\n",
    "    print(\"Expected location: MIMIC-IV ICU data in Parquet or CSV format\")\n",
    "    print(\"You can set it manually:\")\n",
    "    print('config.RAW_DATA_PATH = Path(\"/path/to/your/data.parquet\")')\n",
    "    raise FileNotFoundError(f\"Data file not found at {config.RAW_DATA_PATH}\")\n",
    "\n",
    "if config.RAW_DATA_PATH.suffix == '.parquet':\n",
    "    df_raw = pd.read_parquet(config.RAW_DATA_PATH)\n",
    "elif config.RAW_DATA_PATH.suffix == '.csv':\n",
    "    df_raw = pd.read_csv(config.RAW_DATA_PATH)\n",
    "else:\n",
    "    raise ValueError(f\"Unsupported file format: {config.RAW_DATA_PATH.suffix}\")\n",
    "\n",
    "if 'label' in df_raw.columns:\n",
    "    mortality_rate = df_raw['label'].mean()\n",
    "\n",
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aKG_X9b8Xutj"
   },
   "outputs": [],
   "source": [
    "TARGET_COL = 'label'\n",
    "ID_COLS = ['subject_id', 'hadm_id', 'stay_id', 'icustay_id']\n",
    "\n",
    "id_cols_present = [col for col in ID_COLS if col in df_raw.columns]\n",
    "\n",
    "exclude_cols = [TARGET_COL] + id_cols_present\n",
    "feature_cols = [col for col in df_raw.columns if col not in exclude_cols]\n",
    "\n",
    "print(f\"Target column: {TARGET_COL}\")\n",
    "print(f\"ID columns: {id_cols_present}\")\n",
    "print(f\"Feature columns: {len(feature_cols)}\")\n",
    "\n",
    "X = df_raw[feature_cols].copy()\n",
    "y = df_raw[TARGET_COL].values\n",
    "\n",
    "if id_cols_present:\n",
    "    ids_df = df_raw[id_cols_present].copy()\n",
    "else:\n",
    "    ids_df = pd.DataFrame(index=df_raw.index)\n",
    "\n",
    "print(f\"\\nFeature matrix: {X.shape}\")\n",
    "print(f\"Target vector: {y.shape}\")\n",
    "print(f\"Class distribution: {np.bincount(y.astype(int))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jM7y266bXutj"
   },
   "outputs": [],
   "source": [
    "X_temp, X_test, y_temp, y_test, ids_temp, ids_test = train_test_split(\n",
    "    X, y, ids_df,\n",
    "    test_size=config.TEST_RATIO,\n",
    "    random_state=config.RANDOM_STATE,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "val_size_adjusted = config.VAL_RATIO / (config.TRAIN_RATIO + config.VAL_RATIO)\n",
    "X_train, X_val, y_train, y_val, ids_train, ids_val = train_test_split(\n",
    "    X_temp, y_temp, ids_temp,\n",
    "    test_size=val_size_adjusted,\n",
    "    random_state=config.RANDOM_STATE,\n",
    "    stratify=y_temp\n",
    ")\n",
    "\n",
    "print(f\"Train set: {X_train.shape[0]:,} samples ({y_train.mean():.2%} mortality)\")\n",
    "print(f\"Val set:   {X_val.shape[0]:,} samples ({y_val.mean():.2%} mortality)\")\n",
    "print(f\"Test set:  {X_test.shape[0]:,} samples ({y_test.mean():.2%} mortality)\")\n",
    "\n",
    "del X_temp, y_temp, ids_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JNwSx5V1Xutk"
   },
   "outputs": [],
   "source": [
    "print(\"EXTRACTING ICU TYPES FOR FL PARTITIONING\")\n",
    "\n",
    "icu_column_candidates = [\n",
    "    'first_careunit',\n",
    "    'last_careunit',\n",
    "    'careunit',\n",
    "    'icu_type',\n",
    "    'care_unit',\n",
    "    'unit_name'\n",
    "]\n",
    "\n",
    "icu_column = None\n",
    "for col in icu_column_candidates:\n",
    "    if col in df_raw.columns:\n",
    "        icu_column = col\n",
    "        print(f\"Found ICU column: '{icu_column}'\")\n",
    "        break\n",
    "\n",
    "if icu_column is None:\n",
    "    icu_cols = [col for col in df_raw.columns\n",
    "                if 'icu' in col.lower() or 'care' in col.lower() or 'unit' in col.lower()]\n",
    "    if icu_cols:\n",
    "        icu_column = icu_cols[0]\n",
    "        print(f\"Found ICU-related column: '{icu_column}'\")\n",
    "    else:\n",
    "        raise ValueError(\"No ICU/careunit column found in raw data!\")\n",
    "\n",
    "print(f\"Unique ICU types in raw data ({df_raw[icu_column].nunique()}):\")\n",
    "icu_value_counts = df_raw[icu_column].value_counts()\n",
    "for val, count in icu_value_counts.items():\n",
    "    pct = count / len(df_raw) * 100\n",
    "    print(f\"{str(val):50s}: {count:6d} ({pct:5.1f}%)\")\n",
    "\n",
    "def group_icu_types(careunit: str) -> str:\n",
    "    cu = str(careunit).lower()\n",
    "    if 'micu' in cu or 'medical' in cu:\n",
    "        return 'MICU'\n",
    "    elif 'sicu' in cu or 'surgical' in cu or 'trauma' in cu:\n",
    "        return 'SICU'\n",
    "    elif 'ccu' in cu and 'cvicu' not in cu:\n",
    "        return 'CCU'\n",
    "    elif 'cvicu' in cu or 'cardiac vascular' in cu:\n",
    "        return 'CVICU'\n",
    "    elif 'neuro' in cu or 'nsicu' in cu:\n",
    "        return 'Neuro'\n",
    "    else:\n",
    "        return 'Mixed'\n",
    "\n",
    "train_indices = ids_train.index\n",
    "val_indices = ids_val.index\n",
    "test_indices = ids_test.index\n",
    "\n",
    "icu_train_raw = df_raw.loc[train_indices, icu_column]\n",
    "icu_val_raw = df_raw.loc[val_indices, icu_column]\n",
    "icu_test_raw = df_raw.loc[test_indices, icu_column]\n",
    "\n",
    "icu_types_train = icu_train_raw.apply(group_icu_types).values\n",
    "icu_types_val = icu_val_raw.apply(group_icu_types).values\n",
    "icu_types_test = icu_test_raw.apply(group_icu_types).values\n",
    "\n",
    "print(f\"Extracted {len(icu_types_train)} train ICU types\")\n",
    "print(f\"Extracted {len(icu_types_val)} val ICU types\")\n",
    "print(f\"Extracted {len(icu_types_test)} test ICU types\")\n",
    "\n",
    "print(\"TRAIN SET:\")\n",
    "train_dist = pd.Series(icu_types_train).value_counts()\n",
    "for icu_type, count in train_dist.items():\n",
    "    pct = count / len(icu_types_train) * 100\n",
    "    mortality = y_train[icu_types_train == icu_type].mean() * 100\n",
    "    print(f\"{icu_type:8s}: {count:6d} ({pct:5.1f}%) | Mortality: {mortality:5.1f}%\")\n",
    "\n",
    "print(\"VAL SET:\")\n",
    "val_dist = pd.Series(icu_types_val).value_counts()\n",
    "for icu_type, count in val_dist.items():\n",
    "    pct = count / len(icu_types_val) * 100\n",
    "    mortality = y_val[icu_types_val == icu_type].mean() * 100\n",
    "    print(f\"{icu_type:8s}: {count:6d} ({pct:5.1f}%) | Mortality: {mortality:5.1f}%\")\n",
    "\n",
    "print(\"TEST SET:\")\n",
    "test_dist = pd.Series(icu_types_test).value_counts()\n",
    "for icu_type, count in test_dist.items():\n",
    "    pct = count / len(icu_types_test) * 100\n",
    "    mortality = y_test[icu_types_test == icu_type].mean() * 100\n",
    "    print(f\"{icu_type:8s}: {count:6d} ({pct:5.1f}%) | Mortality: {mortality:5.1f}%\")\n",
    "\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "np.save(config.PROCESSED_DIR / \"icu_types_train.npy\", icu_types_train)\n",
    "np.save(config.PROCESSED_DIR / \"icu_types_val.npy\", icu_types_val)\n",
    "np.save(config.PROCESSED_DIR / \"icu_types_test.npy\", icu_types_test)\n",
    "\n",
    "icu_mortality_rates = {}\n",
    "for icu_type in train_dist.index:\n",
    "    mortality_rate = y_train[icu_types_train == icu_type].mean()\n",
    "    icu_mortality_rates[icu_type] = mortality_rate\n",
    "\n",
    "mortality_std = np.std(list(icu_mortality_rates.values()))\n",
    "print(f\"Mortality rate std dev across ICU types: {mortality_std:.4f}\")\n",
    "print(f\"Total ICU types: {len(train_dist)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TyLj1FmjXutl"
   },
   "outputs": [],
   "source": [
    "X_train = X_train.select_dtypes(include=[np.number]).copy()\n",
    "X_val = X_val.select_dtypes(include=[np.number]).copy()\n",
    "X_test = X_test.select_dtypes(include=[np.number]).copy()\n",
    "\n",
    "feature_selector = BestPracticeFeatureSelector(\n",
    "    missingness_threshold=config.MISSINGNESS_THRESHOLD,\n",
    "    correlation_threshold=config.CORRELATION_THRESHOLD,\n",
    "    random_state=config.RANDOM_STATE\n",
    ")\n",
    "\n",
    "X_train_selected = feature_selector.fit_transform(X_train, y_train)\n",
    "X_val_selected = feature_selector.transform(X_val)\n",
    "X_test_selected = feature_selector.transform(X_test)\n",
    "\n",
    "print(f\"Features after selection:\")\n",
    "print(f\"  Train: {X_train_selected.shape}\")\n",
    "print(f\"  Val:   {X_val_selected.shape}\")\n",
    "print(f\"  Test:  {X_test_selected.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gb7frhHkXutm"
   },
   "outputs": [],
   "source": [
    "outlier_handler = OutlierHandler(iqr_multiplier=config.OUTLIER_THRESHOLD)\n",
    "\n",
    "X_train_no_outliers = outlier_handler.fit_transform(X_train_selected)\n",
    "X_val_no_outliers = outlier_handler.transform(X_val_selected)\n",
    "X_test_no_outliers = outlier_handler.transform(X_test_selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fKmNceusXutm"
   },
   "outputs": [],
   "source": [
    "class ClinicalFeatureEngineer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_top_features: int = 10,\n",
    "        n_interactions: int = 10\n",
    "    ):\n",
    "        self.n_top_features = n_top_features\n",
    "        self.n_interactions = n_interactions\n",
    "        self.lab_quantiles = {}\n",
    "        self.vital_quantiles = {}\n",
    "        self.top_variance_features = None\n",
    "        self.interaction_pairs = []\n",
    "        self.organ_thresholds = {}\n",
    "        self.vital_thresholds = {}\n",
    "        self.lab_keywords = [\n",
    "            'wbc', 'hgb', 'hemoglobin', 'hematocrit', 'platelet', 'plt',\n",
    "            'sodium', 'na', 'potassium', 'k', 'chloride', 'cl',\n",
    "            'bicarbonate', 'hco3', 'bun', 'creatinine', 'cr', 'glucose',\n",
    "            'calcium', 'ca', 'magnesium', 'mg', 'phosphate', 'lactate',\n",
    "            'troponin', 'bnp', 'procalcitonin', 'crp', 'alt', 'ast',\n",
    "            'bilirubin', 'albumin', 'inr', 'pt', 'ptt', 'ph', 'pao2',\n",
    "            'pco2', 'fio2', 'spo2'\n",
    "        ]\n",
    "        self.vital_keywords = [\n",
    "            'heart_rate', 'hr', 'pulse', 'sbp', 'dbp', 'mbp', 'map',\n",
    "            'respiratory_rate', 'rr', 'resprate', 'temperature', 'temp',\n",
    "            'spo2', 'o2sat'\n",
    "        ]\n",
    "        self.feature_info = {\n",
    "            'original_features': [],\n",
    "            'engineered_features': [],\n",
    "            'n_features_added': 0\n",
    "        }\n",
    "\n",
    "    def _find_features(\n",
    "        self,\n",
    "        keywords: List[str],\n",
    "        feature_names: List[str]\n",
    "    ) -> List[str]:\n",
    "        matched_features = []\n",
    "        for feat in feature_names:\n",
    "            feat_lower = feat.lower()\n",
    "            for keyword in keywords:\n",
    "                if keyword in feat_lower:\n",
    "                    matched_features.append(feat)\n",
    "                    break\n",
    "        return matched_features\n",
    "\n",
    "    def _find_column(self, X: pd.DataFrame, keywords: List[str]) -> Optional[str]:\n",
    "        for col in X.columns:\n",
    "            col_lower = col.lower()\n",
    "            for keyword in keywords:\n",
    "                if keyword in col_lower:\n",
    "                    return col\n",
    "        return None\n",
    "\n",
    "    def fit(self, X_train: pd.DataFrame) -> 'ClinicalFeatureEngineer':\n",
    "        self.feature_info['original_features'] = X_train.columns.tolist()\n",
    "        lab_features = self._find_features(self.lab_keywords, X_train.columns.tolist())\n",
    "        vital_features = self._find_features(self.vital_keywords, X_train.columns.tolist())\n",
    "        key_labs = ['creatinine', 'bun', 'glucose', 'lactate', 'wbc',\n",
    "                    'hemoglobin', 'platelet', 'sodium', 'potassium', 'bilirubin']\n",
    "        for lab_keyword in key_labs:\n",
    "            col = self._find_column(X_train, [lab_keyword])\n",
    "            if col:\n",
    "                self.lab_quantiles[col] = {\n",
    "                    'q01': X_train[col].quantile(0.01),\n",
    "                    'q05': X_train[col].quantile(0.05),\n",
    "                    'q25': X_train[col].quantile(0.25),\n",
    "                    'q75': X_train[col].quantile(0.75),\n",
    "                    'q95': X_train[col].quantile(0.95),\n",
    "                    'q99': X_train[col].quantile(0.99),\n",
    "                    'median': X_train[col].median()\n",
    "                }\n",
    "        for col in vital_features:\n",
    "            if col in X_train.columns:\n",
    "                self.vital_quantiles[col] = {\n",
    "                    'q05': X_train[col].quantile(0.05),\n",
    "                    'q95': X_train[col].quantile(0.95),\n",
    "                    'median': X_train[col].median()\n",
    "                }\n",
    "        self.organ_thresholds['liver'] = {\n",
    "            'alt': 80,\n",
    "            'ast': 80,\n",
    "            'bilirubin': 2.0\n",
    "        }\n",
    "        cr_col = self._find_column(X_train, ['creatinine', 'cr'])\n",
    "        if cr_col:\n",
    "            self.organ_thresholds['renal'] = {\n",
    "                'creatinine': 2.0\n",
    "            }\n",
    "        self.organ_thresholds['coagulation'] = {\n",
    "            'inr': 1.5,\n",
    "            'platelet': 100\n",
    "        }\n",
    "        self.vital_thresholds = {\n",
    "            'sbp_low': 90,\n",
    "            'hr_high': 100,\n",
    "            'rr_high': 20,\n",
    "            'temp_high': 38.0,\n",
    "            'temp_low': 36.0\n",
    "        }\n",
    "        feature_variances = X_train.var().sort_values(ascending=False)\n",
    "        self.top_variance_features = feature_variances.head(self.n_top_features).index.tolist()\n",
    "        if len(self.top_variance_features) >= 2:\n",
    "            for i in range(min(self.n_interactions, len(self.top_variance_features) - 1)):\n",
    "                self.interaction_pairs.append(\n",
    "                    (self.top_variance_features[i], self.top_variance_features[i + 1])\n",
    "                )\n",
    "        return self\n",
    "\n",
    "    def transform(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        if self.top_variance_features is None:\n",
    "            raise ValueError(\"FeatureEngineer has not been fitted yet.\")\n",
    "        X_new = X.copy()\n",
    "        engineered_features = []\n",
    "        bun_col = self._find_column(X, ['bun'])\n",
    "        cr_col = self._find_column(X, ['creatinine', 'cr'])\n",
    "        if bun_col and cr_col:\n",
    "            X_new['ratio_bun_cr'] = X[bun_col] / (X[cr_col] + 1e-6)\n",
    "            X_new['ratio_cr_bun'] = X[cr_col] / (X[bun_col] + 1e-6)\n",
    "            engineered_features.extend(['ratio_bun_cr', 'ratio_cr_bun'])\n",
    "        pao2_col = self._find_column(X, ['pao2', 'po2'])\n",
    "        fio2_col = self._find_column(X, ['fio2'])\n",
    "        if pao2_col and fio2_col:\n",
    "            X_new['ratio_pf'] = X[pao2_col] / (X[fio2_col] + 0.01)\n",
    "            engineered_features.append('ratio_pf')\n",
    "        pco2_col = self._find_column(X, ['pco2'])\n",
    "        ph_col = self._find_column(X, ['ph'])\n",
    "        if pco2_col and ph_col:\n",
    "            X_new['ratio_pco2_ph'] = X[pco2_col] / (X[ph_col] + 1e-6)\n",
    "            engineered_features.append('ratio_pco2_ph')\n",
    "        lactate_col = self._find_column(X, ['lactate'])\n",
    "        albumin_col = self._find_column(X, ['albumin'])\n",
    "        if lactate_col and albumin_col:\n",
    "            X_new['ratio_lactate_albumin'] = X[lactate_col] / (X[albumin_col] + 1e-6)\n",
    "            engineered_features.append('ratio_lactate_albumin')\n",
    "        glucose_col = self._find_column(X, ['glucose'])\n",
    "        if glucose_col and albumin_col:\n",
    "            X_new['ratio_glucose_albumin'] = X[glucose_col] / (X[albumin_col] + 1e-6)\n",
    "            engineered_features.append('ratio_glucose_albumin')\n",
    "        hgb_col = self._find_column(X, ['hemoglobin', 'hgb'])\n",
    "        wbc_col = self._find_column(X, ['wbc'])\n",
    "        if hgb_col and wbc_col:\n",
    "            X_new['ratio_hgb_wbc'] = X[hgb_col] / (X[wbc_col] + 1e-6)\n",
    "            engineered_features.append('ratio_hgb_wbc')\n",
    "        plt_col = self._find_column(X, ['platelet', 'plt'])\n",
    "        if plt_col and wbc_col:\n",
    "            X_new['ratio_plt_wbc'] = X[plt_col] / (X[wbc_col] + 1e-6)\n",
    "            engineered_features.append('ratio_plt_wbc')\n",
    "        alt_col = self._find_column(X, ['alt'])\n",
    "        ast_col = self._find_column(X, ['ast'])\n",
    "        bili_col = self._find_column(X, ['bilirubin', 'bili'])\n",
    "        liver_dysfn = 0\n",
    "        if alt_col:\n",
    "            liver_dysfn += (X[alt_col] > self.organ_thresholds['liver']['alt']).astype(int)\n",
    "        if ast_col:\n",
    "            liver_dysfn += (X[ast_col] > self.organ_thresholds['liver']['ast']).astype(int)\n",
    "        if bili_col:\n",
    "            liver_dysfn += (X[bili_col] > self.organ_thresholds['liver']['bilirubin']).astype(int)\n",
    "        X_new['liver_dysfunction'] = (liver_dysfn >= 2).astype(int)\n",
    "        X_new['liver_dysfunction_count'] = liver_dysfn\n",
    "        engineered_features.extend(['liver_dysfunction', 'liver_dysfunction_count'])\n",
    "        if cr_col:\n",
    "            X_new['renal_dysfunction'] = (X[cr_col] > self.organ_thresholds['renal']['creatinine']).astype(int)\n",
    "            engineered_features.append('renal_dysfunction')\n",
    "        inr_col = self._find_column(X, ['inr'])\n",
    "        coag_dysfn = 0\n",
    "        if inr_col:\n",
    "            coag_dysfn += (X[inr_col] > self.organ_thresholds['coagulation']['inr']).astype(int)\n",
    "        if plt_col:\n",
    "            coag_dysfn += (X[plt_col] < self.organ_thresholds['coagulation']['platelet']).astype(int)\n",
    "        X_new['coagulopathy'] = (coag_dysfn >= 1).astype(int)\n",
    "        engineered_features.append('coagulopathy')\n",
    "        sbp_col = self._find_column(X, ['sbp', 'sys'])\n",
    "        if sbp_col:\n",
    "            X_new['cardiovascular_dysfunction'] = (X[sbp_col] < self.vital_thresholds['sbp_low']).astype(int)\n",
    "            engineered_features.append('cardiovascular_dysfunction')\n",
    "        if pao2_col and fio2_col:\n",
    "            pf_ratio = X[pao2_col] / (X[fio2_col] + 0.01)\n",
    "            X_new['respiratory_dysfunction'] = (pf_ratio < 300).astype(int)\n",
    "            engineered_features.append('respiratory_dysfunction')\n",
    "        organ_cols = [c for c in X_new.columns if 'dysfunction' in c and 'count' not in c]\n",
    "        X_new['total_organ_dysfunction'] = X_new[organ_cols].sum(axis=1)\n",
    "        engineered_features.append('total_organ_dysfunction')\n",
    "        critical_count = 0\n",
    "        severe_count = 0\n",
    "        moderate_count = 0\n",
    "        for lab_col, quantiles in self.lab_quantiles.items():\n",
    "            if lab_col in X.columns:\n",
    "                is_critical = ((X[lab_col] < quantiles['q01']) | (X[lab_col] > quantiles['q99'])).astype(int)\n",
    "                critical_count += is_critical\n",
    "                is_severe = ((X[lab_col] < quantiles['q05']) | (X[lab_col] > quantiles['q95'])).astype(int)\n",
    "                is_severe = is_severe & (~is_critical.astype(bool))\n",
    "                severe_count += is_severe.astype(int)\n",
    "                is_moderate = ((X[lab_col] < quantiles['q25']) | (X[lab_col] > quantiles['q75'])).astype(int)\n",
    "                is_moderate = is_moderate & (~is_severe.astype(bool)) & (~is_critical.astype(bool))\n",
    "                moderate_count += is_moderate.astype(int)\n",
    "        X_new['abnormal_labs_critical'] = critical_count\n",
    "        X_new['abnormal_labs_severe'] = severe_count\n",
    "        X_new['abnormal_labs_moderate'] = moderate_count\n",
    "        engineered_features.extend(['abnormal_labs_critical', 'abnormal_labs_severe', 'abnormal_labs_moderate'])\n",
    "        renal_labs = ['creatinine', 'bun']\n",
    "        hepatic_labs = ['bilirubin', 'alt', 'ast']\n",
    "        heme_labs = ['wbc', 'hemoglobin', 'platelet']\n",
    "        for organ, labs_keywords in [('renal', renal_labs), ('hepatic', hepatic_labs), ('heme', heme_labs)]:\n",
    "            count = 0\n",
    "            for keyword in labs_keywords:\n",
    "                col = self._find_column(X, [keyword])\n",
    "                if col and col in self.lab_quantiles:\n",
    "                    q = self.lab_quantiles[col]\n",
    "                    count += ((X[col] < q['q05']) | (X[col] > q['q95'])).astype(int)\n",
    "            X_new[f'abnormal_{organ}_count'] = count\n",
    "            engineered_features.append(f'abnormal_{organ}_count')\n",
    "        hr_col = self._find_column(X, ['heart_rate', 'hr', 'pulse'])\n",
    "        rr_col = self._find_column(X, ['respiratory_rate', 'rr', 'resprate'])\n",
    "        temp_col = self._find_column(X, ['temperature', 'temp'])\n",
    "        if sbp_col:\n",
    "            X_new['hypotensive'] = (X[sbp_col] < self.vital_thresholds['sbp_low']).astype(int)\n",
    "            engineered_features.append('hypotensive')\n",
    "        if hr_col:\n",
    "            X_new['tachycardic'] = (X[hr_col] > self.vital_thresholds['hr_high']).astype(int)\n",
    "            engineered_features.append('tachycardic')\n",
    "        if rr_col:\n",
    "            X_new['tachypneic'] = (X[rr_col] > self.vital_thresholds['rr_high']).astype(int)\n",
    "            engineered_features.append('tachypneic')\n",
    "        if temp_col:\n",
    "            X_new['febrile'] = (X[temp_col] > self.vital_thresholds['temp_high']).astype(int)\n",
    "            X_new['hypothermic'] = (X[temp_col] < self.vital_thresholds['temp_low']).astype(int)\n",
    "            engineered_features.extend(['febrile', 'hypothermic'])\n",
    "        if all(c in X_new.columns for c in ['hypotensive', 'tachycardic', 'tachypneic']):\n",
    "            X_new['septic_pattern'] = (X_new['hypotensive'] & X_new['tachycardic'] & X_new['tachypneic']).astype(int)\n",
    "            X_new['cardiogenic_pattern'] = (X_new['hypotensive'] & ~X_new['tachycardic']).astype(int)\n",
    "            engineered_features.extend(['septic_pattern', 'cardiogenic_pattern'])\n",
    "        vital_binary_cols = [c for c in X_new.columns if c in ['hypotensive', 'tachycardic', 'tachypneic', 'febrile', 'hypothermic']]\n",
    "        if vital_binary_cols:\n",
    "            X_new['vital_instability_score'] = X_new[vital_binary_cols].sum(axis=1)\n",
    "            engineered_features.append('vital_instability_score')\n",
    "        if cr_col and glucose_col:\n",
    "            X_new['interact_cr_glucose'] = X[cr_col] * X[glucose_col]\n",
    "            engineered_features.append('interact_cr_glucose')\n",
    "        if bun_col and glucose_col:\n",
    "            X_new['interact_bun_glucose'] = X[bun_col] * X[glucose_col]\n",
    "            engineered_features.append('interact_bun_glucose')\n",
    "        if sbp_col and hr_col:\n",
    "            X_new['interact_sbp_hr'] = X[sbp_col] * X[hr_col]\n",
    "            engineered_features.append('interact_sbp_hr')\n",
    "        if wbc_col and temp_col:\n",
    "            X_new['interact_wbc_temp'] = X[wbc_col] * X[temp_col]\n",
    "            engineered_features.append('interact_wbc_temp')\n",
    "        spo2_col = self._find_column(X, ['spo2', 'o2sat'])\n",
    "        if hgb_col and spo2_col:\n",
    "            X_new['interact_hgb_spo2'] = X[hgb_col] * X[spo2_col]\n",
    "            engineered_features.append('interact_hgb_spo2')\n",
    "        for feat1, feat2 in self.interaction_pairs[:5]:\n",
    "            if feat1 in X.columns and feat2 in X.columns:\n",
    "                interaction_name = f'interact_{feat1}_{feat2}'\n",
    "                X_new[interaction_name] = X[feat1] * X[feat2]\n",
    "                engineered_features.append(interaction_name)\n",
    "        for feat in self.top_variance_features[:3]:\n",
    "            if feat in X.columns:\n",
    "                X_new[f'{feat}_squared'] = X[feat] ** 2\n",
    "                engineered_features.append(f'{feat}_squared')\n",
    "        self.feature_info['engineered_features'] = engineered_features\n",
    "        self.feature_info['n_features_added'] = len(engineered_features)\n",
    "        return X_new\n",
    "\n",
    "    def fit_transform(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        return self.fit(X).transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GobBQbtdXutm"
   },
   "outputs": [],
   "source": [
    "imputer = BestPracticeImputer(\n",
    "    max_iter=config.MICE_MAX_ITER,\n",
    "    tol=config.MICE_TOL,\n",
    "    random_state=config.RANDOM_STATE\n",
    ")\n",
    "\n",
    "X_train_imputed = imputer.fit_transform(X_train_no_outliers)\n",
    "X_val_imputed = imputer.transform(X_val_no_outliers)\n",
    "X_test_imputed = imputer.transform(X_test_no_outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xb00YI81Xutn"
   },
   "outputs": [],
   "source": [
    "feature_engineer = ClinicalFeatureEngineer(\n",
    "    n_top_features=config.TOP_VARIANCE_FEATURES,\n",
    "    n_interactions=config.TOP_INTERACTION_FEATURES\n",
    ")\n",
    "\n",
    "X_train_scaled = feature_engineer.fit_transform(X_train_imputed)\n",
    "X_val_scaled = feature_engineer.transform(X_val_imputed)\n",
    "X_test_scaled = feature_engineer.transform(X_test_imputed)\n",
    "\n",
    "print(f\"\\nFinal shapes after feature engineering:\")\n",
    "print(f\"  Train: {X_train_scaled.shape}\")\n",
    "print(f\"  Val:   {X_val_scaled.shape}\")\n",
    "print(f\"  Test:  {X_test_scaled.shape}\")\n",
    "\n",
    "engineered_feature_names = feature_engineer.feature_info['engineered_features']\n",
    "print(f\"\\nEngineered features ({len(engineered_feature_names)}):\")\n",
    "for feat in engineered_feature_names[:10]:\n",
    "    print(f\"  - {feat}\")\n",
    "if len(engineered_feature_names) > 10:\n",
    "    print(f\"  ... and {len(engineered_feature_names) - 10} more\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U7KEJZUbXutn"
   },
   "outputs": [],
   "source": [
    "scaler = BestPracticeScaler(method='standard')\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train_scaled)\n",
    "X_val_scaled = scaler.transform(X_val_scaled)\n",
    "X_test_scaled = scaler.transform(X_test_scaled)\n",
    "\n",
    "print(f\"Train: {X_train_scaled.shape}\")\n",
    "print(f\"Val:   {X_val_scaled.shape}\")\n",
    "print(f\"Test:  {X_test_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xIKYygloXutn"
   },
   "source": [
    "## Save Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GLCwlZVVXuto"
   },
   "outputs": [],
   "source": [
    "preprocessed_data = {\n",
    "    'X_train': X_train_scaled,\n",
    "    'X_val': X_val_scaled,\n",
    "    'X_test': X_test_scaled,\n",
    "    'y_train': y_train,\n",
    "    'y_val': y_val,\n",
    "    'y_test': y_test,\n",
    "    'ids_train': ids_train,\n",
    "    'ids_val': ids_val,\n",
    "    'ids_test': ids_test,\n",
    "    'feature_names': X_train_scaled.columns.tolist(),\n",
    "    'icu_types_train': icu_types_train,\n",
    "    'icu_types_val': icu_types_val,\n",
    "    'icu_types_test': icu_types_test,\n",
    "}\n",
    "\n",
    "preprocessed_data_path = config.PROCESSED_DIR / \"preprocessed_data.pkl\"\n",
    "with open(preprocessed_data_path, 'wb') as f:\n",
    "    pickle.dump(preprocessed_data, f)\n",
    "\n",
    "preprocessing_artifacts = {\n",
    "    'scaler': scaler,\n",
    "    'imputer': imputer,\n",
    "    'selected_features': X_train_scaled.columns.tolist(),\n",
    "    'icu_column_name': icu_column,\n",
    "}\n",
    "\n",
    "if 'selector' in locals():\n",
    "    preprocessing_artifacts['feature_selector'] = selector\n",
    "\n",
    "artifacts_path = config.PROCESSED_DIR / \"preprocessing_artifacts.pkl\"\n",
    "with open(artifacts_path, 'wb') as f:\n",
    "    pickle.dump(preprocessing_artifacts, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sW6mKpJlXuto"
   },
   "source": [
    "## Data Distribution Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3hA6bEWJXuto"
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=config.FIG_SIZE_LARGE)\n",
    "\n",
    "ax = axes[0, 0]\n",
    "splits = ['Train', 'Val', 'Test']\n",
    "sizes = [len(y_train), len(y_val), len(y_test)]\n",
    "colors = ['#3498db', '#e74c3c', '#2ecc71']\n",
    "ax.bar(splits, sizes, color=colors, alpha=0.7)\n",
    "ax.set_ylabel('Number of Samples')\n",
    "ax.set_title('Dataset Split Sizes')\n",
    "for i, (split, size) in enumerate(zip(splits, sizes)):\n",
    "    ax.text(i, size + 500, f'{size:,}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "ax = axes[0, 1]\n",
    "mortality_rates = [y_train.mean(), y_val.mean(), y_test.mean()]\n",
    "ax.bar(splits, mortality_rates, color=colors, alpha=0.7)\n",
    "ax.set_ylabel('Mortality Rate')\n",
    "ax.set_title('Mortality Rate by Split')\n",
    "ax.set_ylim([0, max(mortality_rates) * 1.3])\n",
    "for i, (split, rate) in enumerate(zip(splits, mortality_rates)):\n",
    "    ax.text(i, rate + 0.005, f'{rate:.2%}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "ax = axes[1, 0]\n",
    "feature_idx = 0\n",
    "feature_name = X_train_scaled.columns[feature_idx]\n",
    "ax.hist(X_train_scaled.iloc[:, feature_idx], bins=50, alpha=0.5, label='Train', color=colors[0])\n",
    "ax.hist(X_val_scaled.iloc[:, feature_idx], bins=50, alpha=0.5, label='Val', color=colors[1])\n",
    "ax.hist(X_test_scaled.iloc[:, feature_idx], bins=50, alpha=0.5, label='Test', color=colors[2])\n",
    "ax.set_xlabel('Scaled Value')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_title(f'Distribution of {feature_name}')\n",
    "ax.legend()\n",
    "\n",
    "ax = axes[1, 1]\n",
    "missing_info = [\n",
    "    ('Initial', X_train.isnull().sum().sum() / X_train.size),\n",
    "    ('After Selection', X_train_selected.isnull().sum().sum() / X_train_selected.size),\n",
    "    ('After Outlier', X_train_no_outliers.isnull().sum().sum() / X_train_no_outliers.size),\n",
    "    ('After MICE', X_train_imputed.isnull().sum().sum() / X_train_imputed.size)\n",
    "]\n",
    "stages, missing_pcts = zip(*missing_info)\n",
    "ax.plot(range(len(stages)), [m * 100 for m in missing_pcts], marker='o', linewidth=2, markersize=8)\n",
    "ax.set_xticks(range(len(stages)))\n",
    "ax.set_xticklabels(stages, rotation=45, ha='right')\n",
    "ax.set_ylabel('Missing Value %')\n",
    "ax.set_title('Missing Values Through Pipeline')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "save_figure(fig, 'preprocessing_overview')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nS1_Oh5mXutp"
   },
   "source": [
    "# PART 2: Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "02nvzJruXutp"
   },
   "source": [
    "## Ablation Study: Impact of Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IXTEcfFhXutp"
   },
   "outputs": [],
   "source": [
    "def quick_baseline_test(\n",
    "    X_train: pd.DataFrame,\n",
    "    y_train: np.ndarray,\n",
    "    X_val: pd.DataFrame,\n",
    "    y_val: np.ndarray,\n",
    "    label: str\n",
    ") -> Dict[str, float]:\n",
    "    model = LogisticRegression(\n",
    "        max_iter=1000,\n",
    "        random_state=config.RANDOM_STATE,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred_proba = model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "    auroc = roc_auc_score(y_val, y_pred_proba)\n",
    "    auprc = average_precision_score(y_val, y_pred_proba)\n",
    "\n",
    "    return {\n",
    "        'label': label,\n",
    "        'n_features': X_train.shape[1],\n",
    "        'auroc': auroc,\n",
    "        'auprc': auprc\n",
    "    }\n",
    "\n",
    "ablation_results = []\n",
    "\n",
    "engineered_prefixes = ['ratio_', 'organ_', 'abnormal_', 'pattern_', 'interact_', '_squared']\n",
    "engineered_cols = [col for col in X_train_scaled.columns\n",
    "                   if any(prefix in col for prefix in engineered_prefixes)]\n",
    "original_cols = [col for col in X_train_scaled.columns if col not in engineered_cols]\n",
    "\n",
    "print(f\"\\nFeature breakdown:\")\n",
    "print(f\"   Original features: {len(original_cols)}\")\n",
    "print(f\"   Engineered features: {len(engineered_cols)}\")\n",
    "print(f\"   Total: {X_train_scaled.shape[1]}\")\n",
    "\n",
    "if len(original_cols) + len(engineered_cols) != X_train_scaled.shape[1]:\n",
    "    print(\"\\nWARNING: Feature separation may be incomplete!\")\n",
    "\n",
    "print(f\"\\n1. Testing: Original features only ({len(original_cols)} features)\")\n",
    "result = quick_baseline_test(\n",
    "    X_train_scaled[original_cols], y_train,\n",
    "    X_val_scaled[original_cols], y_val,\n",
    "    'Original Features Only'\n",
    ")\n",
    "ablation_results.append(result)\n",
    "print(f\"   AUROC: {result['auroc']:.4f}, AUPRC: {result['auprc']:.4f}\")\n",
    "\n",
    "print(f\"\\n2. Testing: Original + Engineered features ({X_train_scaled.shape[1]} features)\")\n",
    "result = quick_baseline_test(\n",
    "    X_train_scaled, y_train,\n",
    "    X_val_scaled, y_val,\n",
    "    'Original + Engineered'\n",
    ")\n",
    "ablation_results.append(result)\n",
    "print(f\"   AUROC: {result['auroc']:.4f}, AUPRC: {result['auprc']:.4f}\")\n",
    "\n",
    "ablation_df = pd.DataFrame(ablation_results)\n",
    "\n",
    "print(\"\\nAblation Study Results:\")\n",
    "print(ablation_df.to_string(index=False))\n",
    "\n",
    "improvement_auroc = ablation_df.iloc[1]['auroc'] - ablation_df.iloc[0]['auroc']\n",
    "improvement_auprc = ablation_df.iloc[1]['auprc'] - ablation_df.iloc[0]['auprc']\n",
    "pct_improvement_auroc = (improvement_auroc / ablation_df.iloc[0]['auroc']) * 100\n",
    "pct_improvement_auprc = (improvement_auprc / ablation_df.iloc[0]['auprc']) * 100\n",
    "\n",
    "print(f\"\\nImprovement from feature engineering:\")\n",
    "print(f\"  AUROC: {improvement_auroc:+.4f} ({pct_improvement_auroc:+.2f}%)\")\n",
    "print(f\"  AUPRC: {improvement_auprc:+.4f} ({pct_improvement_auprc:+.2f}%)\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "axes[0].bar(ablation_df['label'], ablation_df['auroc'], color=['steelblue', 'coral'])\n",
    "axes[0].set_ylabel('AUROC')\n",
    "axes[0].set_title('Feature Engineering Impact on AUROC')\n",
    "axes[0].set_ylim([0.7, 0.9])\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "for i, row in ablation_df.iterrows():\n",
    "    axes[0].text(i, row['auroc'] + 0.005, f\"{row['auroc']:.4f}\", ha='center', fontsize=10)\n",
    "\n",
    "axes[1].bar(ablation_df['label'], ablation_df['auprc'], color=['steelblue', 'coral'])\n",
    "axes[1].set_ylabel('AUPRC')\n",
    "axes[1].set_title('Feature Engineering Impact on AUPRC')\n",
    "axes[1].set_ylim([0.3, 0.5])\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "for i, row in ablation_df.iterrows():\n",
    "    axes[1].text(i, row['auprc'] + 0.005, f\"{row['auprc']:.4f}\", ha='center', fontsize=10)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.savefig(config.BASE_DIR / 'figures' / 'feature_engineering_ablation.png',\n",
    "            dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "ablation_df.to_csv(config.BASE_DIR / 'results' / 'feature_engineering_ablation.csv', index=False)\n",
    "\n",
    "print(f\"\\nEngineered features ({len(engineered_cols)}):\")\n",
    "for feat in sorted(engineered_cols)[:15]:\n",
    "    print(f\"  - {feat}\")\n",
    "if len(engineered_cols) > 15:\n",
    "    print(f\"  ... and {len(engineered_cols) - 15} more\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k22VM3JOXutq"
   },
   "source": [
    "## Save Enhanced Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UxEKQqBnXutr"
   },
   "outputs": [],
   "source": [
    "enhanced_data = {\n",
    "    'X_train': X_train_scaled,\n",
    "    'X_val': X_val_scaled,\n",
    "    'X_test': X_test_scaled,\n",
    "    'y_train': y_train,\n",
    "    'y_val': y_val,\n",
    "    'y_test': y_test,\n",
    "    'feature_names': X_train_scaled.columns.tolist(),\n",
    "    'engineered_feature_names': engineered_feature_names\n",
    "}\n",
    "\n",
    "enhanced_path = config.PROCESSED_DIR / \"enhanced_features.pkl\"\n",
    "with open(enhanced_path, 'wb') as f:\n",
    "    pickle.dump(enhanced_data, f)\n",
    "\n",
    "engineer_path = config.PROCESSED_DIR / \"feature_engineer.pkl\"\n",
    "with open(engineer_path, 'wb') as f:\n",
    "    pickle.dump(feature_engineer, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if processed_dir:\n",
    "   write_manifest(processed_dir, defaults)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os, json, time, pickle, numpy as np\n",
    "\n",
    "class ExperimentDefaults:\n",
    "    random_seed = 42\n",
    "    dp_epsilon_grid = (0.5, 1, 2, 4, 8, 16)\n",
    "    dp_delta = 1e-5\n",
    "    dp_max_grad_norm = 1.0\n",
    "    dp_rounds = 15\n",
    "    dp_local_epochs = 3\n",
    "    fl_rounds = 30\n",
    "    fl_local_epochs = 5\n",
    "    patience = 7\n",
    "    batch_size = 64\n",
    "    results_dir = \"/kaggle/working/results\"\n",
    "    figures_dir = \"/kaggle/working/figures\"\n",
    "    processed_dir = \"/kaggle/working/processed_data_final\"\n",
    "\n",
    "def write_manifest_autodetect(processed_dir: Path, defaults) -> Path:\n",
    "    candidates = [\"preprocessed_data.pkl\", \"enhanced_features.pkl\"]\n",
    "    pkl_path = None\n",
    "    for name in candidates:\n",
    "        cand = processed_dir / name\n",
    "        if cand.exists():\n",
    "            pkl_path = cand\n",
    "            break\n",
    "    if pkl_path is None:\n",
    "        raise FileNotFoundError(f\"No pickle found in {processed_dir}; contents: {os.listdir(processed_dir)}\")\n",
    "\n",
    "    with open(pkl_path, \"rb\") as f:\n",
    "        data = pickle.load(f)\n",
    "\n",
    "    def get_arr(key):\n",
    "        if key in data: return data[key]\n",
    "        alt = f\"{key}_scaled\"\n",
    "        return data.get(alt)\n",
    "\n",
    "    def shape(key):\n",
    "        arr = get_arr(key)\n",
    "        try: return (arr.shape[0], arr.shape[1])\n",
    "        except Exception: return None\n",
    "\n",
    "    def balance(key):\n",
    "        y = data.get(key)\n",
    "        if y is None: return None\n",
    "        y = np.asarray(y).ravel()\n",
    "        pos, neg = int((y==1).sum()), int((y==0).sum())\n",
    "        tot = pos + neg\n",
    "        return {\"pos\": pos, \"neg\": neg, \"pos_pct\": float(pos/tot) if tot else 0.0}\n",
    "\n",
    "    manifest = {\n",
    "        \"generated_at\": int(time.time()),\n",
    "        \"hostname\": os.uname().nodename if hasattr(os, \"uname\") else \"unknown\",\n",
    "        \"defaults\": defaults.__dict__,\n",
    "        \"artifacts\": {\n",
    "            \"source\": str(pkl_path),\n",
    "            \"shapes\": {\n",
    "                \"X_train\": shape(\"X_train\"),\n",
    "                \"X_val\": shape(\"X_val\"),\n",
    "                \"X_test\": shape(\"X_test\"),\n",
    "            },\n",
    "            \"class_balance\": {\n",
    "                \"train\": balance(\"y_train\"),\n",
    "                \"val\": balance(\"y_val\"),\n",
    "                \"test\": balance(\"y_test\"),\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "    out = processed_dir / \"manifest.json\"\n",
    "    with open(out, \"w\") as f:\n",
    "        json.dump(manifest, f, indent=2)\n",
    "    return out\n",
    "\n",
    "processed_dir = Path(\"/kaggle/working/processed_data_final\")\n",
    "out = write_manifest_autodetect(processed_dir, ExperimentDefaults())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uoGfC5Z7Xutr"
   },
   "source": [
    "# PART 3: Baseline Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h57SXWmsXutr"
   },
   "outputs": [],
   "source": [
    "class EnhancedMLP(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,\n",
    "        hidden_dims: List[int] = [256, 128, 64],\n",
    "        dropout: float = 0.3,\n",
    "        use_batch_norm: bool = True\n",
    "    ):\n",
    "        super(EnhancedMLP, self).__init__()\n",
    "\n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "\n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.append(nn.Linear(prev_dim, hidden_dim))\n",
    "\n",
    "            if use_batch_norm:\n",
    "                layers.append(nn.BatchNorm1d(hidden_dim))\n",
    "\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "\n",
    "            prev_dim = hidden_dim\n",
    "\n",
    "        layers.append(nn.Linear(prev_dim, 1))\n",
    "\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.model(x).squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wsibD_WpXuts"
   },
   "outputs": [],
   "source": [
    "class SAINTModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,\n",
    "        d_model: int = 256,\n",
    "        n_heads: int = 8,\n",
    "        n_layers: int = 2,\n",
    "        dropout: float = 0.1,\n",
    "        activation: str = 'gelu'\n",
    "    ):\n",
    "        super(SAINTModel, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.feature_embedding = nn.Linear(input_dim, d_model)\n",
    "        self.positional_encoding = nn.Parameter(torch.randn(1, 1, d_model))\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=n_heads,\n",
    "            dim_feedforward=d_model * 4,\n",
    "            dropout=dropout,\n",
    "            activation=activation,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            encoder_layer,\n",
    "            num_layers=n_layers\n",
    "        )\n",
    "\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc_out = nn.Linear(d_model, 1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.feature_embedding(x)\n",
    "        x = x.unsqueeze(1)\n",
    "\n",
    "        x = x + self.positional_encoding\n",
    "\n",
    "        x = self.transformer_encoder(x)\n",
    "\n",
    "        x = x.squeeze(1)\n",
    "\n",
    "        x = self.layer_norm(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc_out(x)\n",
    "        x = x.squeeze(-1)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UwgSYnzVXuts"
   },
   "outputs": [],
   "source": [
    "class ResNetBlock(nn.Module):\n",
    "    def __init__(self, dim, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Linear(dim, dim),\n",
    "            nn.GroupNorm(8, dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(dim, dim),\n",
    "            nn.GroupNorm(8, dim)\n",
    "        )\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.activation(x + self.block(x))\n",
    "\n",
    "class ClinicalResNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=256, num_blocks=3, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.input_layer = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.blocks = nn.Sequential(\n",
    "            *[ResNetBlock(hidden_dim, dropout) for _ in range(num_blocks)]\n",
    "        )\n",
    "        self.head = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.input_layer(x)\n",
    "        x = self.blocks(x)\n",
    "        return self.head(x).squeeze(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y1zipJIVXuts"
   },
   "source": [
    "## Training Utilities for Deep Learning Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pGN1Cf9uXutt"
   },
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.25, gamma=2.0, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-BCE_loss)\n",
    "        F_loss = self.alpha * (1-pt)**self.gamma * BCE_loss\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return torch.mean(F_loss)\n",
    "        elif self.reduction == 'sum':\n",
    "            return torch.sum(F_loss)\n",
    "        else:\n",
    "            return F_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bUV3NHmlXutt"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from torch.utils.data import DataLoader\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "def compute_pos_weight(y_train: np.ndarray) -> float:\n",
    "    pos = (y_train == 1).sum()\n",
    "    neg = (y_train == 0).sum()\n",
    "    return float(neg / pos) if pos > 0 else 1.0\n",
    "\n",
    "def train_deep_model(\n",
    "    model: nn.Module,\n",
    "    train_loader: DataLoader,\n",
    "    val_loader: DataLoader,\n",
    "    pos_weight: float,\n",
    "    n_epochs: int = 100,\n",
    "    learning_rate: float = 1e-4,\n",
    "    weight_decay: float = 1e-5,\n",
    "    patience: int = 10,\n",
    "    device: str = \"cpu\",\n",
    "):\n",
    "    model = model.to(device)\n",
    "\n",
    "    criterion = FocalLoss(alpha=0.25, gamma=2.0)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode=\"max\", factor=0.5, patience=5, verbose=False\n",
    "    )\n",
    "\n",
    "    history = {\"train_loss\": [], \"val_loss\": [], \"val_auroc\": [], \"val_auprc\": []}\n",
    "    best_val_auroc = 0.0\n",
    "    best_model_state = None\n",
    "    bad_rounds = 0\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device).float()\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(xb)\n",
    "            loss = criterion(logits, yb)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            train_losses.append(loss.item())\n",
    "        avg_train_loss = float(np.mean(train_losses))\n",
    "\n",
    "        model.eval()\n",
    "        val_losses, preds, targets = [], [], []\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in val_loader:\n",
    "                xb, yb = xb.to(device), yb.to(device).float()\n",
    "                logits = model(xb)\n",
    "                loss = criterion(logits, yb)\n",
    "                val_losses.append(loss.item())\n",
    "                probs = torch.sigmoid(logits)\n",
    "                preds.append(probs.cpu())\n",
    "                targets.append(yb.cpu())\n",
    "\n",
    "        preds = torch.cat(preds).numpy()\n",
    "        targets = torch.cat(targets).numpy()\n",
    "        avg_val_loss = float(np.mean(val_losses))\n",
    "        val_auroc = roc_auc_score(targets, preds)\n",
    "        val_auprc = average_precision_score(targets, preds)\n",
    "\n",
    "        history[\"train_loss\"].append(avg_train_loss)\n",
    "        history[\"val_loss\"].append(avg_val_loss)\n",
    "        history[\"val_auroc\"].append(val_auroc)\n",
    "        history[\"val_auprc\"].append(val_auprc)\n",
    "\n",
    "        scheduler.step(val_auroc)\n",
    "\n",
    "        improved = val_auroc > best_val_auroc\n",
    "        if improved:\n",
    "            best_val_auroc = val_auroc\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            bad_rounds = 0\n",
    "        else:\n",
    "            bad_rounds += 1\n",
    "\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(\n",
    "                f\"Epoch {epoch + 1}/{n_epochs} | \"\n",
    "                f\"Train Loss: {avg_train_loss:.4f} | \"\n",
    "                f\"Val Loss: {avg_val_loss:.4f} | \"\n",
    "                f\"Val AUROC: {val_auroc:.4f}\"\n",
    "            )\n",
    "\n",
    "        if bad_rounds >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "            break\n",
    "\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "\n",
    "    print(f\"Training complete. Best Val AUROC: {best_val_auroc:.4f}\")\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DidsytEPXutu"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from typing import Tuple, Union\n",
    "\n",
    "\n",
    "def evaluate_model(\n",
    "    model: torch.nn.Module,\n",
    "    dataloader: torch.utils.data.DataLoader,\n",
    "    device: Union[torch.device, str],\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    model.eval()\n",
    "    all_probs = []\n",
    "    all_labels = []\n",
    "\n",
    "    device_obj = device if isinstance(device, torch.device) else torch.device(device)\n",
    "    device_type = device_obj.type\n",
    "\n",
    "    with torch.no_grad(), torch.amp.autocast(\n",
    "        device_type=device_type,\n",
    "        enabled=(device_type == \"cuda\")\n",
    "    ):\n",
    "        for batch_x, batch_y in dataloader:\n",
    "            batch_x = batch_x.to(device_obj, non_blocking=True)\n",
    "            batch_y = batch_y.to(device_obj, non_blocking=True)\n",
    "\n",
    "            logits = model(batch_x)\n",
    "            probs = torch.sigmoid(logits.squeeze(-1))\n",
    "\n",
    "            all_probs.append(probs.cpu().numpy())\n",
    "            all_labels.append(batch_y.cpu().numpy())\n",
    "\n",
    "    return np.concatenate(all_probs), np.concatenate(all_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m2CHp3FwXutv"
   },
   "source": [
    "## Cross-Validation and Hyperparameter Optimization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PEeWv6MZXutv"
   },
   "outputs": [],
   "source": [
    "def cross_validate_sklearn_model(\n",
    "    model_class,\n",
    "    model_params: Dict[str, Any],\n",
    "    X: pd.DataFrame,\n",
    "    y: np.ndarray,\n",
    "    cv_folds: int = 5\n",
    ") -> Tuple[float, float, List[float]]:\n",
    "    cv = StratifiedKFold(\n",
    "        n_splits=cv_folds,\n",
    "        shuffle=True,\n",
    "        random_state=config.RANDOM_STATE\n",
    "    )\n",
    "\n",
    "    scores = []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(cv.split(X, y)):\n",
    "        X_train_cv = X.iloc[train_idx]\n",
    "        y_train_cv = y[train_idx]\n",
    "        X_val_cv = X.iloc[val_idx]\n",
    "        y_val_cv = y[val_idx]\n",
    "\n",
    "        model = model_class(**model_params)\n",
    "        model.fit(X_train_cv, y_train_cv)\n",
    "\n",
    "        y_pred_proba = model.predict_proba(X_val_cv)[:, 1]\n",
    "        auroc = roc_auc_score(y_val_cv, y_pred_proba)\n",
    "        scores.append(auroc)\n",
    "\n",
    "    return np.mean(scores), np.std(scores), scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qmtcV6_AXutv"
   },
   "outputs": [],
   "source": [
    "def objective_xgboost(trial, X, y):\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 15),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.3, log=True),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.4, 1.0),\n",
    "        'subsample': trial.suggest_float('subsample', 0.4, 1.0),\n",
    "        'gamma': trial.suggest_float('gamma', 0, 10),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0, 20),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0, 20),\n",
    "        'scale_pos_weight': trial.suggest_float('scale_pos_weight', 1.0, 15.0),\n",
    "        'random_state': config.RANDOM_STATE,\n",
    "        'tree_method': 'hist',\n",
    "        'eval_metric': 'logloss'\n",
    "    }\n",
    "\n",
    "    mean_score, _, _ = cross_validate_sklearn_model(\n",
    "        xgb.XGBClassifier, params, X, y, cv_folds=config.CV_FOLDS\n",
    "    )\n",
    "\n",
    "    return mean_score\n",
    "\n",
    "def objective_lightgbm(trial, X, y):\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 15),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.3, log=True),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 20, 200),\n",
    "        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 10, 100),\n",
    "        'feature_fraction': trial.suggest_float('feature_fraction', 0.4, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.4, 1.0),\n",
    "        'subsample': trial.suggest_float('subsample', 0.4, 1.0),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0, 20),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0, 20),\n",
    "        'scale_pos_weight': trial.suggest_float('scale_pos_weight', 1.0, 15.0),\n",
    "        'random_state': config.RANDOM_STATE,\n",
    "        'verbose': -1\n",
    "    }\n",
    "\n",
    "    mean_score, _, _ = cross_validate_sklearn_model(\n",
    "        lgb.LGBMClassifier, params, X, y, cv_folds=config.CV_FOLDS\n",
    "    )\n",
    "\n",
    "    return mean_score\n",
    "\n",
    "def objective_randomforest(trial, X, y):\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n",
    "        'max_depth': trial.suggest_int('max_depth', 5, 30),\n",
    "        'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),\n",
    "        'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2', 0.5, 0.7, 0.9]),\n",
    "        'random_state': config.RANDOM_STATE\n",
    "    }\n",
    "\n",
    "    mean_score, _, _ = cross_validate_sklearn_model(\n",
    "        RandomForestClassifier, params, X, y, cv_folds=config.CV_FOLDS\n",
    "    )\n",
    "\n",
    "    return mean_score\n",
    "\n",
    "def objective_logistic(trial, X, y):\n",
    "    params = {\n",
    "        'C': trial.suggest_float('C', 1e-4, 100, log=True),\n",
    "        'penalty': trial.suggest_categorical('penalty', ['l1', 'l2', 'elasticnet']),\n",
    "        'solver': 'saga',\n",
    "        'l1_ratio': trial.suggest_float('l1_ratio', 0, 1) if trial.params.get('penalty') == 'elasticnet' else None,\n",
    "        'max_iter': 1000,\n",
    "        'random_state': config.RANDOM_STATE\n",
    "    }\n",
    "\n",
    "    if params['l1_ratio'] is None:\n",
    "        del params['l1_ratio']\n",
    "\n",
    "    mean_score, _, _ = cross_validate_sklearn_model(\n",
    "        LogisticRegression, params, X, y, cv_folds=config.CV_FOLDS\n",
    "    )\n",
    "\n",
    "    return mean_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ejxTKcOXutw"
   },
   "source": [
    "## Load Enhanced Features from Part 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7lJTjAgXXutw"
   },
   "outputs": [],
   "source": [
    "enhanced_path = config.PROCESSED_DIR / \"enhanced_features.pkl\"\n",
    "\n",
    "if not enhanced_path.exists():\n",
    "    raise FileNotFoundError(f\"Enhanced features not found at {enhanced_path}. Please run Part 2 first.\")\n",
    "\n",
    "with open(enhanced_path, 'rb') as f:\n",
    "    enhanced_data = pickle.load(f)\n",
    "\n",
    "X_train_scaled = enhanced_data['X_train']\n",
    "X_val_scaled = enhanced_data['X_val']\n",
    "X_test_scaled = enhanced_data['X_test']\n",
    "y_train = enhanced_data['y_train']\n",
    "y_val = enhanced_data['y_val']\n",
    "y_test = enhanced_data['y_test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7RXaKOOkXutw"
   },
   "source": [
    "## Train Baseline ML Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wxMP_mUHXutx"
   },
   "outputs": [],
   "source": [
    "baseline_models = {}\n",
    "baseline_results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tp6tKj8aXutx"
   },
   "outputs": [],
   "source": [
    "# 1. Initialize and Train\n",
    "lr_model = LogisticRegression(\n",
    "    max_iter=1000,\n",
    "    random_state=config.RANDOM_STATE,\n",
    "    n_jobs=-1,\n",
    "    class_weight='balanced'\n",
    ")\n",
    "\n",
    "lr_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "val_probs_lr = lr_model.predict_proba(X_val_scaled)[:, 1]\n",
    "best_thresh_lr, best_f1_lr = get_optimal_threshold(y_val, val_probs_lr)\n",
    "\n",
    "print(f\"Logistic Regression - Optimal Threshold: {best_thresh_lr:.4f}\")\n",
    "print(f\"Logistic Regression - Best Validation F1: {best_f1_lr:.4f}\")\n",
    "\n",
    "# 3. Apply Optimal Threshold to TEST set\n",
    "y_test_proba_lr = lr_model.predict_proba(X_test_scaled)[:, 1]\n",
    "y_test_pred_lr = (y_test_proba_lr >= best_thresh_lr).astype(int)\n",
    "\n",
    "metrics_lr = compute_metrics_with_ci(y_test, y_test_pred_lr, y_test_proba_lr, n_bootstraps=config.N_BOOTSTRAP)\n",
    "print_metrics_with_ci(metrics_lr, \"Logistic Regression Test Results\")\n",
    "\n",
    "baseline_models['Logistic Regression'] = lr_model\n",
    "baseline_results['Logistic Regression'] = {\n",
    "    'predictions': y_test_pred_lr,\n",
    "    'probabilities': y_test_proba_lr,\n",
    "    'metrics': metrics_lr,\n",
    "    'optimal_threshold': best_thresh_lr\n",
    "}\n",
    "\n",
    "with open(config.MODELS_DIR / \"logistic_regression.pkl\", 'wb') as f:\n",
    "    pickle.dump(lr_model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yKATlrrVXutx"
   },
   "outputs": [],
   "source": [
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=config.RANDOM_STATE,\n",
    "    n_jobs=-1,\n",
    "    class_weight='balanced'\n",
    ")\n",
    "\n",
    "rf_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "val_probs_rf = rf_model.predict_proba(X_val_scaled)[:, 1]\n",
    "best_thresh_rf, best_f1_rf = get_optimal_threshold(y_val, val_probs_rf)\n",
    "\n",
    "print(f\"Random Forest - Optimal Threshold: {best_thresh_rf:.4f}\")\n",
    "print(f\"Random Forest - Best Validation F1: {best_f1_rf:.4f}\")\n",
    "\n",
    "y_test_proba_rf = rf_model.predict_proba(X_test_scaled)[:, 1]\n",
    "y_test_pred_rf = (y_test_proba_rf >= best_thresh_rf).astype(int)\n",
    "\n",
    "metrics_rf = compute_metrics_with_ci(y_test, y_test_pred_rf, y_test_proba_rf, n_bootstraps=config.N_BOOTSTRAP)\n",
    "print_metrics_with_ci(metrics_rf, \"Random Forest Test Results\")\n",
    "\n",
    "baseline_models['Random Forest'] = rf_model\n",
    "baseline_results['Random Forest'] = {\n",
    "    'predictions': y_test_pred_rf,\n",
    "    'probabilities': y_test_proba_rf,\n",
    "    'metrics': metrics_rf,\n",
    "    'optimal_threshold': best_thresh_rf\n",
    "}\n",
    "\n",
    "with open(config.MODELS_DIR / \"random_forest.pkl\", 'wb') as f:\n",
    "    pickle.dump(rf_model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u8_XEKrgXutx"
   },
   "outputs": [],
   "source": [
    "study_xgb = optuna.create_study(\n",
    "    direction='maximize',\n",
    "    sampler=TPESampler(seed=config.RANDOM_STATE)\n",
    ")\n",
    "\n",
    "study_xgb.optimize(\n",
    "    lambda trial: objective_xgboost(trial, X_train_scaled, y_train),\n",
    "    n_trials=config.OPTUNA_N_TRIALS,\n",
    "    timeout=config.OPTUNA_TIMEOUT,\n",
    "    show_progress_bar=True\n",
    ")\n",
    "\n",
    "print(f\"Best AUROC (CV): {study_xgb.best_value:.4f}\")\n",
    "print(f\"Best parameters: {study_xgb.best_params}\")\n",
    "\n",
    "xgb_best_params = study_xgb.best_params.copy()\n",
    "xgb_best_params.update({\n",
    "    'random_state': config.RANDOM_STATE,\n",
    "    'tree_method': 'hist',\n",
    "    'eval_metric': 'logloss'\n",
    "})\n",
    "\n",
    "xgb_model = xgb.XGBClassifier(**xgb_best_params)\n",
    "xgb_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_test_pred_xgb = xgb_model.predict(X_test_scaled)\n",
    "y_test_proba_xgb = xgb_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "metrics_xgb = compute_metrics_with_ci(y_test, y_test_pred_xgb, y_test_proba_xgb, n_bootstraps=config.N_BOOTSTRAP)\n",
    "print_metrics_with_ci(metrics_xgb, \"XGBoost Test Results\")\n",
    "\n",
    "baseline_models['XGBoost'] = xgb_model\n",
    "baseline_results['XGBoost'] = {\n",
    "    'predictions': y_test_pred_xgb,\n",
    "    'probabilities': y_test_proba_xgb,\n",
    "    'metrics': metrics_xgb,\n",
    "    'best_params': study_xgb.best_params,\n",
    "    'cv_auroc': study_xgb.best_value\n",
    "}\n",
    "\n",
    "with open(config.MODELS_DIR / \"xgboost_model.pkl\", 'wb') as f:\n",
    "    pickle.dump(xgb_model, f)\n",
    "with open(config.MODELS_DIR / \"xgboost_study.pkl\", 'wb') as f:\n",
    "    pickle.dump(study_xgb, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6idYd1cnXuty"
   },
   "outputs": [],
   "source": [
    "xgb_best_params = {\n",
    "    'n_estimators': 944,\n",
    "    'max_depth': 13,\n",
    "    'learning_rate': 0.0162832534520785,\n",
    "    'min_child_weight': 6,\n",
    "    'colsample_bytree': 0.40160892521374203,\n",
    "    'subsample': 0.47574287837455026,\n",
    "    'gamma': 4.775973089877698,\n",
    "    'reg_alpha': 8.985970884575682,\n",
    "    'reg_lambda': 8.608624997371484,\n",
    "    'scale_pos_weight': 3.084756766110043,\n",
    "    # Add standard config\n",
    "    'random_state': config.RANDOM_STATE,\n",
    "    'tree_method': 'hist',\n",
    "    'eval_metric': 'logloss'\n",
    "}\n",
    "\n",
    "print(\"Skipping Optuna. Training XGBoost with best parameters from previous run...\")\n",
    "\n",
    "xgb_model = xgb.XGBClassifier(**xgb_best_params)\n",
    "xgb_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "val_probs_xgb = xgb_model.predict_proba(X_val_scaled)[:, 1]\n",
    "best_thresh_xgb, best_f1_xgb = get_optimal_threshold(y_val, val_probs_xgb)\n",
    "\n",
    "print(f\"XGBoost - Optimal Threshold: {best_thresh_xgb:.4f}\")\n",
    "print(f\"XGBoost - Best Validation F1: {best_f1_xgb:.4f}\")\n",
    "\n",
    "y_test_proba_xgb = xgb_model.predict_proba(X_test_scaled)[:, 1]\n",
    "y_test_pred_xgb = (y_test_proba_xgb >= best_thresh_xgb).astype(int)\n",
    "\n",
    "metrics_xgb = compute_metrics_with_ci(y_test, y_test_pred_xgb, y_test_proba_xgb, n_bootstraps=config.N_BOOTSTRAP)\n",
    "print_metrics_with_ci(metrics_xgb, \"XGBoost Test Results\")\n",
    "\n",
    "baseline_models['XGBoost'] = xgb_model\n",
    "baseline_results['XGBoost'] = {\n",
    "    'predictions': y_test_pred_xgb,\n",
    "    'probabilities': y_test_proba_xgb,\n",
    "    'metrics': metrics_xgb,\n",
    "    'best_params': xgb_best_params,\n",
    "    'optimal_threshold': best_thresh_xgb\n",
    "}\n",
    "\n",
    "with open(config.MODELS_DIR / \"xgboost_model.pkl\", 'wb') as f:\n",
    "    pickle.dump(xgb_model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XzH4BVOmXuty"
   },
   "outputs": [],
   "source": [
    "study_lgb = optuna.create_study(\n",
    "    direction='maximize',\n",
    "    sampler=TPESampler(seed=config.RANDOM_STATE)\n",
    ")\n",
    "\n",
    "study_lgb.optimize(\n",
    "    lambda trial: objective_lightgbm(trial, X_train_scaled, y_train),\n",
    "    n_trials=config.OPTUNA_N_TRIALS,\n",
    "    timeout=config.OPTUNA_TIMEOUT,\n",
    "    show_progress_bar=True\n",
    ")\n",
    "\n",
    "print(f\"Best AUROC (CV): {study_lgb.best_value:.4f}\")\n",
    "print(f\"Best parameters: {study_lgb.best_params}\")\n",
    "\n",
    "lgb_best_params = study_lgb.best_params.copy()\n",
    "lgb_best_params.update({\n",
    "    'random_state': config.RANDOM_STATE,\n",
    "    'verbose': -1\n",
    "})\n",
    "\n",
    "lgb_model = lgb.LGBMClassifier(**lgb_best_params)\n",
    "lgb_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_test_pred_lgb = lgb_model.predict(X_test_scaled)\n",
    "y_test_proba_lgb = lgb_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "metrics_lgb = compute_metrics_with_ci(y_test, y_test_pred_lgb, y_test_proba_lgb, n_bootstraps=config.N_BOOTSTRAP)\n",
    "print_metrics_with_ci(metrics_lgb, \"LightGBM Test Results\")\n",
    "\n",
    "baseline_models['LightGBM'] = lgb_model\n",
    "baseline_results['LightGBM'] = {\n",
    "    'predictions': y_test_pred_lgb,\n",
    "    'probabilities': y_test_proba_lgb,\n",
    "    'metrics': metrics_lgb,\n",
    "    'best_params': study_lgb.best_params,\n",
    "    'cv_auroc': study_lgb.best_value\n",
    "}\n",
    "\n",
    "with open(config.MODELS_DIR / \"lightgbm_model.pkl\", 'wb') as f:\n",
    "    pickle.dump(lgb_model, f)\n",
    "with open(config.MODELS_DIR / \"lightgbm_study.pkl\", 'wb') as f:\n",
    "    pickle.dump(study_lgb, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jnNJ8rGDXutz"
   },
   "outputs": [],
   "source": [
    "lgb_best_params = {\n",
    "    'n_estimators': 961,\n",
    "    'max_depth': 13,\n",
    "    'learning_rate': 0.009692970163443861,\n",
    "    'num_leaves': 162,\n",
    "    'min_data_in_leaf': 51,\n",
    "    'feature_fraction': 0.4513246305977976,\n",
    "    'colsample_bytree': 0.600983489371053,\n",
    "    'subsample': 0.8629834144981184,\n",
    "    'reg_alpha': 1.4598070154359528,\n",
    "    'reg_lambda': 13.236885590293403,\n",
    "    'scale_pos_weight': 4.217331563736673,\n",
    "    'random_state': config.RANDOM_STATE,\n",
    "    'verbose': -1\n",
    "}\n",
    "\n",
    "print(\"Skipping Optuna. Training LightGBM with best parameters from previous run...\")\n",
    "\n",
    "lgb_model = lgb.LGBMClassifier(**lgb_best_params)\n",
    "lgb_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "val_probs_lgb = lgb_model.predict_proba(X_val_scaled)[:, 1]\n",
    "best_thresh_lgb, best_f1_lgb = get_optimal_threshold(y_val, val_probs_lgb)\n",
    "\n",
    "print(f\"LightGBM - Optimal Threshold: {best_thresh_lgb:.4f}\")\n",
    "print(f\"LightGBM - Best Validation F1: {best_f1_lgb:.4f}\")\n",
    "\n",
    "y_test_proba_lgb = lgb_model.predict_proba(X_test_scaled)[:, 1]\n",
    "y_test_pred_lgb = (y_test_proba_lgb >= best_thresh_lgb).astype(int)\n",
    "\n",
    "metrics_lgb = compute_metrics_with_ci(y_test, y_test_pred_lgb, y_test_proba_lgb, n_bootstraps=config.N_BOOTSTRAP)\n",
    "print_metrics_with_ci(metrics_lgb, \"LightGBM Test Results\")\n",
    "\n",
    "baseline_models['LightGBM'] = lgb_model\n",
    "baseline_results['LightGBM'] = {\n",
    "    'predictions': y_test_pred_lgb,\n",
    "    'probabilities': y_test_proba_lgb,\n",
    "    'metrics': metrics_lgb,\n",
    "    'best_params': lgb_best_params,\n",
    "    'optimal_threshold': best_thresh_lgb\n",
    "}\n",
    "\n",
    "with open(config.MODELS_DIR / \"lightgbm_model.pkl\", 'wb') as f:\n",
    "    pickle.dump(lgb_model, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PUQuQrzDXutz"
   },
   "source": [
    "## Train Deep Learning Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hUWed5XAXutz"
   },
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "train_dataset = TensorDataset(\n",
    "    torch.FloatTensor(X_train_scaled.values),\n",
    "    torch.FloatTensor(y_train)\n",
    ")\n",
    "val_dataset = TensorDataset(\n",
    "    torch.FloatTensor(X_val_scaled.values),\n",
    "    torch.FloatTensor(y_val)\n",
    ")\n",
    "test_dataset = TensorDataset(\n",
    "    torch.FloatTensor(X_test_scaled.values),\n",
    "    torch.FloatTensor(y_test)\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=config.BATCH_SIZE,\n",
    "    shuffle=True, worker_init_fn=worker_init_fn\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, batch_size=config.BATCH_SIZE, shuffle=False\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, batch_size=config.BATCH_SIZE, shuffle=False\n",
    ")\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Val batches: {len(val_loader)}\")\n",
    "print(f\"Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bAq0BLHxXutz"
   },
   "outputs": [],
   "source": [
    "pos_weight = compute_pos_weight(y_train)\n",
    "\n",
    "mlp_params = {\n",
    "    \"input_dim\": X_train_scaled.shape[1],\n",
    "    \"hidden_dims\": [256, 128, 64],\n",
    "    \"dropout\": 0.3,\n",
    "    \"use_batch_norm\": True,\n",
    "}\n",
    "mlp_model = EnhancedMLP(**mlp_params)\n",
    "learning_rate = 1e-4\n",
    "\n",
    "history_mlp = train_deep_model(\n",
    "    mlp_model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    pos_weight=pos_weight,\n",
    "    n_epochs=config.MAX_EPOCHS,\n",
    "    learning_rate=learning_rate,\n",
    "    patience=config.EARLY_STOPPING_PATIENCE,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "val_probs_mlp, val_true_mlp = evaluate_model(mlp_model, val_loader, device=device)\n",
    "best_thresh_mlp, best_f1_mlp = get_optimal_threshold(val_true_mlp, val_probs_mlp)\n",
    "\n",
    "print(f\"Enhanced MLP - Optimal Threshold: {best_thresh_mlp:.4f}\")\n",
    "print(f\"Enhanced MLP - Best Validation F1: {best_f1_mlp:.4f}\")\n",
    "\n",
    "y_test_proba_mlp, y_test_true_mlp = evaluate_model(mlp_model, test_loader, device=device)\n",
    "y_test_pred_mlp = (y_test_proba_mlp >= best_thresh_mlp).astype(int)\n",
    "\n",
    "metrics_mlp = compute_metrics_with_ci(\n",
    "    y_test_true_mlp, y_test_pred_mlp, y_test_proba_mlp, n_bootstraps=config.N_BOOTSTRAP\n",
    ")\n",
    "print_metrics_with_ci(metrics_mlp, \"Enhanced MLP Test Results\")\n",
    "\n",
    "baseline_models[\"Enhanced MLP\"] = mlp_model\n",
    "baseline_results[\"Enhanced MLP\"] = {\n",
    "    \"predictions\": y_test_pred_mlp,\n",
    "    \"probabilities\": y_test_proba_mlp,\n",
    "    \"metrics\": metrics_mlp,\n",
    "    \"history\": history_mlp,\n",
    "    \"optimal_threshold\": best_thresh_mlp\n",
    "}\n",
    "\n",
    "torch.save(mlp_model.state_dict(), config.MODELS_DIR / \"enhanced_mlp.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c63Z2B0BXut0"
   },
   "outputs": [],
   "source": [
    "saint_params = {\n",
    "    \"input_dim\": X_train_scaled.shape[1],\n",
    "    \"d_model\": config.SAINT_PARAMS[\"d_model\"],\n",
    "    \"n_heads\": config.SAINT_PARAMS[\"n_heads\"],\n",
    "    \"n_layers\": config.SAINT_PARAMS[\"n_layers\"],\n",
    "    \"dropout\": config.SAINT_PARAMS[\"dropout\"],\n",
    "    \"activation\": \"gelu\",\n",
    "}\n",
    "saint_model = SAINTModel(**saint_params)\n",
    "learning_rate = config.SAINT_PARAMS[\"lr\"]\n",
    "\n",
    "history_saint = train_deep_model(\n",
    "    saint_model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    pos_weight=pos_weight,\n",
    "    n_epochs=config.MAX_EPOCHS,\n",
    "    learning_rate=learning_rate,\n",
    "    patience=config.EARLY_STOPPING_PATIENCE,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "val_probs_saint, val_true_saint = evaluate_model(saint_model, val_loader, device=device)\n",
    "best_thresh_saint, best_f1_saint = get_optimal_threshold(val_true_saint, val_probs_saint)\n",
    "\n",
    "print(f\"SAINT - Optimal Threshold: {best_thresh_saint:.4f}\")\n",
    "print(f\"SAINT - Best Validation F1: {best_f1_saint:.4f}\")\n",
    "\n",
    "y_test_proba_saint, y_test_true_saint = evaluate_model(saint_model, test_loader, device=device)\n",
    "y_test_pred_saint = (y_test_proba_saint >= best_thresh_saint).astype(int)\n",
    "\n",
    "metrics_saint = compute_metrics_with_ci(\n",
    "    y_test_true_saint, y_test_pred_saint, y_test_proba_saint, n_bootstraps=config.N_BOOTSTRAP\n",
    ")\n",
    "print_metrics_with_ci(metrics_saint, \"SAINT Test Results\")\n",
    "\n",
    "baseline_models[\"SAINT\"] = saint_model\n",
    "baseline_results[\"SAINT\"] = {\n",
    "    \"predictions\": y_test_pred_saint,\n",
    "    \"probabilities\": y_test_proba_saint,\n",
    "    \"metrics\": metrics_saint,\n",
    "    \"history\": history_saint,\n",
    "    \"optimal_threshold\": best_thresh_saint\n",
    "}\n",
    "\n",
    "torch.save(saint_model.state_dict(), config.MODELS_DIR / \"saint_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R-eep2iOXut0"
   },
   "outputs": [],
   "source": [
    "print(\"Training Clinical ResNet...\")\n",
    "\n",
    "resnet_params = {\n",
    "    \"input_dim\": X_train_scaled.shape[1],\n",
    "    \"hidden_dim\": 128,\n",
    "    \"num_blocks\": 2,\n",
    "    \"dropout\": 0.4\n",
    "}\n",
    "resnet_model = ClinicalResNet(**resnet_params)\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    resnet_model.parameters(),\n",
    "    lr=3e-4,\n",
    "    weight_decay=1e-2\n",
    ")\n",
    "\n",
    "resnet_model = resnet_model.to(device)\n",
    "criterion = FocalLoss(alpha=0.25, gamma=2.0)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode=\"max\", factor=0.5, patience=5, verbose=True\n",
    ")\n",
    "\n",
    "best_val_auroc = 0.0\n",
    "best_model_state = None\n",
    "patience = 15\n",
    "bad_rounds = 0\n",
    "history_resnet = {\"val_auroc\": []}\n",
    "\n",
    "for epoch in range(100):\n",
    "    resnet_model.train()\n",
    "    for xb, yb in train_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device).float()\n",
    "        optimizer.zero_grad()\n",
    "        logits = resnet_model(xb)\n",
    "        loss = criterion(logits, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    val_probs, val_true = evaluate_model(resnet_model, val_loader, device)\n",
    "    val_auroc = roc_auc_score(val_true, val_probs)\n",
    "    history_resnet[\"val_auroc\"].append(val_auroc)\n",
    "\n",
    "    scheduler.step(val_auroc)\n",
    "\n",
    "    if val_auroc > best_val_auroc:\n",
    "        best_val_auroc = val_auroc\n",
    "        best_model_state = resnet_model.state_dict().copy()\n",
    "        bad_rounds = 0\n",
    "    else:\n",
    "        bad_rounds += 1\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch + 1} | Val AUROC: {val_auroc:.4f}\")\n",
    "\n",
    "    if bad_rounds >= patience:\n",
    "        print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "        break\n",
    "\n",
    "if best_model_state:\n",
    "    resnet_model.load_state_dict(best_model_state)\n",
    "\n",
    "print(f\"Training complete. Best Val AUROC: {best_val_auroc:.4f}\")\n",
    "\n",
    "val_probs, val_true = evaluate_model(resnet_model, val_loader, device)\n",
    "best_thresh_rn, best_f1_rn = get_optimal_threshold(val_true, val_probs)\n",
    "\n",
    "print(f\"Clinical ResNet - Optimal Threshold: {best_thresh_rn:.4f}\")\n",
    "print(f\"Clinical ResNet - Best Validation F1: {best_f1_rn:.4f}\")\n",
    "\n",
    "y_test_proba_rn, y_test_true_rn = evaluate_model(resnet_model, test_loader, device)\n",
    "y_test_pred_rn = (y_test_proba_rn >= best_thresh_rn).astype(int)\n",
    "\n",
    "metrics_rn = compute_metrics_with_ci(\n",
    "    y_test_true_rn, y_test_pred_rn, y_test_proba_rn, n_bootstraps=config.N_BOOTSTRAP\n",
    ")\n",
    "print_metrics_with_ci(metrics_rn, \"Clinical ResNet Test Results\")\n",
    "\n",
    "baseline_models[\"ClinicalResNet\"] = resnet_model\n",
    "baseline_results[\"ClinicalResNet\"] = {\n",
    "    \"predictions\": y_test_pred_rn,\n",
    "    \"probabilities\": y_test_proba_rn,\n",
    "    \"metrics\": metrics_rn,\n",
    "    \"optimal_threshold\": best_thresh_rn\n",
    "}\n",
    "torch.save(resnet_model.state_dict(), config.MODELS_DIR / \"resnet_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tBu4DgZZXut0"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "\n",
    "def make_loaders(Xtr, ytr, Xv, yv, batch_size):\n",
    "    train_ds = TensorDataset(\n",
    "        torch.tensor(Xtr.values, dtype=torch.float32),\n",
    "        torch.tensor(ytr, dtype=torch.float32),\n",
    "    )\n",
    "    val_ds = TensorDataset(\n",
    "        torch.tensor(Xv.values, dtype=torch.float32),\n",
    "        torch.tensor(yv, dtype=torch.float32),\n",
    "    )\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=batch_size * 2, shuffle=False)\n",
    "    return train_loader, val_loader\n",
    "\n",
    "def run_mlp_direct():\n",
    "    print(\"Training Enhanced MLP with Best Parameters (Skipping Optuna)...\")\n",
    "\n",
    "    best_params = {\n",
    "        'n_layers': 3,\n",
    "        'h0': 451, 'h1': 349, 'h2': 132,\n",
    "        'dropout': 0.446,\n",
    "        'lr': 9.61e-05,\n",
    "        'weight_decay': 0.000755,\n",
    "        'batch_size': 64\n",
    "    }\n",
    "\n",
    "    pos_weight = compute_pos_weight(y_train)\n",
    "\n",
    "    hidden_dims = [best_params[f\"h{i}\"] for i in range(best_params[\"n_layers\"])]\n",
    "    mlp_model = EnhancedMLP(\n",
    "        input_dim=X_train_scaled.shape[1],\n",
    "        hidden_dims=hidden_dims,\n",
    "        dropout=best_params[\"dropout\"],\n",
    "    )\n",
    "\n",
    "    train_loader, val_loader = make_loaders(X_train_scaled, y_train, X_val_scaled, y_val, best_params[\"batch_size\"])\n",
    "\n",
    "    history_mlp = train_deep_model(\n",
    "        mlp_model,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        pos_weight=pos_weight,\n",
    "        n_epochs=config.MAX_EPOCHS,\n",
    "        learning_rate=best_params[\"lr\"],\n",
    "        weight_decay=best_params[\"weight_decay\"],\n",
    "        patience=config.EARLY_STOPPING_PATIENCE,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    val_probs, val_true = evaluate_model(mlp_model, val_loader, device=device)\n",
    "    best_thresh, _ = get_optimal_threshold(val_true, val_probs)\n",
    "    print(f\"Enhanced MLP - Optimal Threshold: {best_thresh:.4f}\")\n",
    "\n",
    "    y_test_proba, y_test_true = evaluate_model(mlp_model, test_loader, device=device)\n",
    "    y_test_pred = (y_test_proba >= best_thresh).astype(int)\n",
    "\n",
    "    metrics_mlp = compute_metrics_with_ci(\n",
    "        y_test_true, y_test_pred, y_test_proba, n_bootstraps=config.N_BOOTSTRAP\n",
    "    )\n",
    "    print_metrics_with_ci(metrics_mlp, \"Enhanced MLP Test Results\")\n",
    "\n",
    "    baseline_models[\"Enhanced MLP\"] = mlp_model\n",
    "    baseline_results[\"Enhanced MLP\"] = {\n",
    "        \"predictions\": y_test_pred,\n",
    "        \"probabilities\": y_test_proba,\n",
    "        \"metrics\": metrics_mlp,\n",
    "        \"history\": history_mlp,\n",
    "        \"optimal_threshold\": best_thresh\n",
    "    }\n",
    "    torch.save(mlp_model.state_dict(), config.MODELS_DIR / \"enhanced_mlp_optimized.pt\")\n",
    "    print(\"Enhanced MLP Saved!\")\n",
    "\n",
    "def run_saint_direct():\n",
    "    print(\"\\nTraining SAINT with Best Parameters (Skipping Optuna)...\")\n",
    "\n",
    "    best_params = {\n",
    "        'd_model': 256,\n",
    "        'n_heads': 4,\n",
    "        'n_layers': 2,\n",
    "        'dropout': 0.125,\n",
    "        'lr': 2.59e-05,\n",
    "        'weight_decay': 1.59e-06,\n",
    "        'batch_size': 64\n",
    "    }\n",
    "\n",
    "    pos_weight = compute_pos_weight(y_train)\n",
    "\n",
    "    saint_model = SAINTModel(\n",
    "        input_dim=X_train_scaled.shape[1],\n",
    "        d_model=best_params[\"d_model\"],\n",
    "        n_heads=best_params[\"n_heads\"],\n",
    "        n_layers=best_params[\"n_layers\"],\n",
    "        dropout=best_params[\"dropout\"],\n",
    "        activation=\"gelu\",\n",
    "    )\n",
    "\n",
    "    train_loader, val_loader = make_loaders(X_train_scaled, y_train, X_val_scaled, y_val, best_params[\"batch_size\"])\n",
    "\n",
    "    history_saint = train_deep_model(\n",
    "        saint_model,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        pos_weight=pos_weight,\n",
    "        n_epochs=config.MAX_EPOCHS,\n",
    "        learning_rate=best_params[\"lr\"],\n",
    "        weight_decay=best_params[\"weight_decay\"],\n",
    "        patience=config.EARLY_STOPPING_PATIENCE,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    val_probs, val_true = evaluate_model(saint_model, val_loader, device=device)\n",
    "    best_thresh, _ = get_optimal_threshold(val_true, val_probs)\n",
    "    print(f\"SAINT - Optimal Threshold: {best_thresh:.4f}\")\n",
    "\n",
    "    y_test_proba, y_test_true = evaluate_model(saint_model, test_loader, device=device)\n",
    "    y_test_pred = (y_test_proba >= best_thresh).astype(int)\n",
    "\n",
    "    metrics_saint = compute_metrics_with_ci(\n",
    "        y_test_true, y_test_pred, y_test_proba, n_bootstraps=config.N_BOOTSTRAP\n",
    "    )\n",
    "    print_metrics_with_ci(metrics_saint, \"SAINT Test Results\")\n",
    "\n",
    "    baseline_models[\"SAINT\"] = saint_model\n",
    "    baseline_results[\"SAINT\"] = {\n",
    "        \"predictions\": y_test_pred,\n",
    "        \"probabilities\": y_test_proba,\n",
    "        \"metrics\": metrics_saint,\n",
    "        \"history\": history_saint,\n",
    "        \"optimal_threshold\": best_thresh\n",
    "    }\n",
    "    torch.save(saint_model.state_dict(), config.MODELS_DIR / \"saint_model_optimized.pt\")\n",
    "    print(\"SAINT Saved!\")\n",
    "\n",
    "def run_resnet_optuna():\n",
    "    print(\"\\nStarting Optuna Tuning for Clinical ResNet...\")\n",
    "    pos_weight = compute_pos_weight(y_train)\n",
    "\n",
    "    def objective(trial):\n",
    "        hidden_dim = trial.suggest_categorical(\"hidden_dim\", [128, 256, 512])\n",
    "        num_blocks = trial.suggest_int(\"num_blocks\", 2, 4)\n",
    "        dropout = trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
    "        lr = trial.suggest_float(\"lr\", 1e-4, 5e-3, log=True)\n",
    "        weight_decay = trial.suggest_float(\"weight_decay\", 1e-5, 1e-2, log=True)\n",
    "\n",
    "        model = ClinicalResNet(\n",
    "            input_dim=X_train_scaled.shape[1],\n",
    "            hidden_dim=hidden_dim,\n",
    "            num_blocks=num_blocks,\n",
    "            dropout=dropout\n",
    "        ).to(device)\n",
    "\n",
    "        pos_weight_tensor = torch.tensor([pos_weight], device=device)\n",
    "        criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight_tensor)\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "        for epoch in range(15):\n",
    "            model.train()\n",
    "            for xb, yb in train_loader:\n",
    "                xb, yb = xb.to(device), yb.to(device).float()\n",
    "                optimizer.zero_grad()\n",
    "                logits = model(xb)\n",
    "                loss = criterion(logits, yb)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            val_probs, val_true = evaluate_model(model, val_loader, device)\n",
    "            val_auroc = roc_auc_score(val_true, val_probs)\n",
    "\n",
    "            trial.report(val_auroc, epoch)\n",
    "            if trial.should_prune():\n",
    "                raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "        return val_auroc\n",
    "\n",
    "    study = optuna.create_study(direction=\"maximize\", sampler=TPESampler(seed=42))\n",
    "    study.optimize(objective, n_trials=30, timeout=1800)\n",
    "\n",
    "    print(\"Best ResNet Params:\", study.best_params)\n",
    "\n",
    "    print(\"Retraining Best ResNet Fully...\")\n",
    "    best = study.best_params\n",
    "    final_resnet = ClinicalResNet(\n",
    "        input_dim=X_train_scaled.shape[1],\n",
    "        hidden_dim=best[\"hidden_dim\"],\n",
    "        num_blocks=best[\"num_blocks\"],\n",
    "        dropout=best[\"dropout\"]\n",
    "    ).to(device)\n",
    "\n",
    "    pos_weight_tensor = torch.tensor([pos_weight], device=device)\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight_tensor)\n",
    "    optimizer = torch.optim.AdamW(final_resnet.parameters(), lr=best[\"lr\"], weight_decay=best[\"weight_decay\"])\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=5)\n",
    "\n",
    "    best_val_auc = 0.0\n",
    "    best_state = None\n",
    "\n",
    "    for epoch in range(config.MAX_EPOCHS):\n",
    "        final_resnet.train()\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device).float()\n",
    "            optimizer.zero_grad()\n",
    "            logits = final_resnet(xb)\n",
    "            loss = criterion(logits, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        val_probs, val_true = evaluate_model(final_resnet, val_loader, device)\n",
    "        val_auc = roc_auc_score(val_true, val_probs)\n",
    "        scheduler.step(val_auc)\n",
    "\n",
    "        if val_auc > best_val_auc:\n",
    "            best_val_auc = val_auc\n",
    "            best_state = final_resnet.state_dict().copy()\n",
    "\n",
    "        if (epoch+1) % 10 == 0:\n",
    "            print(f\"ResNet Epoch {epoch+1}: Val AUROC {val_auc:.4f}\")\n",
    "\n",
    "    final_resnet.load_state_dict(best_state)\n",
    "    val_probs, val_true = evaluate_model(final_resnet, val_loader, device)\n",
    "    best_thresh, _ = get_optimal_threshold(val_true, val_probs)\n",
    "    print(f\"ResNet - Optimal Threshold: {best_thresh:.4f}\")\n",
    "\n",
    "    y_test_prob, y_test_true = evaluate_model(final_resnet, test_loader, device)\n",
    "    y_test_pred = (y_test_prob >= best_thresh).astype(int)\n",
    "\n",
    "    metrics = compute_metrics_with_ci(y_test_true, y_test_pred, y_test_prob, n_bootstraps=config.N_BOOTSTRAP)\n",
    "    print_metrics_with_ci(metrics, \"Optimized Clinical ResNet\")\n",
    "\n",
    "    baseline_models[\"ClinicalResNet\"] = final_resnet\n",
    "    baseline_results[\"ClinicalResNet\"] = {\n",
    "        \"predictions\": y_test_pred,\n",
    "        \"probabilities\": y_test_prob,\n",
    "        \"metrics\": metrics,\n",
    "        \"optimal_threshold\": best_thresh\n",
    "    }\n",
    "    torch.save(final_resnet.state_dict(), config.MODELS_DIR / \"resnet_model_optimized.pt\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_mlp_direct()\n",
    "    run_saint_direct().\n",
    "    run_resnet_optuna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gH_0Eq9cXut1"
   },
   "outputs": [],
   "source": [
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "from optuna.pruners import HyperbandPruner\n",
    "import torch.optim as optim\n",
    "\n",
    "def run_resnet_full_optuna():\n",
    "    print(\"Starting RIGOROUS Optuna Search for Clinical ResNet...\")\n",
    "\n",
    "\n",
    "    pos_weight = compute_pos_weight(y_train)\n",
    "    pos_weight_tensor = torch.tensor([pos_weight], device=device)\n",
    "\n",
    "    def objective(trial):\n",
    "        hidden_dim = trial.suggest_categorical(\"hidden_dim\", [128, 256])\n",
    "        num_blocks = trial.suggest_int(\"num_blocks\", 2, 4)\n",
    "        dropout = trial.suggest_float(\"dropout\", 0.2, 0.5)\n",
    "\n",
    "        lr = trial.suggest_float(\"lr\", 1e-4, 2e-3, log=True)\n",
    "        weight_decay = trial.suggest_float(\"weight_decay\", 1e-5, 1e-2, log=True)\n",
    "\n",
    "        batch_size = trial.suggest_categorical(\"batch_size\", [64, 128])\n",
    "\n",
    "        model = ClinicalResNet(\n",
    "            input_dim=X_train_scaled.shape[1],\n",
    "            hidden_dim=hidden_dim,\n",
    "            num_blocks=num_blocks,\n",
    "            dropout=dropout\n",
    "        ).to(device)\n",
    "\n",
    "        train_loader_trial, val_loader_trial = make_loaders(\n",
    "            X_train_scaled, y_train, X_val_scaled, y_val, batch_size\n",
    "        )\n",
    "\n",
    "        criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight_tensor)\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "        for epoch in range(25):\n",
    "            model.train()\n",
    "            for xb, yb in train_loader_trial:\n",
    "                xb, yb = xb.to(device), yb.to(device).float()\n",
    "                optimizer.zero_grad()\n",
    "                logits = model(xb)\n",
    "                loss = criterion(logits, yb)\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "\n",
    "            val_probs, val_true = evaluate_model(model, val_loader_trial, device)\n",
    "            val_auroc = roc_auc_score(val_true, val_probs)\n",
    "\n",
    "            trial.report(val_auroc, epoch)\n",
    "\n",
    "            if trial.should_prune():\n",
    "                raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "        return val_auroc\n",
    "\n",
    "    study = optuna.create_study(\n",
    "        direction=\"maximize\",\n",
    "        sampler=TPESampler(seed=42),\n",
    "        pruner=HyperbandPruner(min_resource=5, max_resource=25, reduction_factor=3)\n",
    "    )\n",
    "\n",
    "    study.optimize(objective, n_trials=50, timeout=5400, show_progress_bar=True)\n",
    "\n",
    "    print(\"SEARCH COMPLETE\")\n",
    "    print(f\"Best Val AUROC: {study.best_value:.4f}\")\n",
    "    print(\"Best Params:\", study.best_params)\n",
    "\n",
    "    print(\"Retraining best architecture for full convergence...\")\n",
    "    bp = study.best_params\n",
    "\n",
    "    final_model = ClinicalResNet(\n",
    "        input_dim=X_train_scaled.shape[1],\n",
    "        hidden_dim=bp[\"hidden_dim\"],\n",
    "        num_blocks=bp[\"num_blocks\"],\n",
    "        dropout=bp[\"dropout\"]\n",
    "    ).to(device)\n",
    "\n",
    "    train_loader, val_loader = make_loaders(\n",
    "        X_train_scaled, y_train, X_val_scaled, y_val, bp[\"batch_size\"]\n",
    "    )\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight_tensor)\n",
    "    optimizer = optim.AdamW(final_model.parameters(), lr=bp[\"lr\"], weight_decay=bp[\"weight_decay\"])\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='max', factor=0.5, patience=5, verbose=True\n",
    "    )\n",
    "\n",
    "    best_final_auc = 0.0\n",
    "    best_state = None\n",
    "\n",
    "    for epoch in range(config.MAX_EPOCHS):\n",
    "        final_model.train()\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device).float()\n",
    "            optimizer.zero_grad()\n",
    "            logits = final_model(xb)\n",
    "            loss = criterion(logits, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        val_probs, val_true = evaluate_model(final_model, val_loader, device)\n",
    "        val_auc = roc_auc_score(val_true, val_probs)\n",
    "        scheduler.step(val_auc)\n",
    "\n",
    "        if val_auc > best_final_auc:\n",
    "            best_final_auc = val_auc\n",
    "            best_state = final_model.state_dict().copy()\n",
    "\n",
    "        if (epoch+1) % 10 == 0:\n",
    "            print(f\"Final Training Epoch {epoch+1}: Val AUROC {val_auc:.4f}\")\n",
    "\n",
    "    print(f\"\\nFinal Best Val AUROC: {best_final_auc:.4f}\")\n",
    "\n",
    "    final_model.load_state_dict(best_state)\n",
    "\n",
    "    val_probs, val_true = evaluate_model(final_model, val_loader, device)\n",
    "    best_thresh, _ = get_optimal_threshold(val_true, val_probs)\n",
    "    print(f\"Optimal Threshold: {best_thresh:.4f}\")\n",
    "\n",
    "    y_test_probs, y_test_true = evaluate_model(final_model, test_loader, device)\n",
    "    y_test_pred = (y_test_probs >= best_thresh).astype(int)\n",
    "\n",
    "    metrics = compute_metrics_with_ci(y_test_true, y_test_pred, y_test_probs, n_bootstraps=config.N_BOOTSTRAP)\n",
    "    print_metrics_with_ci(metrics, \"Optimized Clinical ResNet\")\n",
    "\n",
    "    baseline_models[\"ClinicalResNet\"] = final_model\n",
    "    baseline_results[\"ClinicalResNet\"] = {\n",
    "        \"predictions\": y_test_pred,\n",
    "        \"probabilities\": y_test_probs,\n",
    "        \"metrics\": metrics,\n",
    "        \"optimal_threshold\": best_thresh,\n",
    "        \"best_params\": bp\n",
    "    }\n",
    "    torch.save(final_model.state_dict(), config.MODELS_DIR / \"resnet_model_optimized.pt\")\n",
    "\n",
    "    with open(config.MODELS_DIR / \"resnet_optuna_study.pkl\", 'wb') as f:\n",
    "        pickle.dump(study, f)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_resnet_full_optuna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "otKJ7KI8Xut2"
   },
   "outputs": [],
   "source": [
    "!pip install pytorch_tabnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oeAbOD9kXut2"
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "class TabNetWrapper(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, **params):\n",
    "        self.params = params.copy()\n",
    "        self.params.setdefault(\"verbose\", 1)\n",
    "        self.model = None\n",
    "\n",
    "    def fit(self, X, y, eval_set=None, eval_metric=['auc']):\n",
    "        self.model = TabNetClassifier(**self.params)\n",
    "\n",
    "        X_np = X.values if hasattr(X, 'values') else X\n",
    "        y_np = y\n",
    "\n",
    "        eval_set_np = []\n",
    "        if eval_set:\n",
    "            for X_val, y_val in eval_set:\n",
    "                X_val_np = X_val.values if hasattr(X_val, 'values') else X_val\n",
    "                eval_set_np.append((X_val_np, y_val))\n",
    "\n",
    "        self.model.fit(\n",
    "            X_train=X_np, y_train=y_np,\n",
    "            eval_set=eval_set_np,\n",
    "            eval_metric=eval_metric,\n",
    "            max_epochs=config.MAX_EPOCHS,\n",
    "            patience=config.EARLY_STOPPING_PATIENCE,\n",
    "            batch_size=config.BATCH_SIZE,\n",
    "            virtual_batch_size=128,\n",
    "            num_workers=0,\n",
    "            drop_last=False\n",
    "        )\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        X_np = X.values if hasattr(X, 'values') else X\n",
    "        return self.model.predict(X_np)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        X_np = X.values if hasattr(X, 'values') else X\n",
    "        return self.model.predict_proba(X_np)\n",
    "\n",
    "class FTTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_cont_features: int,\n",
    "        d_token: int = 192,\n",
    "        n_layers: int = 3,\n",
    "        n_heads: int = 8,\n",
    "        d_ffn: int = 256,\n",
    "        dropout: float = 0.1,\n",
    "        attention_dropout: float = 0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.n_cont_features = n_cont_features\n",
    "\n",
    "        self.feature_tokenizer = nn.ModuleList([\n",
    "            nn.Linear(1, d_token) for _ in range(n_cont_features)\n",
    "        ])\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, d_token))\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_token,\n",
    "            nhead=n_heads,\n",
    "            dim_feedforward=d_ffn,\n",
    "            dropout=dropout,\n",
    "            activation='gelu',\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            encoder_layer,\n",
    "            num_layers=n_layers\n",
    "        )\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.LayerNorm(d_token),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_token, 1)\n",
    "        )\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "\n",
    "        tokens = []\n",
    "        for i in range(self.n_cont_features):\n",
    "            feature_i = x[:, i].unsqueeze(1)\n",
    "            token_i = self.feature_tokenizer[i](feature_i)\n",
    "            tokens.append(token_i.unsqueeze(1))\n",
    "\n",
    "        x_emb = torch.cat(tokens, dim=1)\n",
    "\n",
    "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n",
    "        x_emb = torch.cat((cls_tokens, x_emb), dim=1)\n",
    "\n",
    "        x_out = self.transformer(x_emb)\n",
    "\n",
    "        cls_out = x_out[:, 0, :]\n",
    "\n",
    "        logits = self.head(cls_out).squeeze(-1)\n",
    "        return logits\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class EnhancedMLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims=[451, 349, 132], dropout=0.1):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        in_dim = input_dim\n",
    "        for h_dim in hidden_dims:\n",
    "            layers.append(nn.Linear(in_dim, h_dim))\n",
    "            layers.append(nn.BatchNorm1d(h_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            if dropout > 0:\n",
    "                layers.append(nn.Dropout(dropout))\n",
    "            in_dim = h_dim\n",
    "        layers.append(nn.Linear(in_dim, 1))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EhDOR4k_Xut3"
   },
   "outputs": [],
   "source": [
    "def run_tabnet_optuna():\n",
    "    def objective_tabnet(trial):\n",
    "        params = {\n",
    "            'n_d': trial.suggest_int('n_d', 8, 64),\n",
    "            'n_a': trial.suggest_int('n_a', 8, 64),\n",
    "            'n_steps': trial.suggest_int('n_steps', 3, 10),\n",
    "            'gamma': trial.suggest_float('gamma', 1.0, 2.0),\n",
    "            'lambda_sparse': trial.suggest_float('lambda_sparse', 1e-6, 1e-3, log=True),\n",
    "            'optimizer_params': dict(lr=trial.suggest_float('lr', 1e-3, 5e-2, log=True)),\n",
    "            'mask_type': 'entmax',\n",
    "            'verbose': 1\n",
    "        }\n",
    "        model = TabNetWrapper(**params)\n",
    "        model.fit(\n",
    "            X_train_scaled, y_train,\n",
    "            eval_set=[(X_val_scaled, y_val)],\n",
    "            eval_metric=['auc']\n",
    "        )\n",
    "        preds = model.predict_proba(X_val_scaled)[:, 1]\n",
    "        val_auc = roc_auc_score(y_val, preds)\n",
    "        print(f\"[TabNet][Trial {trial.number + 1}] val AUROC={val_auc:.4f}\", flush=True)\n",
    "        return val_auc\n",
    "\n",
    "    def log_best_tabnet(study, trial):\n",
    "        if study.best_trial:\n",
    "            best = study.best_trial\n",
    "            print(\n",
    "                f\"[TabNet][Best so far] Trial {best.number} AUROC={best.value:.4f} params={best.params}\",\n",
    "                flush=True,\n",
    "            )\n",
    "\n",
    "    study_tabnet = optuna.create_study(direction=\"maximize\", sampler=TPESampler(seed=config.RANDOM_STATE))\n",
    "    study_tabnet.optimize(\n",
    "        objective_tabnet,\n",
    "        n_trials=config.OPTUNA_N_TRIALS,\n",
    "        timeout=config.OPTUNA_TIMEOUT,\n",
    "        show_progress_bar=True,\n",
    "        callbacks=[log_best_tabnet],\n",
    "    )\n",
    "\n",
    "    print(f\"\\nBest AUROC (val): {study_tabnet.best_value:.4f}\")\n",
    "    print(f\"Best params: {study_tabnet.best_params}\")\n",
    "\n",
    "    best_params = study_tabnet.best_params.copy()\n",
    "    if 'lr' in best_params:\n",
    "        best_params['optimizer_params'] = dict(lr=best_params.pop('lr'))\n",
    "    best_params['verbose'] = 1\n",
    "    best_params['mask_type'] = 'entmax'\n",
    "\n",
    "    tabnet_model = TabNetWrapper(**best_params)\n",
    "    print(f\"[TabNet] Training final model with best params and {config.MAX_EPOCHS} max epochs...\", flush=True)\n",
    "    tabnet_model.fit(\n",
    "        X_train_scaled, y_train,\n",
    "        eval_set=[(X_val_scaled, y_val)],\n",
    "        eval_metric=['auc']\n",
    "    )\n",
    "    print(\"[TabNet] Final training finished.\", flush=True)\n",
    "\n",
    "    y_test_proba_tab = tabnet_model.predict_proba(X_test_scaled)[:, 1]\n",
    "    y_test_pred_tab = (y_test_proba_tab >= 0.5).astype(int)\n",
    "\n",
    "    metrics_tab = compute_metrics_with_ci(\n",
    "        y_test, y_test_pred_tab, y_test_proba_tab, n_bootstraps=config.N_BOOTSTRAP\n",
    "    )\n",
    "    print_metrics_with_ci(metrics_tab, \"TabNet Test Results\")\n",
    "\n",
    "    baseline_models[\"TabNet\"] = tabnet_model\n",
    "    baseline_results[\"TabNet\"] = {\n",
    "        \"predictions\": y_test_pred_tab,\n",
    "        \"probabilities\": y_test_proba_tab,\n",
    "        \"metrics\": metrics_tab,\n",
    "        \"best_params\": study_tabnet.best_params,\n",
    "        \"best_val_auroc\": study_tabnet.best_value,\n",
    "    }\n",
    "    with open(config.MODELS_DIR / \"tabnet_model.pkl\", \"wb\") as f:\n",
    "        pickle.dump(tabnet_model, f)\n",
    "    print(\"TabNet model saved!\")\n",
    "\n",
    "\n",
    "def run_fttransformer_optuna():\n",
    "    pos_weight = compute_pos_weight(y_train)\n",
    "\n",
    "    def objective_ftt(trial):\n",
    "        d_token = trial.suggest_categorical(\"d_token\", [64, 128, 192, 256])\n",
    "        n_layers = trial.suggest_int(\"n_layers\", 1, 4)\n",
    "        n_heads = trial.suggest_categorical(\"n_heads\", [4, 8])\n",
    "        d_ffn_factor = trial.suggest_float(\"d_ffn_factor\", 1.0, 4.0)\n",
    "        dropout = trial.suggest_float(\"dropout\", 0.0, 0.5)\n",
    "        lr = trial.suggest_float(\"lr\", 1e-5, 1e-3, log=True)\n",
    "        weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-3, log=True)\n",
    "        batch_size = trial.suggest_categorical(\"batch_size\", [64, 128])\n",
    "\n",
    "        if d_token % n_heads != 0:\n",
    "            return 0.0\n",
    "\n",
    "        model = FTTransformer(\n",
    "            n_cont_features=X_train_scaled.shape[1],\n",
    "            d_token=d_token,\n",
    "            n_layers=n_layers,\n",
    "            n_heads=n_heads,\n",
    "            d_ffn=int(d_token * d_ffn_factor),\n",
    "            dropout=dropout,\n",
    "            attention_dropout=dropout\n",
    "        )\n",
    "\n",
    "        train_loader, val_loader = make_loaders(X_train_scaled, y_train, X_val_scaled, y_val, batch_size)\n",
    "\n",
    "        hist = train_deep_model(\n",
    "            model,\n",
    "            train_loader,\n",
    "            val_loader,\n",
    "            pos_weight=pos_weight,\n",
    "            n_epochs=40,\n",
    "            learning_rate=lr,\n",
    "            weight_decay=weight_decay,\n",
    "            patience=8,\n",
    "            device=device,\n",
    "        )\n",
    "        best_val_auc = max(hist[\"val_auroc\"])\n",
    "        print(f\"[FTT][Trial {trial.number + 1}] best val AUROC={best_val_auc:.4f}\", flush=True)\n",
    "        return best_val_auc\n",
    "\n",
    "    def log_best_ftt(study, trial):\n",
    "        if study.best_trial:\n",
    "            best = study.best_trial\n",
    "            print(\n",
    "                f\"[FTT][Best so far] Trial {best.number} AUROC={best.value:.4f} params={best.params}\",\n",
    "                flush=True,\n",
    "            )\n",
    "\n",
    "    study_ftt = optuna.create_study(direction=\"maximize\", sampler=TPESampler(seed=config.RANDOM_STATE))\n",
    "    study_ftt.optimize(\n",
    "        objective_ftt,\n",
    "        n_trials=config.OPTUNA_N_TRIALS,\n",
    "        timeout=config.OPTUNA_TIMEOUT,\n",
    "        show_progress_bar=True,\n",
    "        callbacks=[log_best_ftt],\n",
    "    )\n",
    "\n",
    "    print(f\"\\nBest AUROC (val): {study_ftt.best_value:.4f}\")\n",
    "    print(f\"Best params: {study_ftt.best_params}\")\n",
    "\n",
    "    bp = study_ftt.best_params\n",
    "    ftt_model = FTTransformer(\n",
    "        n_cont_features=X_train_scaled.shape[1],\n",
    "        d_token=bp[\"d_token\"],\n",
    "        n_layers=bp[\"n_layers\"],\n",
    "        n_heads=bp[\"n_heads\"],\n",
    "        d_ffn=int(bp[\"d_token\"] * bp[\"d_ffn_factor\"]),\n",
    "        dropout=bp[\"dropout\"],\n",
    "        attention_dropout=bp[\"dropout\"]\n",
    "    )\n",
    "\n",
    "    train_loader, val_loader = make_loaders(X_train_scaled, y_train, X_val_scaled, y_val, bp[\"batch_size\"])\n",
    "\n",
    "    print(f\"[FTT] Training final model for up to {config.MAX_EPOCHS} epochs (batch_size={bp['batch_size']})...\", flush=True)\n",
    "    history_ftt = train_deep_model(\n",
    "        ftt_model,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        pos_weight=pos_weight,\n",
    "        n_epochs=config.MAX_EPOCHS,\n",
    "        learning_rate=bp[\"lr\"],\n",
    "        weight_decay=bp[\"weight_decay\"],\n",
    "        patience=config.EARLY_STOPPING_PATIENCE,\n",
    "        device=device,\n",
    "    )\n",
    "    print(f\"[FTT] Final training finished. Best val AUROC={max(history_ftt['val_auroc']):.4f}\", flush=True)\n",
    "\n",
    "    y_test_proba_ftt, y_test_true_ftt = evaluate_model(ftt_model, test_loader, device=device)\n",
    "    y_test_pred_ftt = (y_test_proba_ftt >= 0.5).astype(int)\n",
    "\n",
    "    metrics_ftt = compute_metrics_with_ci(\n",
    "        y_test, y_test_pred_ftt, y_test_proba_ftt, n_bootstraps=config.N_BOOTSTRAP\n",
    "    )\n",
    "    print_metrics_with_ci(metrics_ftt, \"FT-Transformer Test Results\")\n",
    "\n",
    "    baseline_models[\"FT-Transformer\"] = ftt_model\n",
    "    baseline_results[\"FT-Transformer\"] = {\n",
    "        \"predictions\": y_test_pred_ftt,\n",
    "        \"probabilities\": y_test_proba_ftt,\n",
    "        \"metrics\": metrics_ftt,\n",
    "        \"history\": history_ftt,\n",
    "        \"best_params\": study_ftt.best_params,\n",
    "        \"best_val_auroc\": study_ftt.best_value,\n",
    "    }\n",
    "    torch.save(ftt_model.state_dict(), config.MODELS_DIR / \"fttransformer_model.pt\")\n",
    "    print(\"FT-Transformer model saved!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_tabnet_optuna()\n",
    "    run_fttransformer_optuna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch\n",
    "\n",
    "\n",
    "def run_tabnet_direct(X_train_scaled, y_train, X_val_scaled, y_val, X_test_scaled, y_test,\n",
    "                      config, baseline_models, baseline_results):\n",
    "    print(\"[TabNet] Training with best params (no Optuna)...\")\n",
    "    best_params = {\n",
    "        'n_d': 32,\n",
    "        'n_a': 24,\n",
    "        'n_steps': 7,\n",
    "        'gamma': 1.139493860652042,\n",
    "        'lambda_sparse': 7.523742884534855e-06,\n",
    "        'optimizer_params': dict(lr=0.004192159350410977),\n",
    "        'mask_type': 'entmax',\n",
    "        'verbose': 1,\n",
    "    }\n",
    "    model = TabNetWrapper(**best_params)\n",
    "    model.fit(\n",
    "        X_train_scaled, y_train,\n",
    "        eval_set=[(X_val_scaled, y_val)],\n",
    "        eval_metric=['auc']\n",
    "    )\n",
    "\n",
    "    val_probs = model.predict_proba(X_val_scaled)[:, 1]\n",
    "    best_thresh, _ = get_optimal_threshold(y_val, val_probs)\n",
    "    print(f\"[TabNet] Optimal Threshold: {best_thresh:.4f}\")\n",
    "\n",
    "    test_probs = model.predict_proba(X_test_scaled)[:, 1]\n",
    "    test_pred = (test_probs >= best_thresh).astype(int)\n",
    "\n",
    "    metrics_tab = compute_metrics_with_ci(\n",
    "        y_test, test_pred, test_probs, n_bootstraps=config.N_BOOTSTRAP\n",
    "    )\n",
    "    print_metrics_with_ci(metrics_tab, \"[TabNet] Test Results\")\n",
    "\n",
    "    baseline_models[\"TabNet\"] = model\n",
    "    baseline_results[\"TabNet\"] = {\n",
    "        \"predictions\": test_pred,\n",
    "        \"probabilities\": test_probs,\n",
    "        \"metrics\": metrics_tab,\n",
    "        \"best_params\": best_params,\n",
    "        \"optimal_threshold\": best_thresh,\n",
    "    }\n",
    "    with open(config.MODELS_DIR / \"tabnet_model.pkl\", \"wb\") as f:\n",
    "        pickle.dump(model, f)\n",
    "    print(\"[TabNet] Saved to tabnet_model.pkl\")\n",
    "\n",
    "\n",
    "def run_ftt_direct(X_train_scaled, y_train, X_val_scaled, y_val, X_test_scaled, y_test,\n",
    "                   config, device, test_loader, baseline_models, baseline_results):\n",
    "    print(\"[FT-Transformer] Training with best params (no Optuna)...\")\n",
    "    best_params = {\n",
    "        'd_token': 192,\n",
    "        'n_layers': 4,\n",
    "        'n_heads': 4,\n",
    "        'd_ffn_factor': 1.0165663513708072,\n",
    "        'dropout': 0.4077307142274171,\n",
    "        'lr': 0.0002592475660475158,\n",
    "        'weight_decay': 0.00015382308040278996,\n",
    "        'batch_size': 64,\n",
    "    }\n",
    "\n",
    "    pos_weight = compute_pos_weight(y_train)\n",
    "    train_loader, val_loader = make_loaders(\n",
    "        X_train_scaled, y_train,\n",
    "        X_val_scaled, y_val,\n",
    "        best_params[\"batch_size\"]\n",
    "    )\n",
    "\n",
    "    ftt_model = FTTransformer(\n",
    "        n_cont_features=X_train_scaled.shape[1],\n",
    "        d_token=best_params[\"d_token\"],\n",
    "        n_layers=best_params[\"n_layers\"],\n",
    "        n_heads=best_params[\"n_heads\"],\n",
    "        d_ffn=int(best_params[\"d_token\"] * best_params[\"d_ffn_factor\"]),\n",
    "        dropout=best_params[\"dropout\"],\n",
    "        attention_dropout=best_params[\"dropout\"],\n",
    "    )\n",
    "\n",
    "    history_ftt = train_deep_model(\n",
    "        ftt_model,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        pos_weight=pos_weight,\n",
    "        n_epochs=config.MAX_EPOCHS,\n",
    "        learning_rate=best_params[\"lr\"],\n",
    "        weight_decay=best_params[\"weight_decay\"],\n",
    "        patience=config.EARLY_STOPPING_PATIENCE,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    val_probs, val_true = evaluate_model(ftt_model, val_loader, device=device)\n",
    "    best_thresh, _ = get_optimal_threshold(val_true, val_probs)\n",
    "    print(f\"[FT-Transformer] Optimal Threshold: {best_thresh:.4f}\")\n",
    "\n",
    "    test_probs, test_true = evaluate_model(ftt_model, test_loader, device=device)\n",
    "    test_pred = (test_probs >= best_thresh).astype(int)\n",
    "\n",
    "    metrics_ftt = compute_metrics_with_ci(\n",
    "        test_true, test_pred, test_probs, n_bootstraps=config.N_BOOTSTRAP\n",
    "    )\n",
    "    print_metrics_with_ci(metrics_ftt, \"[FT-Transformer] Test Results\")\n",
    "\n",
    "    baseline_models[\"FT-Transformer\"] = ftt_model\n",
    "    baseline_results[\"FT-Transformer\"] = {\n",
    "        \"predictions\": test_pred,\n",
    "        \"probabilities\": test_probs,\n",
    "        \"metrics\": metrics_ftt,\n",
    "        \"history\": history_ftt,\n",
    "        \"best_params\": best_params,\n",
    "        \"optimal_threshold\": best_thresh,\n",
    "    }\n",
    "    torch.save(ftt_model.state_dict(), config.MODELS_DIR / \"fttransformer_model.pt\")\n",
    "    print(\"[FT-Transformer] Saved to fttransformer_model.pt\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import sys\n",
    "    ns = sys.modules.get(\"__main__\")\n",
    "\n",
    "    if not hasattr(ns, \"TabNetClassifier\"):\n",
    "        try:\n",
    "            from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "            setattr(ns, \"TabNetClassifier\", TabNetClassifier)\n",
    "        except Exception as exc:\n",
    "            print(f\"TabNetClassifier not found. Error: {exc}\")\n",
    "            sys.exit(1)\n",
    "\n",
    "    required = [\n",
    "        \"X_train_scaled\", \"y_train\", \"X_val_scaled\", \"y_val\", \"X_test_scaled\", \"y_test\",\n",
    "        \"config\", \"device\", \"test_loader\", \"baseline_models\", \"baseline_results\",\n",
    "        \"make_loaders\", \"compute_pos_weight\", \"evaluate_model\", \"get_optimal_threshold\",\n",
    "        \"compute_metrics_with_ci\", \"print_metrics_with_ci\", \"TabNetWrapper\", \"FTTransformer\", \"train_deep_model\",\n",
    "    ]\n",
    "\n",
    "    missing = [name for name in required if not hasattr(ns, name)]\n",
    "\n",
    "    if missing:\n",
    "        print(f\"Missing prerequisites: {', '.join(missing)}\")\n",
    "    else:\n",
    "        print(\"All prerequisites found; running TabNet and FT-Transformer with best params (no Optuna).\")\n",
    "        run_tabnet_direct(\n",
    "            ns.X_train_scaled, ns.y_train,\n",
    "            ns.X_val_scaled, ns.y_val,\n",
    "            ns.X_test_scaled, ns.y_test,\n",
    "            ns.config, ns.baseline_models, ns.baseline_results\n",
    "        )\n",
    "        run_ftt_direct(\n",
    "            ns.X_train_scaled, ns.y_train,\n",
    "            ns.X_val_scaled, ns.y_val,\n",
    "            ns.X_test_scaled, ns.y_test,\n",
    "            ns.config, ns.device, ns.test_loader,\n",
    "            ns.baseline_models, ns.baseline_results\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JeiddyTpXut3"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, accuracy_score, f1_score\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "\n",
    "DATA_DIR = Path(\"/kaggle/input/databefore/processed_data_final\")\n",
    "N_TRIALS = 10\n",
    "TIMEOUT = 3600\n",
    "MAX_EPOCHS = 60\n",
    "PATIENCE = 10\n",
    "BATCH_SIZES = [64, 128]\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "SEED = 42\n",
    "\n",
    "def load_splits():\n",
    "    npy_names = [\n",
    "        \"X_train_scaled.npy\",\n",
    "        \"X_val_scaled.npy\",\n",
    "        \"X_test_scaled.npy\",\n",
    "        \"y_train.npy\",\n",
    "        \"y_val.npy\",\n",
    "        \"y_test.npy\",\n",
    "    ]\n",
    "    if all((DATA_DIR / name).exists() for name in npy_names):\n",
    "        x_train = np.load(DATA_DIR / \"X_train_scaled.npy\")\n",
    "        x_val = np.load(DATA_DIR / \"X_val_scaled.npy\")\n",
    "        x_test = np.load(DATA_DIR / \"X_test_scaled.npy\")\n",
    "        y_train = np.load(DATA_DIR / \"y_train.npy\")\n",
    "        y_val = np.load(DATA_DIR / \"y_val.npy\")\n",
    "        y_test = np.load(DATA_DIR / \"y_test.npy\")\n",
    "        return x_train, x_val, x_test, y_train, y_val, y_test\n",
    "\n",
    "    pkl_path = DATA_DIR / \"preprocessed_data.pkl\"\n",
    "    if pkl_path.exists():\n",
    "        import pickle\n",
    "\n",
    "        with open(pkl_path, \"rb\") as f:\n",
    "            obj = pickle.load(f)\n",
    "        if not isinstance(obj, dict):\n",
    "            raise ValueError(f\"{pkl_path} does not contain a dict; got {type(obj)}\")\n",
    "\n",
    "        def grab(key_opts):\n",
    "            for k in key_opts:\n",
    "                if k in obj:\n",
    "                    return obj[k]\n",
    "            raise KeyError(f\"None of {key_opts} found in {pkl_path}. Available keys: {list(obj.keys())}\")\n",
    "\n",
    "        x_train = grab([\"X_train_scaled\", \"X_train\"])\n",
    "        x_val = grab([\"X_val_scaled\", \"X_val\"])\n",
    "        x_test = grab([\"X_test_scaled\", \"X_test\"])\n",
    "        y_train = grab([\"y_train\"])\n",
    "        y_val = grab([\"y_val\"])\n",
    "        y_test = grab([\"y_test\"])\n",
    "        return x_train, x_val, x_test, y_train, y_val, y_test\n",
    "\n",
    "    raise FileNotFoundError(\n",
    "        \"Could not find expected preprocessed splits. \"\n",
    "        \"Place npy files or preprocessed_data.pkl in DATA_DIR.\"\n",
    "    )\n",
    "\n",
    "class FTTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_cont_features: int,\n",
    "        d_token: int = 192,\n",
    "        n_layers: int = 3,\n",
    "        n_heads: int = 8,\n",
    "        d_ffn: int = 256,\n",
    "        dropout: float = 0.1,\n",
    "        attention_dropout: float = 0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.n_cont_features = n_cont_features\n",
    "        self.feature_tokenizer = nn.ModuleList([nn.Linear(1, d_token) for _ in range(n_cont_features)])\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, d_token))\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_token,\n",
    "            nhead=n_heads,\n",
    "            dim_feedforward=d_ffn,\n",
    "            dropout=dropout,\n",
    "            activation=\"gelu\",\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
    "        self.head = nn.Sequential(nn.LayerNorm(d_token), nn.ReLU(), nn.Linear(d_token, 1))\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        tokens = []\n",
    "        for i in range(self.n_cont_features):\n",
    "            feature_i = x[:, i].unsqueeze(1)\n",
    "            token_i = self.feature_tokenizer[i](feature_i)\n",
    "            tokens.append(token_i.unsqueeze(1))\n",
    "        x_emb = torch.cat(tokens, dim=1)\n",
    "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n",
    "        x_emb = torch.cat((cls_tokens, x_emb), dim=1)\n",
    "        x_out = self.transformer(x_emb)\n",
    "        cls_out = x_out[:, 0, :]\n",
    "        logits = self.head(cls_out).squeeze(-1)\n",
    "        return logits\n",
    "\n",
    "def set_seeds(seed: int = 42):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "def compute_pos_weight(y: np.ndarray) -> torch.Tensor:\n",
    "    pos = (y == 1).sum()\n",
    "    neg = (y == 0).sum()\n",
    "    w = neg / max(pos, 1)\n",
    "    return torch.tensor([w], dtype=torch.float32, device=DEVICE)\n",
    "\n",
    "def make_loaders(x_train, y_train, x_val, y_val, batch_size):\n",
    "    train_ds = TensorDataset(torch.tensor(x_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.float32))\n",
    "    val_ds = TensorDataset(torch.tensor(x_val, dtype=torch.float32), torch.tensor(y_val, dtype=torch.float32))\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "    return train_loader, val_loader\n",
    "\n",
    "def train_deep_model(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    pos_weight,\n",
    "    n_epochs,\n",
    "    learning_rate,\n",
    "    weight_decay,\n",
    "    patience,\n",
    "    device,\n",
    "):\n",
    "    model.to(device)\n",
    "    optim = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "    best_auc = 0.0\n",
    "    best_state = None\n",
    "    bad_epochs = 0\n",
    "    hist = {\"val_auroc\": []}\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            optim.zero_grad()\n",
    "            logits = model(xb)\n",
    "            loss = criterion(logits, yb)\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "        model.eval()\n",
    "        all_logits, all_y = [], []\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in val_loader:\n",
    "                xb = xb.to(device)\n",
    "                logits = model(xb)\n",
    "                all_logits.append(logits.cpu())\n",
    "                all_y.append(yb)\n",
    "        all_logits = torch.cat(all_logits).numpy()\n",
    "        all_y = torch.cat(all_y).numpy()\n",
    "        probs = 1 / (1 + np.exp(-all_logits))\n",
    "        val_auc = roc_auc_score(all_y, probs)\n",
    "        hist[\"val_auroc\"].append(val_auc)\n",
    "        print(f\"[FTT][Epoch {epoch}] val AUROC={val_auc:.4f}\", flush=True)\n",
    "        if val_auc > best_auc:\n",
    "            best_auc = val_auc\n",
    "            best_state = model.state_dict()\n",
    "            bad_epochs = 0\n",
    "        else:\n",
    "            bad_epochs += 1\n",
    "            if bad_epochs >= patience:\n",
    "                print(f\"[FTT] Early stopping at epoch {epoch} best AUROC={best_auc:.4f}\", flush=True)\n",
    "                break\n",
    "    if best_state:\n",
    "        model.load_state_dict(best_state)\n",
    "    return hist\n",
    "\n",
    "def evaluate_model(model, x_test, batch_size=256):\n",
    "    ds = TensorDataset(torch.tensor(x_test, dtype=torch.float32))\n",
    "    loader = DataLoader(ds, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "    model.eval()\n",
    "    probs = []\n",
    "    with torch.no_grad():\n",
    "        for (xb,) in loader:\n",
    "            xb = xb.to(DEVICE)\n",
    "            logits = model(xb)\n",
    "            probs.append(torch.sigmoid(logits).cpu())\n",
    "    probs = torch.cat(probs).numpy()\n",
    "    return probs\n",
    "\n",
    "def run_fttransformer_optuna():\n",
    "    x_train, x_val, x_test, y_train, y_val, y_test = load_splits()\n",
    "    x_train = np.asarray(x_train)\n",
    "    x_val = np.asarray(x_val)\n",
    "    x_test = np.asarray(x_test)\n",
    "    y_train = np.asarray(y_train).ravel()\n",
    "    y_val = np.asarray(y_val).ravel()\n",
    "    y_test = np.asarray(y_test).ravel()\n",
    "    set_seeds(SEED)\n",
    "    pos_weight = compute_pos_weight(y_train)\n",
    "\n",
    "    def objective(trial):\n",
    "        d_token = trial.suggest_categorical(\"d_token\", [64, 128, 192, 256])\n",
    "        n_layers = trial.suggest_int(\"n_layers\", 1, 4)\n",
    "        n_heads = trial.suggest_categorical(\"n_heads\", [4, 8])\n",
    "        d_ffn_factor = trial.suggest_float(\"d_ffn_factor\", 1.0, 4.0)\n",
    "        dropout = trial.suggest_float(\"dropout\", 0.0, 0.5)\n",
    "        lr = trial.suggest_float(\"lr\", 1e-5, 1e-3, log=True)\n",
    "        weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-3, log=True)\n",
    "        batch_size = trial.suggest_categorical(\"batch_size\", BATCH_SIZES)\n",
    "        if d_token % n_heads != 0:\n",
    "            return 0.0\n",
    "        model = FTTransformer(\n",
    "            n_cont_features=x_train.shape[1],\n",
    "            d_token=d_token,\n",
    "            n_layers=n_layers,\n",
    "            n_heads=n_heads,\n",
    "            d_ffn=int(d_token * d_ffn_factor),\n",
    "            dropout=dropout,\n",
    "            attention_dropout=dropout,\n",
    "        )\n",
    "        train_loader, val_loader = make_loaders(x_train, y_train, x_val, y_val, batch_size)\n",
    "        hist = train_deep_model(\n",
    "            model,\n",
    "            train_loader,\n",
    "            val_loader,\n",
    "            pos_weight=pos_weight,\n",
    "            n_epochs=40,\n",
    "            learning_rate=lr,\n",
    "            weight_decay=weight_decay,\n",
    "            patience=8,\n",
    "            device=DEVICE,\n",
    "        )\n",
    "        best_val_auc = max(hist[\"val_auroc\"])\n",
    "        print(f\"[FTT][Trial {trial.number + 1}] best val AUROC={best_val_auc:.4f}\", flush=True)\n",
    "        return best_val_auc\n",
    "\n",
    "    def log_best(study, trial):\n",
    "        if study.best_trial:\n",
    "            b = study.best_trial\n",
    "            print(f\"[FTT][Best so far] Trial {b.number} AUROC={b.value:.4f} params={b.params}\", flush=True)\n",
    "\n",
    "    study = optuna.create_study(direction=\"maximize\", sampler=TPESampler(seed=SEED))\n",
    "    study.optimize(objective, n_trials=N_TRIALS, timeout=TIMEOUT, show_progress_bar=True, callbacks=[log_best])\n",
    "    print(f\"\\nBest AUROC (val): {study.best_value:.4f}\")\n",
    "    print(f\"Best params: {study.best_params}\")\n",
    "\n",
    "    bp = study.best_params\n",
    "    model = FTTransformer(\n",
    "        n_cont_features=x_train.shape[1],\n",
    "        d_token=bp[\"d_token\"],\n",
    "        n_layers=bp[\"n_layers\"],\n",
    "        n_heads=bp[\"n_heads\"],\n",
    "        d_ffn=int(bp[\"d_token\"] * bp[\"d_ffn_factor\"]),\n",
    "        dropout=bp[\"dropout\"],\n",
    "        attention_dropout=bp[\"dropout\"],\n",
    "    )\n",
    "    train_loader, val_loader = make_loaders(x_train, y_train, x_val, y_val, bp[\"batch_size\"])\n",
    "    history = train_deep_model(\n",
    "        model,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        pos_weight=pos_weight,\n",
    "        n_epochs=MAX_EPOCHS,\n",
    "        learning_rate=bp[\"lr\"],\n",
    "        weight_decay=bp[\"weight_decay\"],\n",
    "        patience=PATIENCE,\n",
    "        device=DEVICE,\n",
    "    )\n",
    "    print(f\"[FTT] Final training done. Best val AUROC={max(history['val_auroc']):.4f}\", flush=True)\n",
    "\n",
    "    test_probs = evaluate_model(model, x_test)\n",
    "    test_pred = (test_probs >= 0.5).astype(int)\n",
    "    print(\"\\nFT-Transformer Test Metrics:\")\n",
    "    print(f\"AUROC: {roc_auc_score(y_test, test_probs):.4f}\")\n",
    "    print(f\"AUPRC: {average_precision_score(y_test, test_probs):.4f}\")\n",
    "    print(f\"Accuracy: {accuracy_score(y_test, test_pred):.4f}\")\n",
    "    print(f\"F1: {f1_score(y_test, test_pred):.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_fttransformer_optuna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mJhyutaFXut4"
   },
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import importlib.util\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score,\n",
    "    average_precision_score,\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    brier_score_loss,\n",
    ")\n",
    "\n",
    "BASE = Path(\"/kaggle/working\")\n",
    "DATA_DIR = BASE / \"processed_data_final\"\n",
    "MODELS_DIR = BASE / \"models\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "N_BOOTSTRAPS = 1000\n",
    "\n",
    "\n",
    "def load_cell_models():\n",
    "    search_paths = [\n",
    "        BASE / \"cell_models.py\",\n",
    "        Path.cwd() / \"cell_models.py\",\n",
    "        DATA_DIR.parent / \"cell_models.py\",\n",
    "    ]\n",
    "    for path in search_paths:\n",
    "        if path.exists():\n",
    "            spec = importlib.util.spec_from_file_location(\"cell_models\", path)\n",
    "            if spec and spec.loader:\n",
    "                try:\n",
    "                    module = importlib.util.module_from_spec(spec)\n",
    "                    sys.modules[\"cell_models\"] = module\n",
    "                    spec.loader.exec_module(module)\n",
    "                    for name in (\"TabNetWrapper\", \"EnhancedMLP\", \"SAINTModel\", \"FTTransformer\", \"ClinicalResNet\"):\n",
    "                        if hasattr(module, name):\n",
    "                            globals()[name] = getattr(module, name)\n",
    "                    return True\n",
    "                except Exception:\n",
    "                    pass\n",
    "    print(\"cell_models.py not found; PyTorch/TabNet classes must already be in scope.\")\n",
    "    return False\n",
    "\n",
    "\n",
    "load_cell_models()\n",
    "\n",
    "\n",
    "def compute_metrics_with_ci(y_true, y_pred, y_proba, n_bootstraps=1000, random_state=42):\n",
    "    rng = np.random.RandomState(random_state)\n",
    "    n = len(y_true)\n",
    "\n",
    "    def boot(fn, use_proba=False):\n",
    "        scores = []\n",
    "        for _ in range(n_bootstraps):\n",
    "            idx = rng.choice(n, n, replace=True)\n",
    "            try:\n",
    "                scores.append(fn(y_true[idx], (y_proba if use_proba else y_pred)[idx]))\n",
    "            except Exception:\n",
    "                continue\n",
    "        scores = np.array(scores)\n",
    "        point = fn(y_true, y_proba if use_proba else y_pred)\n",
    "        lo, hi = np.percentile(scores, [2.5, 97.5])\n",
    "        return (point, lo, hi)\n",
    "\n",
    "    return {\n",
    "        \"auroc\": boot(roc_auc_score, use_proba=True),\n",
    "        \"auprc\": boot(average_precision_score, use_proba=True),\n",
    "        \"accuracy\": boot(accuracy_score),\n",
    "        \"f1\": boot(f1_score),\n",
    "        \"brier\": boot(brier_score_loss, use_proba=True),\n",
    "    }\n",
    "\n",
    "\n",
    "def print_metrics_with_ci(metrics, title=\"Metrics\"):\n",
    "    print(f\"\\n{title}:\")\n",
    "    for k, (m, lo, hi) in metrics.items():\n",
    "        print(f\"{k:10s}: {m:.4f} (95% CI: {lo:.4f}-{hi:.4f})\")\n",
    "\n",
    "\n",
    "def eval_torch_model(model, x):\n",
    "    ds = TensorDataset(torch.tensor(x, dtype=torch.float32))\n",
    "    loader = DataLoader(ds, batch_size=256, shuffle=False)\n",
    "    model.to(device).eval()\n",
    "    out = []\n",
    "    with torch.no_grad():\n",
    "        for (xb,) in loader:\n",
    "            xb = xb.to(device)\n",
    "            out.append(torch.sigmoid(model(xb)).cpu())\n",
    "    return torch.cat(out).numpy().squeeze()\n",
    "\n",
    "\n",
    "def build_mlp_from_state(state_dict, input_dim):\n",
    "    w_keys = [k for k in state_dict if k.endswith(\"weight\") and \"model\" in k and state_dict[k].dim() == 2]\n",
    "    w_keys = sorted(w_keys, key=lambda k: int(k.split(\".\")[1]))\n",
    "    sizes = []\n",
    "    for k in w_keys:\n",
    "        out_f, in_f = state_dict[k].shape\n",
    "        sizes.append((in_f, out_f))\n",
    "    if len(sizes) < 2:\n",
    "        raise ValueError(\"Cannot infer MLP shapes from checkpoint.\")\n",
    "    h1 = sizes[0][1]\n",
    "    h2 = sizes[1][1]\n",
    "    h3 = sizes[2][1] if len(sizes) > 2 else sizes[1][1] // 2\n",
    "    import torch.nn as nn\n",
    "    mlp = nn.Sequential(\n",
    "        nn.Linear(input_dim, h1),\n",
    "        nn.BatchNorm1d(h1),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(h1, h2),\n",
    "        nn.BatchNorm1d(h2),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(h2, h3),\n",
    "        nn.BatchNorm1d(h3),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(h3, 1),\n",
    "    )\n",
    "    return mlp\n",
    "\n",
    "\n",
    "with open(DATA_DIR / \"preprocessed_data.pkl\", \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "\n",
    "def grab(keys):\n",
    "    for k in keys:\n",
    "        if k in data:\n",
    "            return data[k]\n",
    "    raise KeyError(keys)\n",
    "\n",
    "\n",
    "X_test = np.asarray(grab([\"X_test_scaled\", \"X_test\"]))\n",
    "y_test = np.asarray(grab([\"y_test\"])).ravel()\n",
    "\n",
    "X_val = np.asarray(grab([\"X_val_scaled\", \"X_val\"]))\n",
    "y_val = np.asarray(grab([\"y_val\"])).ravel()\n",
    "\n",
    "test_loader = DataLoader(TensorDataset(torch.tensor(X_test, dtype=torch.float32)), batch_size=256, shuffle=False)\n",
    "val_loader = DataLoader(TensorDataset(torch.tensor(X_val, dtype=torch.float32)), batch_size=256, shuffle=False)\n",
    "\n",
    "baseline_results = {}\n",
    "\n",
    "sk_models = {\n",
    "    \"Logistic Regression\": MODELS_DIR / \"logistic_regression.pkl\",\n",
    "    \"Random Forest\": MODELS_DIR / \"random_forest.pkl\",\n",
    "    \"XGBoost\": MODELS_DIR / \"xgboost_model.pkl\",\n",
    "    \"LightGBM\": MODELS_DIR / \"lightgbm_model.pkl\",\n",
    "}\n",
    "for name, path in sk_models.items():\n",
    "    if not path.exists():\n",
    "        continue\n",
    "    with open(path, \"rb\") as f:\n",
    "        model = pickle.load(f)\n",
    "    proba = model.predict_proba(X_test)[:, 1]\n",
    "    pred = (proba >= 0.5).astype(int)\n",
    "    metrics = compute_metrics_with_ci(y_test, pred, proba, n_bootstraps=N_BOOTSTRAPS)\n",
    "    print_metrics_with_ci(metrics, f\"{name} Test Results\")\n",
    "    baseline_results[name] = {\"model\": model, \"predictions\": pred, \"probabilities\": proba, \"metrics\": metrics}\n",
    "\n",
    "tabnet_path = MODELS_DIR / \"tabnet_model.pkl\"\n",
    "if tabnet_path.exists() and \"TabNetWrapper\" in globals():\n",
    "    with open(tabnet_path, \"rb\") as f:\n",
    "        tabnet_model = pickle.load(f)\n",
    "    proba = tabnet_model.predict_proba(X_test)[:, 1]\n",
    "    pred = (proba >= 0.5).astype(int)\n",
    "    metrics = compute_metrics_with_ci(y_test, pred, proba, n_bootstraps=N_BOOTSTRAPS)\n",
    "    print_metrics_with_ci(metrics, \"TabNet Test Results\")\n",
    "    baseline_results[\"TabNet\"] = {\"model\": tabnet_model, \"predictions\": pred, \"probabilities\": proba, \"metrics\": metrics}\n",
    "else:\n",
    "    print(\"TabNet skipped (model file missing or TabNetWrapper not in scope)\")\n",
    "\n",
    "mlp_paths = [\n",
    "    MODELS_DIR / \"enhanced_mlp_optimized.pt\",\n",
    "    MODELS_DIR / \"mlp_model.pt\",\n",
    "    MODELS_DIR / \"mlp_study.pkl\",\n",
    "]\n",
    "mlp_state_path = next((p for p in mlp_paths if p.exists()), None)\n",
    "if mlp_state_path is not None:\n",
    "    mlp_model = None\n",
    "    try:\n",
    "        if mlp_state_path.suffix == \".pkl\":\n",
    "            with open(mlp_state_path, \"rb\") as f:\n",
    "                maybe_model = pickle.load(f)\n",
    "            if hasattr(maybe_model, \"predict_proba\"):\n",
    "                proba = maybe_model.predict_proba(X_test)[:, 1]\n",
    "                pred = (proba >= 0.5).astype(int)\n",
    "                metrics = compute_metrics_with_ci(y_test, pred, proba, n_bootstraps=N_BOOTSTRAPS)\n",
    "                print_metrics_with_ci(metrics, \"MLP (pkl) Test Results\")\n",
    "                baseline_results[\"Enhanced MLP\"] = {\"model\": maybe_model, \"predictions\": pred, \"probabilities\": proba, \"metrics\": metrics}\n",
    "            else:\n",
    "                print(f\"MLP skipped: pkl file not a model with predict_proba ({mlp_state_path})\")\n",
    "        else:\n",
    "            state = torch.load(mlp_state_path, map_location=device, weights_only=False)\n",
    "            if \"EnhancedMLP\" in globals():\n",
    "                mlp_model = EnhancedMLP(input_dim=X_test.shape[1])\n",
    "                try:\n",
    "                    mlp_model.load_state_dict(state)\n",
    "                except RuntimeError:\n",
    "                    mlp_model.load_state_dict(state, strict=False)\n",
    "            else:\n",
    "                mlp_model = build_mlp_from_state(state, input_dim=X_test.shape[1])\n",
    "                mlp_model.load_state_dict(state, strict=False)\n",
    "            proba = eval_torch_model(mlp_model, X_test)\n",
    "            pred = (proba >= 0.5).astype(int)\n",
    "            metrics = compute_metrics_with_ci(y_test, pred, proba, n_bootstraps=N_BOOTSTRAPS)\n",
    "            print_metrics_with_ci(metrics, \"Enhanced MLP Test Results\")\n",
    "            baseline_results[\"Enhanced MLP\"] = {\"model\": mlp_model, \"predictions\": pred, \"probabilities\": proba, \"metrics\": metrics}\n",
    "    except Exception as e2:\n",
    "        print(f\"Enhanced MLP skipped: {e2}\")\n",
    "else:\n",
    "    print(\"Enhanced MLP skipped (state file missing)\")\n",
    "\n",
    "saint_path = MODELS_DIR / \"saint_model.pt\"\n",
    "if saint_path.exists() and \"SAINTModel\" in globals():\n",
    "    saint_model = SAINTModel(input_dim=X_test.shape[1])\n",
    "    saint_model.load_state_dict(torch.load(saint_path, map_location=device))\n",
    "    proba = eval_torch_model(saint_model, X_test)\n",
    "    pred = (proba >= 0.5).astype(int)\n",
    "    metrics = compute_metrics_with_ci(y_test, pred, proba, n_bootstraps=N_BOOTSTRAPS)\n",
    "    print_metrics_with_ci(metrics, \"SAINT Test Results\")\n",
    "    baseline_results[\"SAINT\"] = {\"model\": saint_model, \"predictions\": pred, \"probabilities\": proba, \"metrics\": metrics}\n",
    "else:\n",
    "    print(\"SAINT skipped (state file missing or class not in scope)\")\n",
    "\n",
    "ftt_path = MODELS_DIR / \"fttransformer_model.pt\"\n",
    "if ftt_path.exists() and \"FTTransformer\" in globals():\n",
    "    ftt_model = FTTransformer(\n",
    "        n_cont_features=X_test.shape[1],\n",
    "        d_token=192,\n",
    "        n_layers=4,\n",
    "        n_heads=4,\n",
    "        d_ffn=int(192 * 1.02),\n",
    "        dropout=0.408,\n",
    "        attention_dropout=0.408,\n",
    "    )\n",
    "    ftt_model.load_state_dict(torch.load(ftt_path, map_location=device))\n",
    "    proba = eval_torch_model(ftt_model, X_test)\n",
    "    pred = (proba >= 0.5).astype(int)\n",
    "    metrics = compute_metrics_with_ci(y_test, pred, proba, n_bootstraps=N_BOOTSTRAPS)\n",
    "    print_metrics_with_ci(metrics, \"FT-Transformer Test Results\")\n",
    "    baseline_results[\"FT-Transformer\"] = {\"model\": ftt_model, \"predictions\": pred, \"probabilities\": proba, \"metrics\": metrics}\n",
    "else:\n",
    "    print(\"FT-Transformer skipped (state file missing or class not in scope)\")\n",
    "\n",
    "resnet_path = MODELS_DIR / \"resnet_model_optimized.pt\"\n",
    "if resnet_path.exists() and \"ClinicalResNet\" in globals():\n",
    "    state = torch.load(resnet_path, map_location=device)\n",
    "    block_ids = []\n",
    "    for k in state.keys():\n",
    "        if k.startswith(\"blocks.\"):\n",
    "            parts = k.split(\".\")\n",
    "            if len(parts) > 1 and parts[1].isdigit():\n",
    "                block_ids.append(int(parts[1]))\n",
    "    num_blocks = max(block_ids) + 1 if block_ids else 2\n",
    "\n",
    "    rn_model = ClinicalResNet(\n",
    "        input_dim=X_test.shape[1],\n",
    "        hidden_dim=128,\n",
    "        num_blocks=num_blocks,\n",
    "        dropout=0.4,\n",
    "    )\n",
    "    rn_model.load_state_dict(state, strict=False)\n",
    "    proba = eval_torch_model(rn_model, X_test)\n",
    "    pred = (proba >= 0.5).astype(int)\n",
    "    metrics = compute_metrics_with_ci(y_test, pred, proba, n_bootstraps=N_BOOTSTRAPS)\n",
    "    print_metrics_with_ci(metrics, \"Clinical ResNet Test Results\")\n",
    "    baseline_results[\"ClinicalResNet\"] = {\"model\": rn_model, \"predictions\": pred, \"probabilities\": proba, \"metrics\": metrics}\n",
    "else:\n",
    "    print(\"Clinical ResNet skipped (state file missing or class not in scope)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Geh54-XYXut5"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np, torch\n",
    "\n",
    "def loader_to_array(loader):\n",
    "    xs = []\n",
    "    for batch in loader:\n",
    "        xb = batch[0] if isinstance(batch, (list, tuple)) else batch\n",
    "        xs.append(xb)\n",
    "    return torch.cat(xs).cpu().numpy()\n",
    "\n",
    "def torch_probs(model, loader):\n",
    "    model.eval().to(device)\n",
    "    out = []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            xb = batch[0] if isinstance(batch, (list, tuple)) else batch\n",
    "            xb = xb.to(device)\n",
    "            out.append(torch.sigmoid(model(xb)).cpu())\n",
    "    return torch.cat(out).numpy().squeeze()\n",
    "\n",
    "def get_model_probs(model_name, loader):\n",
    "    model = baseline_models[model_name]\n",
    "    if hasattr(model, \"parameters\"):\n",
    "        return torch_probs(model, loader)\n",
    "    else:\n",
    "        X_data = loader_to_array(loader)\n",
    "        return model.predict_proba(X_data)[:, 1]\n",
    "\n",
    "sorted_models = sorted(\n",
    "    baseline_results.items(),\n",
    "    key=lambda item: item[1][\"metrics\"][\"auroc\"][0],\n",
    "    reverse=True,\n",
    ")\n",
    "top_3_names = [name for name, _ in sorted_models[:3]]\n",
    "print(f\"Top 3 Models selected for Stacking: {top_3_names}\")\n",
    "\n",
    "val_meta_features = [get_model_probs(name, val_loader) for name in top_3_names]\n",
    "test_meta_features = [get_model_probs(name, test_loader) for name in top_3_names]\n",
    "\n",
    "X_val_meta = np.column_stack(val_meta_features)\n",
    "X_test_meta = np.column_stack(test_meta_features)\n",
    "\n",
    "meta_model = LogisticRegression(random_state=config.RANDOM_STATE)\n",
    "meta_model.fit(X_val_meta, y_val)\n",
    "\n",
    "print(\"\\nEnsemble Weights:\")\n",
    "for name, weight in zip(top_3_names, meta_model.coef_[0]):\n",
    "    print(f\"  {name}: {weight:.4f}\")\n",
    "\n",
    "val_probs_ens = meta_model.predict_proba(X_val_meta)[:, 1]\n",
    "test_probs_ens = meta_model.predict_proba(X_test_meta)[:, 1]\n",
    "\n",
    "best_thresh_ens, best_f1_ens = get_optimal_threshold(y_val, val_probs_ens)\n",
    "print(f\"\\nStacked Ensemble - Optimal Threshold: {best_thresh_ens:.4f}\")\n",
    "\n",
    "y_test_pred_ens = (test_probs_ens >= best_thresh_ens).astype(int)\n",
    "\n",
    "metrics_ens = compute_metrics_with_ci(y_test, y_test_pred_ens, test_probs_ens, n_bootstraps=config.N_BOOTSTRAP)\n",
    "print_metrics_with_ci(metrics_ens, \"Stacked Ensemble Results\")\n",
    "\n",
    "baseline_results[\"StackedEnsemble\"] = {\n",
    "    \"predictions\": y_test_pred_ens,\n",
    "    \"probabilities\": test_probs_ens,\n",
    "    \"metrics\": metrics_ens,\n",
    "    \"optimal_threshold\": best_thresh_ens,\n",
    "    \"constituent_models\": top_3_names,\n",
    "}\n",
    "\n",
    "with open(config.MODELS_DIR / \"stacking_meta_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(meta_model, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fcLUBKIMXut5"
   },
   "source": [
    "## Model Comparison and Statistical Testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "glyP8mIhXut5"
   },
   "outputs": [],
   "source": [
    "model_names = list(baseline_results.keys())\n",
    "n_models = len(model_names)\n",
    "delong_results = []\n",
    "for i in range(n_models):\n",
    "    for j in range(i+1, n_models):\n",
    "        model1_name = model_names[i]\n",
    "        model2_name = model_names[j]\n",
    "        proba1 = baseline_results[model1_name]['probabilities']\n",
    "        proba2 = baseline_results[model2_name]['probabilities']\n",
    "        z_score, p_value = delong_test(y_test, proba1, proba2)\n",
    "        delong_results.append({\n",
    "            'Model 1': model1_name,\n",
    "            'Model 2': model2_name,\n",
    "            'Z-Score': z_score,\n",
    "            'P-Value': p_value,\n",
    "            'Significant': 'Yes' if p_value < 0.05 else 'No'\n",
    "        })\n",
    "delong_df = pd.DataFrame(delong_results)\n",
    "print(delong_df.to_string(index=False))\n",
    "\n",
    "# Fix: Use Path directly\n",
    "from pathlib import Path\n",
    "RESULTS_DIR = Path(\"/kaggle/working/results\")\n",
    "delong_df.to_csv(RESULTS_DIR / \"delong_test_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gs-rvsdbXut6"
   },
   "outputs": [],
   "source": [
    "results_summary = []\n",
    "\n",
    "for model_name, model_results in baseline_results.items():\n",
    "    metrics = model_results['metrics']\n",
    "\n",
    "    row = {\n",
    "        'Model': model_name,\n",
    "        'AUROC': f\"{metrics['auroc'][0]:.4f} [{metrics['auroc'][1]:.4f}, {metrics['auroc'][2]:.4f}]\",\n",
    "        'AUPRC': f\"{metrics['auprc'][0]:.4f} [{metrics['auprc'][1]:.4f}, {metrics['auprc'][2]:.4f}]\",\n",
    "        'Accuracy': f\"{metrics['accuracy'][0]:.4f} [{metrics['accuracy'][1]:.4f}, {metrics['accuracy'][2]:.4f}]\",\n",
    "        'F1': f\"{metrics['f1'][0]:.4f} [{metrics['f1'][1]:.4f}, {metrics['f1'][2]:.4f}]\",\n",
    "        'Brier': f\"{metrics['brier'][0]:.4f} [{metrics['brier'][1]:.4f}, {metrics['brier'][2]:.4f}]\"\n",
    "    }\n",
    "\n",
    "    results_summary.append(row)\n",
    "\n",
    "results_df = pd.DataFrame(results_summary)\n",
    "\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "results_df.to_csv(config.RESULTS_DIR / \"baseline_results_summary.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rszCtfHcXut6"
   },
   "source": [
    "## Visualization: Model Performance Comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YLR3XLCoXut6"
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=config.FIG_SIZE_WIDE)\n",
    "\n",
    "ax = axes[0]\n",
    "for model_name, model_results in baseline_results.items():\n",
    "    proba = model_results['probabilities']\n",
    "    fpr, tpr, _ = roc_curve(y_test, proba)\n",
    "    auroc = model_results['metrics']['auroc'][0]\n",
    "    ax.plot(fpr, tpr, label=f\"{model_name} (AUROC={auroc:.3f})\", linewidth=2)\n",
    "\n",
    "ax.plot([0, 1], [0, 1], 'k--', label='Random', linewidth=1)\n",
    "ax.set_xlabel('False Positive Rate')\n",
    "ax.set_ylabel('True Positive Rate')\n",
    "ax.set_title('ROC Curves - All Models')\n",
    "ax.legend(loc='lower right', fontsize=8)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "ax = axes[1]\n",
    "for model_name, model_results in baseline_results.items():\n",
    "    proba = model_results['probabilities']\n",
    "    precision, recall, _ = precision_recall_curve(y_test, proba)\n",
    "    auprc = model_results['metrics']['auprc'][0]\n",
    "    ax.plot(recall, precision, label=f\"{model_name} (AUPRC={auprc:.3f})\", linewidth=2)\n",
    "\n",
    "baseline_precision = y_test.mean()\n",
    "ax.axhline(baseline_precision, color='k', linestyle='--', label=f'Baseline ({baseline_precision:.3f})', linewidth=1)\n",
    "ax.set_xlabel('Recall')\n",
    "ax.set_ylabel('Precision')\n",
    "ax.set_title('Precision-Recall Curves - All Models')\n",
    "ax.legend(loc='lower left', fontsize=8)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "save_figure(fig, 'model_comparison_curves')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-I2_dKxpXut6"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=config.FIG_SIZE_LARGE)\n",
    "\n",
    "model_names_plot = list(baseline_results.keys())\n",
    "auroc_values = [baseline_results[name]['metrics']['auroc'][0] for name in model_names_plot]\n",
    "auroc_ci_lower = [baseline_results[name]['metrics']['auroc'][1] for name in model_names_plot]\n",
    "auroc_ci_upper = [baseline_results[name]['metrics']['auroc'][2] for name in model_names_plot]\n",
    "\n",
    "yerr_lower = [val - ci_low for val, ci_low in zip(auroc_values, auroc_ci_lower)]\n",
    "yerr_upper = [ci_up - val for val, ci_up in zip(auroc_values, auroc_ci_upper)]\n",
    "\n",
    "x_pos = np.arange(len(model_names_plot))\n",
    "ax.bar(x_pos, auroc_values, yerr=[yerr_lower, yerr_upper],\n",
    "       capsize=5, alpha=0.7, color='steelblue')\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels(model_names_plot, rotation=45, ha='right')\n",
    "ax.set_ylabel('AUROC')\n",
    "ax.set_title('Model Performance Comparison (with 95% CI)')\n",
    "ax.set_ylim([min(auroc_values) - 0.05, max(auroc_values) + 0.05])\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "save_figure(fig, 'model_auroc_comparison')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A2grwoSnXut7"
   },
   "source": [
    "# PART 4: Calibration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jSsl0XNzXut7"
   },
   "source": [
    "## Calibration Utilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P9VyfMjWXut7"
   },
   "outputs": [],
   "source": [
    "class TemperatureScaling:\n",
    "    def __init__(self):\n",
    "        self.temperature = None\n",
    "\n",
    "    def fit(self, y_true: np.ndarray, y_pred_logits: np.ndarray):\n",
    "        from scipy.optimize import minimize\n",
    "\n",
    "        def neg_log_likelihood(temp):\n",
    "            scaled_probs = 1 / (1 + np.exp(-y_pred_logits / temp))\n",
    "            epsilon = 1e-15\n",
    "            scaled_probs = np.clip(scaled_probs, epsilon, 1 - epsilon)\n",
    "            loss = -np.mean(y_true * np.log(scaled_probs) + (1 - y_true) * np.log(1 - scaled_probs))\n",
    "            return loss\n",
    "\n",
    "        result = minimize(neg_log_likelihood, x0=1.0, bounds=[(0.01, 10.0)], method='L-BFGS-B')\n",
    "        self.temperature = result.x[0]\n",
    "\n",
    "    def transform(self, y_pred_logits: np.ndarray) -> np.ndarray:\n",
    "        if self.temperature is None:\n",
    "            raise ValueError(\"Must fit before transform\")\n",
    "\n",
    "        scaled_probs = 1 / (1 + np.exp(-y_pred_logits / self.temperature))\n",
    "        return scaled_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V8_2RmjOXut7"
   },
   "outputs": [],
   "source": [
    "def compute_ece(y_true: np.ndarray, y_prob: np.ndarray, n_bins: int = 10) -> float:\n",
    "    bin_boundaries = np.linspace(0, 1, n_bins + 1)\n",
    "    bin_lowers = bin_boundaries[:-1]\n",
    "    bin_uppers = bin_boundaries[1:]\n",
    "\n",
    "    ece = 0.0\n",
    "    for bin_lower, bin_upper in zip(bin_lowers, bin_uppers):\n",
    "        in_bin = np.logical_and(y_prob > bin_lower, y_prob <= bin_upper)\n",
    "        prop_in_bin = np.mean(in_bin)\n",
    "\n",
    "        if prop_in_bin > 0:\n",
    "            accuracy_in_bin = np.mean(y_true[in_bin])\n",
    "            avg_confidence_in_bin = np.mean(y_prob[in_bin])\n",
    "            ece += np.abs(accuracy_in_bin - avg_confidence_in_bin) * prop_in_bin\n",
    "\n",
    "    return ece\n",
    "\n",
    "def compute_mce(y_true: np.ndarray, y_prob: np.ndarray, n_bins: int = 10) -> float:\n",
    "    bin_boundaries = np.linspace(0, 1, n_bins + 1)\n",
    "    bin_lowers = bin_boundaries[:-1]\n",
    "    bin_uppers = bin_boundaries[1:]\n",
    "\n",
    "    max_ce = 0.0\n",
    "    for bin_lower, bin_upper in zip(bin_lowers, bin_uppers):\n",
    "        in_bin = np.logical_and(y_prob > bin_lower, y_prob <= bin_upper)\n",
    "\n",
    "        if np.sum(in_bin) > 0:\n",
    "            accuracy_in_bin = np.mean(y_true[in_bin])\n",
    "            avg_confidence_in_bin = np.mean(y_prob[in_bin])\n",
    "            ce = np.abs(accuracy_in_bin - avg_confidence_in_bin)\n",
    "            max_ce = max(max_ce, ce)\n",
    "\n",
    "    return max_ce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CyJZemZqXut7"
   },
   "source": [
    "## Calibrate All Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4lw9o89IXut7"
   },
   "outputs": [],
   "source": [
    "import numpy as np, torch\n",
    "from sklearn.calibration import IsotonicRegression\n",
    "from sklearn.metrics import brier_score_loss\n",
    "import pandas as pd, matplotlib.pyplot as plt, seaborn as sns\n",
    "\n",
    "def loader_to_array(loader):\n",
    "    xs = []\n",
    "    for batch in loader:\n",
    "        xb = batch[0] if isinstance(batch, (list, tuple)) else batch\n",
    "        xs.append(xb)\n",
    "    return torch.cat(xs).cpu().numpy()\n",
    "\n",
    "def get_probs_robust(model, loader, is_torch):\n",
    "    if is_torch:\n",
    "        model.eval().to(device)\n",
    "        out = []\n",
    "        with torch.no_grad():\n",
    "            for batch in loader:\n",
    "                xb = batch[0] if isinstance(batch, (list, tuple)) else batch\n",
    "                xb = xb.to(device)\n",
    "                out.append(torch.sigmoid(model(xb)).cpu())\n",
    "        return torch.cat(out).numpy().squeeze()\n",
    "    else:\n",
    "        X_data = loader_to_array(loader)\n",
    "        return model.predict_proba(X_data)[:, 1]\n",
    "\n",
    "calibrated_results = {}\n",
    "calibration_stats = []\n",
    "models_to_calibrate = list(baseline_results.keys())\n",
    "\n",
    "for name in models_to_calibrate:\n",
    "    if name == \"StackedEnsemble\":\n",
    "        top_3 = baseline_results['StackedEnsemble']['constituent_models']\n",
    "        val_metas = [get_probs_robust(baseline_models[m], val_loader, hasattr(baseline_models[m],'parameters')) for m in top_3]\n",
    "        X_val_meta_re = np.column_stack(val_metas)\n",
    "        val_probs_uncal = meta_model.predict_proba(X_val_meta_re)[:, 1]\n",
    "        test_probs_uncal = baseline_results[name]['probabilities']\n",
    "    else:\n",
    "        model = baseline_models[name]\n",
    "        is_torch = hasattr(model, 'parameters')\n",
    "        val_probs_uncal = get_probs_robust(model, val_loader, is_torch)\n",
    "        test_probs_uncal = baseline_results[name]['probabilities']\n",
    "\n",
    "    iso_reg = IsotonicRegression(out_of_bounds='clip')\n",
    "    iso_reg.fit(val_probs_uncal, y_val)\n",
    "\n",
    "    test_probs_cal = iso_reg.predict(test_probs_uncal)\n",
    "\n",
    "    ece_pre = compute_ece(y_test, test_probs_uncal)\n",
    "    brier_pre = brier_score_loss(y_test, test_probs_uncal)\n",
    "\n",
    "    ece_post = compute_ece(y_test, test_probs_cal)\n",
    "    brier_post = brier_score_loss(y_test, test_probs_cal)\n",
    "\n",
    "    print(f\"[{name}] ECE: {ece_pre:.4f} -> {ece_post:.4f} | Brier: {brier_pre:.4f} -> {brier_post:.4f}\")\n",
    "\n",
    "    calibrated_results[name] = {\n",
    "        'probs_uncal': test_probs_uncal,\n",
    "        'probs_cal': test_probs_cal,\n",
    "        'ece_post': ece_post,\n",
    "        'brier_post': brier_post\n",
    "    }\n",
    "    calibration_stats.append({\n",
    "        'Model': name,\n",
    "        'ECE_Pre': ece_pre,\n",
    "        'ECE_Post': ece_post,\n",
    "        'Brier_Pre': brier_pre,\n",
    "        'Brier_Post': brier_post,\n",
    "        'Improvement_ECE': (ece_pre - ece_post) / ece_pre if ece_pre else 0.0\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6SYXdsz1Xut8"
   },
   "source": [
    "## Calibration Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JFs_h8L0Xut8"
   },
   "outputs": [],
   "source": [
    "from sklearn.calibration import calibration_curve\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "RESULTS_DIR = Path(\"/kaggle/working/results\")\n",
    "FIGURES_DIR = Path(\"/kaggle/working/figures\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
    "\n",
    "# Plot 1: Reliability Diagram\n",
    "ax1 = axes[0]\n",
    "ax1.plot([0, 1], [0, 1], \"k:\", label=\"Perfectly Calibrated\", linewidth=2)\n",
    "\n",
    "plot_names = ['XGBoost', 'ClinicalResNet', 'StackedEnsemble']\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c']\n",
    "\n",
    "for name, color in zip(plot_names, colors):\n",
    "    if name in calibrated_results:\n",
    "        probs_uncal = calibrated_results[name]['probs_uncal']\n",
    "        probs_cal = calibrated_results[name]['probs_cal']\n",
    "        \n",
    "        prob_true, prob_pred = calibration_curve(y_test, probs_uncal, n_bins=10, strategy='uniform')\n",
    "        ax1.plot(prob_pred, prob_true, linestyle=\"--\", color=color, alpha=0.5, label=f\"{name} (Uncal)\")\n",
    "        \n",
    "        prob_true_cal, prob_pred_cal = calibration_curve(y_test, probs_cal, n_bins=10, strategy='uniform')\n",
    "        ax1.plot(prob_pred_cal, prob_true_cal, marker='o', linestyle=\"-\", color=color, linewidth=2,\n",
    "                 label=f\"{name} (Calibrated) ECE={calibrated_results[name]['ece_post']:.3f}\")\n",
    "\n",
    "ax1.set_xlabel(\"Mean Predicted Probability\", fontsize=11)\n",
    "ax1.set_ylabel(\"Fraction of Positives\", fontsize=11)\n",
    "ax1.set_title(\"Calibration Curves (Reliability Diagram)\", fontsize=12)\n",
    "ax1.legend(loc=\"lower right\", fontsize=9)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_xlim([0, 1])\n",
    "ax1.set_ylim([0, 1])\n",
    "\n",
    "# Plot 2: Probability Distribution\n",
    "ax2 = axes[1]\n",
    "if 'StackedEnsemble' in calibrated_results:\n",
    "    probs_cal = calibrated_results['StackedEnsemble']['probs_cal']\n",
    "    sns.histplot(probs_cal[y_test == 0], color=\"blue\", alpha=0.5, label=\"Survivors\", bins=20, ax=ax2)\n",
    "    sns.histplot(probs_cal[y_test == 1], color=\"red\", alpha=0.5, label=\"Deaths\", bins=20, ax=ax2)\n",
    "    ax2.set_xlabel(\"Calibrated Probability\", fontsize=11)\n",
    "    ax2.set_ylabel(\"Count\", fontsize=11)\n",
    "    ax2.set_title(\"Calibrated Probability Distribution (Stacked Ensemble)\", fontsize=12)\n",
    "    ax2.legend(fontsize=10)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Bin sample counts\n",
    "ax3 = axes[2]\n",
    "n_bins = 10\n",
    "bins = np.linspace(0, 1, n_bins + 1)\n",
    "\n",
    "if 'StackedEnsemble' in calibrated_results:\n",
    "    probs_cal = calibrated_results['StackedEnsemble']['probs_cal']\n",
    "    bin_counts = []\n",
    "    bin_centers = []\n",
    "    bin_accuracies = []\n",
    "    bin_confidences = []\n",
    "    \n",
    "    for i in range(n_bins):\n",
    "        mask = (probs_cal >= bins[i]) & (probs_cal < bins[i + 1])\n",
    "        if i == n_bins - 1:\n",
    "            mask = (probs_cal >= bins[i]) & (probs_cal <= bins[i + 1])\n",
    "        \n",
    "        count = mask.sum()\n",
    "        bin_counts.append(count)\n",
    "        bin_centers.append((bins[i] + bins[i + 1]) / 2)\n",
    "        \n",
    "        if count > 0:\n",
    "            bin_accuracies.append(y_test[mask].mean())\n",
    "            bin_confidences.append(probs_cal[mask].mean())\n",
    "        else:\n",
    "            bin_accuracies.append(0)\n",
    "            bin_confidences.append(0)\n",
    "    \n",
    "    bars = ax3.bar(bin_centers, bin_counts, width=0.08, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "    \n",
    "    for bar, count in zip(bars, bin_counts):\n",
    "        if count > 0:\n",
    "            ax3.annotate(f'{count}', xy=(bar.get_x() + bar.get_width() / 2, bar.get_height()),\n",
    "                        ha='center', va='bottom', fontsize=8)\n",
    "    \n",
    "    ax3.set_xlabel(\"Probability Bin\", fontsize=11)\n",
    "    ax3.set_ylabel(\"Sample Count\", fontsize=11)\n",
    "    ax3.set_title(\"Samples per Calibration Bin (Stacked Ensemble)\", fontsize=12)\n",
    "    ax3.set_xticks(bin_centers)\n",
    "    ax3.set_xticklabels([f'{bins[i]:.1f}-{bins[i+1]:.1f}' for i in range(n_bins)], rotation=45, ha='right', fontsize=8)\n",
    "    ax3.grid(True, alpha=0.3, axis='y')\n",
    "    ax3.text(0.02, 0.98, f'Total n={len(probs_cal)}', transform=ax3.transAxes, fontsize=9, verticalalignment='top')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / \"calibration_plots_detailed.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Print bin statistics\n",
    "print(\"\\nBin Statistics (Stacked Ensemble):\")\n",
    "print(f\"{'Bin':<12} {'Count':>8} {'Accuracy':>10} {'Confidence':>12} {'|Error|':>10}\")\n",
    "print(\"-\" * 55)\n",
    "for i in range(n_bins):\n",
    "    error = abs(bin_accuracies[i] - bin_confidences[i]) if bin_counts[i] > 0 else 0\n",
    "    print(f\"{bins[i]:.1f}-{bins[i+1]:.1f}    {bin_counts[i]:>8} {bin_accuracies[i]:>10.4f} {bin_confidences[i]:>12.4f} {error:>10.4f}\")\n",
    "\n",
    "df_cal = pd.DataFrame(calibration_stats).sort_values(\"Brier_Post\")\n",
    "print(\"\\n\" + df_cal.to_string(index=False))\n",
    "df_cal.to_csv(RESULTS_DIR / \"calibration_summary.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_FkemnG7Xut8"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve, confusion_matrix, brier_score_loss\n",
    "from sklearn.calibration import calibration_curve\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.dpi'] = 300\n",
    "plt.rcParams['savefig.dpi'] = 300\n",
    "plt.rcParams['font.size'] = 10\n",
    "plt.rcParams['axes.labelsize'] = 11\n",
    "plt.rcParams['axes.titlesize'] = 12\n",
    "plt.rcParams['legend.fontsize'] = 9\n",
    "\n",
    "os.makedirs(config.BASE_DIR / 'figures', exist_ok=True)\n",
    "\n",
    "colors = {\n",
    "    'Ensemble (Calibrated)': '#e74c3c',\n",
    "    'Stacked Ensemble': '#e74c3c',\n",
    "    'LightGBM': '#3498db',\n",
    "    'XGBoost': '#2ecc71',\n",
    "    'ClinicalResNet': '#9b59b6',\n",
    "    'SAINT': '#9b59b6',\n",
    "    'Enhanced MLP': '#f39c12',\n",
    "    'Random Forest': '#1abc9c',\n",
    "    'Logistic Regression': '#95a5a6'\n",
    "}\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "available_models = [m for m in baseline_results.keys() if m in colors or m == 'Ensemble (Calibrated)']\n",
    "\n",
    "for model_name in available_models:\n",
    "    if model_name in baseline_results:\n",
    "        y_proba = baseline_results[model_name]['probabilities']\n",
    "        fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "\n",
    "        auroc_metric = baseline_results[model_name]['metrics']['auroc']\n",
    "        if isinstance(auroc_metric, tuple):\n",
    "            auroc_mean = auroc_metric[0]\n",
    "            label = f'{model_name} (AUC={auroc_mean:.3f})'\n",
    "        else:\n",
    "            label = f'{model_name} (AUC={roc_auc:.3f})'\n",
    "\n",
    "        linewidth = 3 if 'Ensemble' in model_name else 2\n",
    "        linestyle = '-' if 'Ensemble' in model_name else '--'\n",
    "\n",
    "        c = colors.get(model_name, '#333333')\n",
    "\n",
    "        ax.plot(fpr, tpr, color=c, lw=linewidth, linestyle=linestyle, label=label)\n",
    "\n",
    "ax.plot([0, 1], [0, 1], 'k--', lw=1, label='Random Classifier (AUC=0.500)')\n",
    "\n",
    "ax.set_xlim([0.0, 1.0])\n",
    "ax.set_ylim([0.0, 1.05])\n",
    "ax.set_xlabel('False Positive Rate (1 - Specificity)', fontweight='bold')\n",
    "ax.set_ylabel('True Positive Rate (Sensitivity)', fontweight='bold')\n",
    "ax.set_title('ROC Curves - All Models Comparison', fontweight='bold')\n",
    "ax.legend(loc=\"lower right\", frameon=True, shadow=True)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(config.BASE_DIR / 'figures' / 'roc_curves_all_models.png')\n",
    "plt.close()\n",
    "\n",
    "n_models = len(available_models)\n",
    "n_cols = 3\n",
    "n_rows = (n_models + n_cols - 1) // n_cols\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(5*n_cols, 4*n_rows))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, model_name in enumerate(available_models):\n",
    "    if idx >= len(axes): break\n",
    "    ax = axes[idx]\n",
    "\n",
    "    y_proba = baseline_results[model_name]['probabilities']\n",
    "\n",
    "    if model_name in calibrated_results:\n",
    "        ece = calibrated_results[model_name]['ece_post']\n",
    "        brier = calibrated_results[model_name]['brier_post']\n",
    "        plot_proba = calibrated_results[model_name]['probs_cal']\n",
    "        label_txt = 'Calibrated'\n",
    "    else:\n",
    "        ece = compute_ece(y_test, y_proba)\n",
    "        brier = brier_score_loss(y_test, y_proba)\n",
    "        plot_proba = y_proba\n",
    "        label_txt = 'Uncalibrated'\n",
    "\n",
    "    fraction_of_positives, mean_predicted_value = calibration_curve(\n",
    "        y_test, plot_proba, n_bins=10, strategy='uniform'\n",
    "    )\n",
    "\n",
    "    ax.plot(mean_predicted_value, fraction_of_positives, 's-',\n",
    "            color='#3498db', linewidth=2, markersize=8, label=label_txt)\n",
    "    ax.plot([0, 1], [0, 1], 'k--', lw=1.5, label='Perfect')\n",
    "\n",
    "    ax.set_xlabel('Mean Predicted Probability')\n",
    "    ax.set_ylabel('Fraction of Positives')\n",
    "    ax.set_title(f'{model_name}\\nECE={ece:.4f}, Brier={brier:.4f}', fontsize=10)\n",
    "    ax.legend(loc='upper left', fontsize=8)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "for i in range(idx + 1, len(axes)):\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(config.BASE_DIR / 'figures' / 'calibration_curves.png')\n",
    "plt.close()\n",
    "\n",
    "model_names = []\n",
    "aurocs = []\n",
    "auprcs = []\n",
    "f1s = []\n",
    "\n",
    "for name in available_models:\n",
    "    if name in baseline_results:\n",
    "        model_names.append(name)\n",
    "        metrics = baseline_results[name]['metrics']\n",
    "\n",
    "        aurocs.append(metrics['auroc'][0] if isinstance(metrics['auroc'], tuple) else metrics['auroc'])\n",
    "        auprcs.append(metrics['auprc'][0] if isinstance(metrics['auprc'], tuple) else metrics['auprc'])\n",
    "        f1s.append(metrics['f1'][0] if isinstance(metrics['f1'], tuple) else metrics['f1'])\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "def plot_bar(ax, data, title, color):\n",
    "    bars = ax.barh(model_names, data, color=color, edgecolor='black', alpha=0.7)\n",
    "    ax.set_title(title, fontweight='bold')\n",
    "    ax.grid(axis='x', alpha=0.3)\n",
    "    for i, v in enumerate(data):\n",
    "        ax.text(v + 0.005, i, f'{v:.4f}', va='center', fontweight='bold', fontsize=9)\n",
    "\n",
    "plot_bar(axes[0], aurocs, 'Discrimination (AUROC)', '#3498db')\n",
    "axes[0].axvline(x=0.87, color='red', linestyle='--', label='SOTA Target')\n",
    "axes[0].legend()\n",
    "\n",
    "plot_bar(axes[1], auprcs, 'Precision-Recall (AUPRC)', '#2ecc71')\n",
    "plot_bar(axes[2], f1s, 'F1 Score (Optimized Threshold)', '#f39c12')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(config.BASE_DIR / 'figures' / 'model_performance_comparison.png')\n",
    "plt.close()\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "has_tree_model = False\n",
    "\n",
    "if 'LightGBM' in baseline_models:\n",
    "    ax = axes[0]\n",
    "    lgbm_model = baseline_models['LightGBM']\n",
    "    importances = lgbm_model.feature_importances_\n",
    "    importances = importances / importances.sum()\n",
    "    indices = np.argsort(importances)[-15:]\n",
    "\n",
    "    ax.barh(range(15), importances[indices], color='#3498db', edgecolor='black')\n",
    "    ax.set_yticks(range(15))\n",
    "    ax.set_yticklabels([X_train_scaled.columns[i] for i in indices])\n",
    "    ax.set_title('LightGBM - Top 15 Features')\n",
    "    has_tree_model = True\n",
    "\n",
    "if 'XGBoost' in baseline_models:\n",
    "    ax = axes[1]\n",
    "    xgb_model = baseline_models['XGBoost']\n",
    "    importances = xgb_model.feature_importances_\n",
    "    importances = importances / importances.sum()\n",
    "    indices = np.argsort(importances)[-15:]\n",
    "\n",
    "    ax.barh(range(15), importances[indices], color='#2ecc71', edgecolor='black')\n",
    "    ax.set_yticks(range(15))\n",
    "    ax.set_yticklabels([X_train_scaled.columns[i] for i in indices])\n",
    "    ax.set_title('XGBoost - Top 15 Features')\n",
    "    has_tree_model = True\n",
    "\n",
    "if has_tree_model:\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(config.BASE_DIR / 'figures' / 'feature_importance.png')\n",
    "    plt.close()\n",
    "else:\n",
    "    plt.close()\n",
    "\n",
    "print(\"All visualizations generated and saved to 'figures/' folder.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-SwXGrnGXut9"
   },
   "outputs": [],
   "source": [
    "def compute_mce(y_true, y_prob, n_bins=10):\n",
    "    bin_boundaries = np.linspace(0, 1, n_bins + 1)\n",
    "    mce = 0.0\n",
    "    for i in range(n_bins):\n",
    "        bin_mask = (y_prob > bin_boundaries[i]) & (y_prob <= bin_boundaries[i+1])\n",
    "        if np.sum(bin_mask) > 0:\n",
    "            bin_acc = np.mean(y_true[bin_mask])\n",
    "            bin_conf = np.mean(y_prob[bin_mask])\n",
    "            abs_err = np.abs(bin_acc - bin_conf)\n",
    "            if abs_err > mce:\n",
    "                mce = abs_err\n",
    "    return mce\n",
    "\n",
    "cal_summary = []\n",
    "\n",
    "for model_name, res in calibrated_results.items():\n",
    "    probs_uncal = res['probs_uncal']\n",
    "    probs_cal = res['probs_cal']\n",
    "\n",
    "    ece_pre = compute_ece(y_test, probs_uncal)\n",
    "    mce_pre = compute_mce(y_test, probs_uncal)\n",
    "    brier_pre = brier_score_loss(y_test, probs_uncal)\n",
    "\n",
    "    if 'ece_post' in res:\n",
    "        ece_post = res['ece_post']\n",
    "    else:\n",
    "        ece_post = compute_ece(y_test, probs_cal)\n",
    "\n",
    "    mce_post = compute_mce(y_test, probs_cal)\n",
    "\n",
    "    if 'brier_post' in res:\n",
    "        brier_post = res['brier_post']\n",
    "    else:\n",
    "        brier_post = brier_score_loss(y_test, probs_cal)\n",
    "\n",
    "    cal_summary.append({\n",
    "        'Model': model_name,\n",
    "        'ECE Before': ece_pre,\n",
    "        'ECE After': ece_post,\n",
    "        'ECE Improvement': ece_pre - ece_post,\n",
    "        'MCE Before': mce_pre,\n",
    "        'MCE After': mce_post,\n",
    "        'Brier Before': brier_pre,\n",
    "        'Brier After': brier_post\n",
    "    })\n",
    "\n",
    "cal_df = pd.DataFrame(cal_summary)\n",
    "cal_df = cal_df.sort_values('ECE After', ascending=True)\n",
    "\n",
    "cal_df_display = cal_df.copy()\n",
    "for col in cal_df_display.columns:\n",
    "    if col != 'Model':\n",
    "        cal_df_display[col] = cal_df_display[col].apply(lambda x: f\"{x:.4f}\")\n",
    "\n",
    "print(\"Calibration Summary:\")\n",
    "print(cal_df_display.to_string(index=False))\n",
    "\n",
    "best_ece_row = cal_df.loc[cal_df['ECE After'].idxmin()]\n",
    "best_brier_row = cal_df.loc[cal_df['Brier After'].idxmin()]\n",
    "\n",
    "print(f\"\\nLowest ECE:    {best_ece_row['Model']} ({best_ece_row['ECE After']:.4f})\")\n",
    "print(f\"Lowest Brier: {best_brier_row['Model']} ({best_brier_row['Brier After']:.4f})\")\n",
    "\n",
    "cal_df.to_csv(config.RESULTS_DIR / \"calibration_results.csv\", index=False)\n",
    "print(f\"Calibration results saved to: {config.RESULTS_DIR / 'calibration_results.csv'}\")\n",
    "\n",
    "latex_table = cal_df.to_latex(\n",
    "    index=False,\n",
    "    float_format=\"%.4f\",\n",
    "    caption=\"Calibration metrics (ECE, MCE, Brier Score) before and after isotonic regression.\",\n",
    "    label=\"tab:calibration_results\"\n",
    ")\n",
    "\n",
    "with open(config.RESULTS_DIR / \"calibration_results.tex\", 'w') as f:\n",
    "    f.write(latex_table)\n",
    "print(f\"LaTeX table saved to: {config.RESULTS_DIR / 'calibration_results.tex'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mGJwauOaXut9"
   },
   "source": [
    "# PART 5: Federated Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m5ytqjbQXut9"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import copy\n",
    "from typing import Dict, List, Tuple, Any, Optional\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EaW_VVWnXut-"
   },
   "source": [
    "## 5.2 Data Partitioning for Federated Learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xOoBihqzXut-"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "enhanced_features_path = Path(\"/kaggle/input/databefore/processed_data_final/enhanced_features.pkl\")\n",
    "client_data_dir = Path(\"/kaggle/input/databefore/processed_data_final\")\n",
    "\n",
    "with open(enhanced_features_path, 'rb') as f:\n",
    "    enhanced_data = pickle.load(f)\n",
    "\n",
    "X_train_enhanced = enhanced_data['X_train']\n",
    "X_val_enhanced = enhanced_data['X_val']\n",
    "X_test_enhanced = enhanced_data['X_test']\n",
    "\n",
    "y_train = enhanced_data['y_train']\n",
    "y_val = enhanced_data['y_val']\n",
    "y_test = enhanced_data['y_test']\n",
    "\n",
    "if 'icu_types_train' in enhanced_data:\n",
    "    icu_types_train = enhanced_data['icu_types_train']\n",
    "    icu_types_val = enhanced_data['icu_types_val']\n",
    "    icu_types_test = enhanced_data['icu_types_test']\n",
    "else:\n",
    "    icu_types_train = np.load(client_data_dir / \"icu_types_train.npy\", allow_pickle=True)\n",
    "    icu_types_val = np.load(client_data_dir / \"icu_types_val.npy\", allow_pickle=True)\n",
    "    icu_types_test = np.load(client_data_dir / \"icu_types_test.npy\", allow_pickle=True)\n",
    "\n",
    "def to_numpy(obj):\n",
    "    return obj.values if hasattr(obj, \"values\") else obj\n",
    "\n",
    "X_train_np = to_numpy(X_train_enhanced)\n",
    "X_val_np = to_numpy(X_val_enhanced)\n",
    "X_test_np = to_numpy(X_test_enhanced)\n",
    "y_train_np = to_numpy(y_train)\n",
    "y_val_np = to_numpy(y_val)\n",
    "y_test_np = to_numpy(y_test)\n",
    "\n",
    "ICU_TYPES = sorted(np.unique(icu_types_train))\n",
    "\n",
    "client_data = {}\n",
    "\n",
    "for icu_type in ICU_TYPES:\n",
    "    train_mask = (icu_types_train == icu_type)\n",
    "    val_mask = (icu_types_val == icu_type)\n",
    "    test_mask = (icu_types_test == icu_type)\n",
    "\n",
    "    client_data[icu_type] = {\n",
    "        'X_train': X_train_np[train_mask],\n",
    "        'y_train': y_train_np[train_mask],\n",
    "        'X_val': X_val_np[val_mask],\n",
    "        'y_val': y_val_np[val_mask],\n",
    "        'X_test': X_test_np[test_mask],\n",
    "        'y_test': y_test_np[test_mask],\n",
    "    }\n",
    "\n",
    "    n_train = client_data[icu_type]['X_train'].shape[0]\n",
    "    n_val = client_data[icu_type]['X_val'].shape[0]\n",
    "    n_test = client_data[icu_type]['X_test'].shape[0]\n",
    "    mortality = client_data[icu_type]['y_train'].mean() * 100\n",
    "    n_features = client_data[icu_type]['X_train'].shape[1]\n",
    "\n",
    "client_data = {\n",
    "    cid: data for cid, data in client_data.items()\n",
    "    if cid != \"Mixed\"\n",
    "}\n",
    "\n",
    "total_samples = sum(len(d['y_train']) for d in client_data.values())\n",
    "\n",
    "icu_mortality_rates = {}\n",
    "icu_sample_sizes = {}\n",
    "\n",
    "for icu_type in client_data.keys():\n",
    "    mortality = client_data[icu_type]['y_train'].mean()\n",
    "    n_samples = len(client_data[icu_type]['y_train'])\n",
    "\n",
    "    icu_mortality_rates[icu_type] = mortality\n",
    "    icu_sample_sizes[icu_type] = n_samples\n",
    "\n",
    "mortality_mean = np.mean(list(icu_mortality_rates.values()))\n",
    "mortality_std = np.std(list(icu_mortality_rates.values()))\n",
    "mortality_cv = mortality_std / mortality_mean\n",
    "\n",
    "size_mean = np.mean(list(icu_sample_sizes.values()))\n",
    "size_std = np.std(list(icu_sample_sizes.values()))\n",
    "size_cv = size_std / size_mean\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "icu_names = list(icu_sample_sizes.keys())\n",
    "sample_counts = list(icu_sample_sizes.values())\n",
    "\n",
    "axes[0].bar(icu_names, sample_counts, color='steelblue', alpha=0.7)\n",
    "axes[0].set_xlabel('ICU Type', fontsize=12)\n",
    "axes[0].set_ylabel('Number of Training Samples', fontsize=12)\n",
    "axes[0].set_title('FL Client Data Distribution (Non-IID) - Mixed Removed', fontsize=14, fontweight='bold')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "mortality_rates = [v * 100 for v in icu_mortality_rates.values()]\n",
    "colors = ['#2ecc71' if m < 10 else '#e74c3c' if m > 12 else '#f39c12' for m in mortality_rates]\n",
    "\n",
    "axes[1].bar(icu_names, mortality_rates, color=colors, alpha=0.7)\n",
    "axes[1].axhline(y=mortality_mean*100, color='black', linestyle='--', linewidth=2, label=f'Mean: {mortality_mean*100:.1f}%')\n",
    "axes[1].set_xlabel('ICU Type', fontsize=12)\n",
    "axes[1].set_ylabel('Mortality Rate (%)', fontsize=12)\n",
    "axes[1].set_title('Mortality Rate by ICU Type (5 clients, Mixed removed)', fontsize=14, fontweight='bold')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "axes[1].legend()\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/kaggle/working/fl_icu_partitioning_heterogeneity.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xT6deJ9dXut_"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "save_path = Path(\"/kaggle/working/client_data_fl.pkl\")\n",
    "with open(save_path, 'wb') as f:\n",
    "    pickle.dump(client_data, f)\n",
    "print(f\"Saved to: {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dfhqPykoXut_"
   },
   "source": [
    "## 5.3 Data Heterogeneity Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yPRDHjm-Xut_"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "figures_dir = Path(\"/kaggle/working/figures\")\n",
    "figures_dir.mkdir(exist_ok=True)\n",
    "\n",
    "plt.savefig(figures_dir / 'fl_data_distribution.png', dpi=300, bbox_inches='tight')\n",
    "plt.savefig(figures_dir / 'fl_data_distribution.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Data heterogeneity visualization saved to: {figures_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AkxCys-8Xut_"
   },
   "source": [
    "## 5.4 Federated Learning Algorithms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x5vonOl7XuuA"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import copy\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import roc_auc_score, f1_score, precision_recall_curve\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def find_best_threshold_f1(y_true, y_probs):\n",
    "    precision, recall, thresholds = precision_recall_curve(y_true, y_probs)\n",
    "    f1_scores = 2 * (precision * recall) / (precision + recall + 1e-10)\n",
    "    if len(thresholds) > 0:\n",
    "        idx = np.argmax(f1_scores)\n",
    "        return thresholds[idx], f1_scores[idx]\n",
    "    return 0.5, 0.0\n",
    "\n",
    "class EnhancedMLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims=[451, 349, 132], dropout=0.45):\n",
    "        super(EnhancedMLP, self).__init__()\n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        for h_dim in hidden_dims:\n",
    "            layers.append(nn.Linear(prev_dim, h_dim))\n",
    "            layers.append(nn.BatchNorm1d(h_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "            prev_dim = h_dim\n",
    "        layers.append(nn.Linear(prev_dim, 1))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x).squeeze(-1)\n",
    "\n",
    "class FLTrainer:\n",
    "    def __init__(self, client_data, input_dim, device):\n",
    "        self.client_data = client_data\n",
    "        self.clients = list(client_data.keys())\n",
    "        self.device = device\n",
    "\n",
    "        self.global_model = EnhancedMLP(input_dim=input_dim).to(device)\n",
    "\n",
    "        self.X_val_global = np.concatenate([d['X_val'] for d in client_data.values()])\n",
    "        self.y_val_global = np.concatenate([d['y_val'] for d in client_data.values()])\n",
    "\n",
    "        self.client_weights = {}\n",
    "        for cid, data in client_data.items():\n",
    "            neg = (data['y_train'] == 0).sum()\n",
    "            pos = (data['y_train'] == 1).sum()\n",
    "            ratio = min(neg / (pos + 1e-6), 20.0)\n",
    "            self.client_weights[cid] = torch.tensor([ratio], dtype=torch.float32).to(device)\n",
    "\n",
    "        self.client_bn_states = {cid: None for cid in self.clients}\n",
    "\n",
    "    def _get_bn_keys(self, model):\n",
    "        bn_keys = []\n",
    "        for name, module in model.named_modules():\n",
    "            if isinstance(module, nn.BatchNorm1d):\n",
    "                bn_keys.append(f\"{name}.weight\")\n",
    "                bn_keys.append(f\"{name}.bias\")\n",
    "                bn_keys.append(f\"{name}.running_mean\")\n",
    "                bn_keys.append(f\"{name}.running_var\")\n",
    "                bn_keys.append(f\"{name}.num_batches_tracked\")\n",
    "        return set(bn_keys)\n",
    "\n",
    "    def train_algorithm(self, algo_name, rounds=20, local_epochs=3, lr=1e-4, mu=0.0, lam=0.0):\n",
    "        if algo_name == 'ditto':\n",
    "            self.personalized_models = {cid: copy.deepcopy(self.global_model) for cid in self.clients}\n",
    "\n",
    "        history = {'auroc': [], 'f1': []}\n",
    "\n",
    "        for r in range(rounds):\n",
    "            local_weights = []\n",
    "            local_sizes = []\n",
    "\n",
    "            global_state_dict = copy.deepcopy(self.global_model.state_dict())\n",
    "            bn_keys = set()\n",
    "            if algo_name == 'fedbn':\n",
    "                bn_keys = self._get_bn_keys(self.global_model)\n",
    "\n",
    "            for cid in self.clients:\n",
    "                data = self.client_data[cid]\n",
    "\n",
    "                temp_model = copy.deepcopy(self.global_model)\n",
    "\n",
    "                if algo_name == 'fedbn' and self.client_bn_states[cid] is not None:\n",
    "                    current_state = temp_model.state_dict()\n",
    "                    local_bn = self.client_bn_states[cid]\n",
    "                    for k in bn_keys:\n",
    "                        if k in local_bn:\n",
    "                            current_state[k] = local_bn[k]\n",
    "                    temp_model.load_state_dict(current_state)\n",
    "\n",
    "                temp_model.train()\n",
    "                optimizer = torch.optim.AdamW(temp_model.parameters(), lr=lr, weight_decay=1e-3)\n",
    "                criterion = nn.BCEWithLogitsLoss(pos_weight=self.client_weights[cid])\n",
    "\n",
    "                ds = TensorDataset(torch.tensor(data['X_train'], dtype=torch.float32),\n",
    "                                   torch.tensor(data['y_train'], dtype=torch.float32))\n",
    "                loader = DataLoader(ds, batch_size=64, shuffle=True)\n",
    "\n",
    "                for _ in range(local_epochs):\n",
    "                    for xb, yb in loader:\n",
    "                        xb, yb = xb.to(self.device), yb.to(self.device)\n",
    "                        optimizer.zero_grad()\n",
    "                        logits = temp_model(xb)\n",
    "                        loss = criterion(logits, yb)\n",
    "\n",
    "                        if algo_name == 'fedprox':\n",
    "                            prox = 0\n",
    "                            for w, w_t in zip(temp_model.parameters(), self.global_model.parameters()):\n",
    "                                prox += (w - w_t).norm(2)\n",
    "                            loss += (mu / 2) * prox\n",
    "\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                if algo_name == 'fedbn':\n",
    "                    st = temp_model.state_dict()\n",
    "                    self.client_bn_states[cid] = {k: st[k].clone() for k in bn_keys if k in st}\n",
    "\n",
    "                local_weights.append(copy.deepcopy(temp_model.state_dict()))\n",
    "                local_sizes.append(len(data['y_train']))\n",
    "\n",
    "                if algo_name == 'ditto':\n",
    "                    p_model = self.personalized_models[cid]\n",
    "                    p_model.train()\n",
    "                    p_opt = torch.optim.AdamW(p_model.parameters(), lr=lr, weight_decay=1e-3)\n",
    "\n",
    "                    for _ in range(local_epochs):\n",
    "                        for xb, yb in loader:\n",
    "                            xb, yb = xb.to(self.device), yb.to(self.device)\n",
    "                            p_opt.zero_grad()\n",
    "                            logits = p_model(xb)\n",
    "                            loss = criterion(logits, yb)\n",
    "\n",
    "                            ditto_term = 0\n",
    "                            for w_p, w_g in zip(p_model.parameters(), self.global_model.parameters()):\n",
    "                                ditto_term += (w_p - w_g).norm(2)\n",
    "\n",
    "                            loss += (lam / 2) * ditto_term\n",
    "                            loss.backward()\n",
    "                            p_opt.step()\n",
    "\n",
    "            new_state = copy.deepcopy(global_state_dict)\n",
    "            total = sum(local_sizes)\n",
    "            for key in new_state.keys():\n",
    "                if algo_name == 'fedbn' and key in bn_keys: continue\n",
    "\n",
    "                w_sum = torch.stack([local_weights[i][key].float() * local_sizes[i] for i in range(len(self.clients))], 0).sum(0)\n",
    "                new_state[key] = w_sum / total\n",
    "            self.global_model.load_state_dict(new_state)\n",
    "\n",
    "            val_auc, val_f1 = 0, 0\n",
    "\n",
    "            if algo_name in ['ditto', 'fedbn']:\n",
    "                aucs, f1s = [], []\n",
    "                for cid in self.clients:\n",
    "                    if algo_name == 'ditto':\n",
    "                        eval_m = self.personalized_models[cid]\n",
    "                    else:\n",
    "                        eval_m = copy.deepcopy(self.global_model)\n",
    "                        if self.client_bn_states[cid]:\n",
    "                            curr = eval_m.state_dict()\n",
    "                            for k, v in self.client_bn_states[cid].items(): curr[k] = v\n",
    "                            eval_m.load_state_dict(curr)\n",
    "\n",
    "                    a, f = self._evaluate_metrics(eval_m, self.client_data[cid]['X_val'], self.client_data[cid]['y_val'])\n",
    "                    weight = len(self.client_data[cid]['y_val']) / len(self.y_val_global)\n",
    "                    aucs.append(a * weight)\n",
    "                    f1s.append(f * weight)\n",
    "                val_auc = sum(aucs)\n",
    "                val_f1 = sum(f1s)\n",
    "            else:\n",
    "                val_auc, val_f1 = self._evaluate_metrics(self.global_model, self.X_val_global, self.y_val_global)\n",
    "\n",
    "            history['auroc'].append(val_auc)\n",
    "            history['f1'].append(val_f1)\n",
    "\n",
    "            if (r+1) % 5 == 0:\n",
    "                print(f\"Round {r+1:2d} | Val AUROC: {val_auc:.4f} | Val F1: {val_f1:.4f}\")\n",
    "\n",
    "        return history\n",
    "\n",
    "    def _evaluate_metrics(self, model, X, y):\n",
    "        model.eval()\n",
    "        ds = TensorDataset(torch.tensor(X, dtype=torch.float32))\n",
    "        ld = DataLoader(ds, batch_size=256, shuffle=False)\n",
    "        probs = []\n",
    "        with torch.no_grad():\n",
    "            for xb, in ld:\n",
    "                probs.append(torch.sigmoid(model(xb.to(self.device))).cpu())\n",
    "\n",
    "        y_probs = torch.cat(probs).numpy()\n",
    "\n",
    "        try:\n",
    "            auc = roc_auc_score(y, y_probs)\n",
    "        except:\n",
    "            auc = 0.5\n",
    "\n",
    "        thresh, f1 = find_best_threshold_f1(y, y_probs)\n",
    "\n",
    "        return auc, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YNY-PKOJXuuB"
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import warnings\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import average_precision_score, f1_score, precision_recall_curve, roc_auc_score\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "OPTUNA_CONFIG = {\n",
    "    \"hidden_dims\": [451, 349, 132],\n",
    "    \"dropout\": 0.446,\n",
    "    \"lr\": 9.6e-05,\n",
    "    \"weight_decay\": 7.5e-04,\n",
    "    \"batch_size\": 64,\n",
    "}\n",
    "\n",
    "class MortalityMLP(nn.Module):\n",
    "    def __init__(self, input_dim: int, hidden_dims: List[int], dropout: float, use_bn: bool = False):\n",
    "        super().__init__()\n",
    "        self.use_bn = use_bn\n",
    "        self.bns = nn.ModuleList()\n",
    "\n",
    "        if use_bn:\n",
    "            self.linears = nn.ModuleList()\n",
    "            prev_dim = input_dim\n",
    "            for hidden_dim in hidden_dims:\n",
    "                self.linears.append(nn.Linear(prev_dim, hidden_dim))\n",
    "                self.bns.append(nn.BatchNorm1d(hidden_dim))\n",
    "                prev_dim = hidden_dim\n",
    "            self.dropout = nn.Dropout(dropout)\n",
    "        else:\n",
    "            layers = []\n",
    "            prev_dim = input_dim\n",
    "            for hidden_dim in hidden_dims:\n",
    "                layers.extend(\n",
    "                    [\n",
    "                        nn.Linear(prev_dim, hidden_dim),\n",
    "                        nn.LayerNorm(hidden_dim),\n",
    "                        nn.ReLU(inplace=True),\n",
    "                        nn.Dropout(dropout),\n",
    "                    ]\n",
    "                )\n",
    "                prev_dim = hidden_dim\n",
    "            self.encoder = nn.Sequential(*layers)\n",
    "\n",
    "        self.classifier = nn.Linear(hidden_dims[-1], 1)\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        if self.use_bn:\n",
    "            for linear, bn in zip(self.linears, self.bns):\n",
    "                x = linear(x)\n",
    "                x = bn(x)\n",
    "                x = F.relu(x, inplace=True)\n",
    "                x = self.dropout(x)\n",
    "        else:\n",
    "            x = self.encoder(x)\n",
    "        return self.classifier(x)\n",
    "\n",
    "def find_optimal_threshold(y_true: np.ndarray, y_probs: np.ndarray) -> Tuple[float, float]:\n",
    "    precision, recall, thresholds = precision_recall_curve(y_true, y_probs)\n",
    "    f1_scores = np.where((precision + recall) > 0, 2 * precision * recall / (precision + recall), 0)\n",
    "    if len(thresholds) > 0:\n",
    "        idx = np.argmax(f1_scores[:-1])\n",
    "        return thresholds[idx], f1_scores[idx]\n",
    "    return 0.5, 0.0\n",
    "\n",
    "class BaseFLTrainer:\n",
    "    def __init__(self, client_data: Dict[str, Dict], use_bn: bool = False):\n",
    "        self.device = DEVICE\n",
    "        self.client_data = client_data\n",
    "        self.client_ids = list(client_data.keys())\n",
    "\n",
    "        self.class_weights = {}\n",
    "        for cid, data in client_data.items():\n",
    "            n_pos = np.sum(data[\"y_train\"] == 1)\n",
    "            n_neg = np.sum(data[\"y_train\"] == 0)\n",
    "            self.class_weights[cid] = n_neg / (n_pos + 1e-8)\n",
    "\n",
    "        input_dim = client_data[self.client_ids[0]][\"X_train\"].shape[1]\n",
    "        self.global_model = MortalityMLP(\n",
    "            input_dim,\n",
    "            OPTUNA_CONFIG[\"hidden_dims\"],\n",
    "            OPTUNA_CONFIG[\"dropout\"],\n",
    "            use_bn,\n",
    "        ).to(DEVICE)\n",
    "\n",
    "        self.X_val = np.vstack([d[\"X_val\"] for d in client_data.values()])\n",
    "        self.y_val = np.concatenate([d[\"y_val\"] for d in client_data.values()])\n",
    "        self.X_test = np.vstack([d[\"X_test\"] for d in client_data.values()])\n",
    "        self.y_test = np.concatenate([d[\"y_test\"] for d in client_data.values()])\n",
    "\n",
    "        self.optimal_threshold = 0.5\n",
    "        self.best_threshold = 0.5\n",
    "        self.best_model_state = copy.deepcopy(self.global_model.state_dict())\n",
    "        self.best_val_f1 = 0.0\n",
    "\n",
    "        self.history = {\"val_f1\": [], \"val_auroc\": [], \"val_auprc\": []}\n",
    "\n",
    "    def _get_probs(self, model: nn.Module, X: np.ndarray) -> np.ndarray:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            X_t = torch.FloatTensor(X).to(self.device)\n",
    "            return torch.sigmoid(model(X_t)).cpu().numpy().flatten()\n",
    "\n",
    "    def _evaluate(self, model: nn.Module, X: np.ndarray, y: np.ndarray, threshold: float = None) -> Dict[str, float]:\n",
    "        probs = self._get_probs(model, X)\n",
    "        threshold = self.optimal_threshold if threshold is None else threshold\n",
    "        preds = (probs >= threshold).astype(int)\n",
    "        return {\n",
    "            \"auroc\": roc_auc_score(y, probs),\n",
    "            \"auprc\": average_precision_score(y, probs),\n",
    "            \"f1\": f1_score(y, preds),\n",
    "        }\n",
    "\n",
    "    def _update_threshold(self, model: nn.Module) -> Tuple[float, float, float]:\n",
    "        probs = self._get_probs(model, self.X_val)\n",
    "        self.optimal_threshold, val_f1 = find_optimal_threshold(self.y_val, probs)\n",
    "        val_auroc = roc_auc_score(self.y_val, probs)\n",
    "        val_auprc = average_precision_score(self.y_val, probs)\n",
    "        return val_f1, val_auroc, val_auprc\n",
    "\n",
    "    def _finalize_and_eval(self, eval_test: bool = True):\n",
    "        self.global_model.load_state_dict(self.best_model_state)\n",
    "        self.optimal_threshold = self.best_threshold\n",
    "        if not eval_test:\n",
    "            return None\n",
    "        return self._evaluate(self.global_model, self.X_test, self.y_test)\n",
    "\n",
    "class FedAvgTrainer(BaseFLTrainer):\n",
    "    def __init__(self, client_data: Dict[str, Dict]):\n",
    "        super().__init__(client_data, use_bn=False)\n",
    "        self.name = \"FedAvg\"\n",
    "\n",
    "    def _local_train(self, client_id: str, local_epochs: int):\n",
    "        data = self.client_data[client_id]\n",
    "        local_model = copy.deepcopy(self.global_model)\n",
    "        local_model.train()\n",
    "\n",
    "        pos_weight = torch.tensor([self.class_weights[client_id]], device=self.device)\n",
    "        criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\n",
    "        X_t = torch.FloatTensor(data[\"X_train\"]).to(self.device)\n",
    "        y_t = torch.FloatTensor(data[\"y_train\"]).to(self.device)\n",
    "        loader = DataLoader(TensorDataset(X_t, y_t), batch_size=OPTUNA_CONFIG[\"batch_size\"], shuffle=True)\n",
    "\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            local_model.parameters(),\n",
    "            lr=OPTUNA_CONFIG[\"lr\"],\n",
    "            weight_decay=OPTUNA_CONFIG[\"weight_decay\"],\n",
    "        )\n",
    "\n",
    "        for _ in range(local_epochs):\n",
    "            for batch_X, batch_y in loader:\n",
    "                optimizer.zero_grad()\n",
    "                loss = criterion(local_model(batch_X), batch_y.unsqueeze(1))\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(local_model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "\n",
    "        return local_model.state_dict(), len(data[\"y_train\"])\n",
    "\n",
    "    def _aggregate(self, states, counts):\n",
    "        total = sum(counts)\n",
    "        weights = [c / total for c in counts]\n",
    "        global_state = self.global_model.state_dict()\n",
    "        for key in global_state:\n",
    "            global_state[key] = sum(weights[i] * states[i][key].float() for i in range(len(states)))\n",
    "        self.global_model.load_state_dict(global_state)\n",
    "\n",
    "    def train(self, num_rounds: int = 30, local_epochs: int = 5, patience: int = 7, eval_test: bool = True):\n",
    "        no_improve = 0\n",
    "        for r in range(num_rounds):\n",
    "            states, counts = [], []\n",
    "            for cid in self.client_ids:\n",
    "                s, n = self._local_train(cid, local_epochs)\n",
    "                states.append(s)\n",
    "                counts.append(n)\n",
    "            self._aggregate(states, counts)\n",
    "\n",
    "            val_f1, val_auroc, val_auprc = self._update_threshold(self.global_model)\n",
    "            self.history[\"val_f1\"].append(val_f1)\n",
    "            self.history[\"val_auroc\"].append(val_auroc)\n",
    "            self.history[\"val_auprc\"].append(val_auprc)\n",
    "\n",
    "            if (r + 1) % 5 == 0:\n",
    "                print(f\"  Round {r+1}: Val F1={val_f1:.4f}, AUROC={val_auroc:.4f}, AUPRC={val_auprc:.4f}\")\n",
    "\n",
    "            if val_f1 > self.best_val_f1:\n",
    "                self.best_val_f1 = val_f1\n",
    "                self.best_model_state = copy.deepcopy(self.global_model.state_dict())\n",
    "                self.best_threshold = self.optimal_threshold\n",
    "                no_improve = 0\n",
    "            else:\n",
    "                no_improve += 1\n",
    "                if no_improve >= patience:\n",
    "                    print(f\"  Early stopping at round {r+1}\")\n",
    "                    break\n",
    "\n",
    "        test_metrics = self._finalize_and_eval(eval_test)\n",
    "        return {\"val_f1\": self.best_val_f1, \"test\": test_metrics}\n",
    "\n",
    "class FedProxTrainer(BaseFLTrainer):\n",
    "    def __init__(self, client_data: Dict[str, Dict], mu: float = 0.01):\n",
    "        super().__init__(client_data, use_bn=False)\n",
    "        self.mu = mu\n",
    "        self.name = f\"FedProx(mu={mu})\"\n",
    "\n",
    "    def _local_train(self, client_id: str, local_epochs: int):\n",
    "        data = self.client_data[client_id]\n",
    "        local_model = copy.deepcopy(self.global_model)\n",
    "        local_model.train()\n",
    "\n",
    "        global_weights = {n: p.clone().detach() for n, p in self.global_model.named_parameters()}\n",
    "\n",
    "        pos_weight = torch.tensor([self.class_weights[client_id]], device=self.device)\n",
    "        criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\n",
    "        X_t = torch.FloatTensor(data[\"X_train\"]).to(self.device)\n",
    "        y_t = torch.FloatTensor(data[\"y_train\"]).to(self.device)\n",
    "        loader = DataLoader(TensorDataset(X_t, y_t), batch_size=OPTUNA_CONFIG[\"batch_size\"], shuffle=True)\n",
    "\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            local_model.parameters(),\n",
    "            lr=OPTUNA_CONFIG[\"lr\"],\n",
    "            weight_decay=OPTUNA_CONFIG[\"weight_decay\"],\n",
    "        )\n",
    "\n",
    "        for _ in range(local_epochs):\n",
    "            for batch_X, batch_y in loader:\n",
    "                optimizer.zero_grad()\n",
    "                loss = criterion(local_model(batch_X), batch_y.unsqueeze(1))\n",
    "                prox = sum(torch.norm(p - global_weights[n]) ** 2 for n, p in local_model.named_parameters())\n",
    "                loss = loss + (self.mu / 2) * prox\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(local_model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "\n",
    "        return local_model.state_dict(), len(data[\"y_train\"])\n",
    "\n",
    "    def _aggregate(self, states, counts):\n",
    "        total = sum(counts)\n",
    "        weights = [c / total for c in counts]\n",
    "        global_state = self.global_model.state_dict()\n",
    "        for key in global_state:\n",
    "            global_state[key] = sum(weights[i] * states[i][key].float() for i in range(len(states)))\n",
    "        self.global_model.load_state_dict(global_state)\n",
    "\n",
    "    def train(self, num_rounds: int = 30, local_epochs: int = 5, patience: int = 7, eval_test: bool = True):\n",
    "        no_improve = 0\n",
    "        for r in range(num_rounds):\n",
    "            states, counts = [], []\n",
    "            for cid in self.client_ids:\n",
    "                s, n = self._local_train(cid, local_epochs)\n",
    "                states.append(s)\n",
    "                counts.append(n)\n",
    "            self._aggregate(states, counts)\n",
    "\n",
    "            val_f1, val_auroc, val_auprc = self._update_threshold(self.global_model)\n",
    "            self.history[\"val_f1\"].append(val_f1)\n",
    "            self.history[\"val_auroc\"].append(val_auroc)\n",
    "            self.history[\"val_auprc\"].append(val_auprc)\n",
    "\n",
    "            if (r + 1) % 5 == 0:\n",
    "                print(f\"  Round {r+1}: Val F1={val_f1:.4f}, AUROC={val_auroc:.4f}, AUPRC={val_auprc:.4f}\")\n",
    "\n",
    "            if val_f1 > self.best_val_f1:\n",
    "                self.best_val_f1 = val_f1\n",
    "                self.best_model_state = copy.deepcopy(self.global_model.state_dict())\n",
    "                self.best_threshold = self.optimal_threshold\n",
    "                no_improve = 0\n",
    "            else:\n",
    "                no_improve += 1\n",
    "                if no_improve >= patience:\n",
    "                    print(f\"  Early stopping at round {r+1}\")\n",
    "                    break\n",
    "\n",
    "        test_metrics = self._finalize_and_eval(eval_test)\n",
    "        return {\"val_f1\": self.best_val_f1, \"test\": test_metrics}\n",
    "\n",
    "class FedBNTrainer(BaseFLTrainer):\n",
    "    def __init__(self, client_data: Dict[str, Dict]):\n",
    "        super().__init__(client_data, use_bn=True)\n",
    "        self.name = \"FedBN\"\n",
    "        self.client_bn_states = {cid: None for cid in self.client_ids}\n",
    "\n",
    "    def _local_train(self, client_id: str, local_epochs: int):\n",
    "        data = self.client_data[client_id]\n",
    "        local_model = copy.deepcopy(self.global_model)\n",
    "\n",
    "        if self.client_bn_states[client_id] is not None:\n",
    "            for i, bn in enumerate(local_model.bns):\n",
    "                bn.load_state_dict(self.client_bn_states[client_id][i])\n",
    "\n",
    "        local_model.train()\n",
    "\n",
    "        pos_weight = torch.tensor([self.class_weights[client_id]], device=self.device)\n",
    "        criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\n",
    "        X_t = torch.FloatTensor(data[\"X_train\"]).to(self.device)\n",
    "        y_t = torch.FloatTensor(data[\"y_train\"]).to(self.device)\n",
    "        loader = DataLoader(TensorDataset(X_t, y_t), batch_size=OPTUNA_CONFIG[\"batch_size\"], shuffle=True)\n",
    "\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            local_model.parameters(),\n",
    "            lr=OPTUNA_CONFIG[\"lr\"],\n",
    "            weight_decay=OPTUNA_CONFIG[\"weight_decay\"],\n",
    "        )\n",
    "\n",
    "        for _ in range(local_epochs):\n",
    "            for batch_X, batch_y in loader:\n",
    "                optimizer.zero_grad()\n",
    "                loss = criterion(local_model(batch_X), batch_y.unsqueeze(1))\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(local_model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "\n",
    "        self.client_bn_states[client_id] = [bn.state_dict() for bn in local_model.bns]\n",
    "        return local_model.state_dict(), len(data[\"y_train\"])\n",
    "\n",
    "    def _aggregate(self, states, counts):\n",
    "        total = sum(counts)\n",
    "        weights = [c / total for c in counts]\n",
    "        global_state = self.global_model.state_dict()\n",
    "\n",
    "        bn_keys = set()\n",
    "        for i in range(len(self.global_model.bns)):\n",
    "            for k in self.global_model.bns[i].state_dict().keys():\n",
    "                bn_keys.add(f\"bns.{i}.{k}\")\n",
    "\n",
    "        for key in global_state:\n",
    "            if key not in bn_keys:\n",
    "                global_state[key] = sum(weights[i] * states[i][key].float() for i in range(len(states)))\n",
    "\n",
    "        self.global_model.load_state_dict(global_state)\n",
    "\n",
    "    def train(self, num_rounds: int = 30, local_epochs: int = 5, patience: int = 7, eval_test: bool = True):\n",
    "        no_improve = 0\n",
    "        for r in range(num_rounds):\n",
    "            states, counts = [], []\n",
    "            for cid in self.client_ids:\n",
    "                s, n = self._local_train(cid, local_epochs)\n",
    "                states.append(s)\n",
    "                counts.append(n)\n",
    "            self._aggregate(states, counts)\n",
    "\n",
    "            val_f1, val_auroc, val_auprc = self._update_threshold(self.global_model)\n",
    "            self.history[\"val_f1\"].append(val_f1)\n",
    "            self.history[\"val_auroc\"].append(val_auroc)\n",
    "            self.history[\"val_auprc\"].append(val_auprc)\n",
    "\n",
    "            if (r + 1) % 5 == 0:\n",
    "                print(f\"  Round {r+1}: Val F1={val_f1:.4f}, AUROC={val_auroc:.4f}, AUPRC={val_auprc:.4f}\")\n",
    "\n",
    "            if val_f1 > self.best_val_f1:\n",
    "                self.best_val_f1 = val_f1\n",
    "                self.best_model_state = copy.deepcopy(self.global_model.state_dict())\n",
    "                self.best_threshold = self.optimal_threshold\n",
    "                no_improve = 0\n",
    "            else:\n",
    "                no_improve += 1\n",
    "                if no_improve >= patience:\n",
    "                    print(f\"  Early stopping at round {r+1}\")\n",
    "                    break\n",
    "\n",
    "        test_metrics = self._finalize_and_eval(eval_test)\n",
    "        return {\"val_f1\": self.best_val_f1, \"test\": test_metrics}\n",
    "\n",
    "class DittoTrainer(BaseFLTrainer):\n",
    "    def __init__(self, client_data: Dict[str, Dict], lambda_reg: float = 0.1):\n",
    "        super().__init__(client_data, use_bn=False)\n",
    "        self.lambda_reg = lambda_reg\n",
    "        self.name = f\"Ditto(lambda={lambda_reg})\"\n",
    "        self.personalized_models = {cid: copy.deepcopy(self.global_model) for cid in self.client_ids}\n",
    "        self.best_personal_states = None\n",
    "\n",
    "    def _local_train_global(self, client_id: str, local_epochs: int):\n",
    "        data = self.client_data[client_id]\n",
    "        local_model = copy.deepcopy(self.global_model)\n",
    "        local_model.train()\n",
    "\n",
    "        pos_weight = torch.tensor([self.class_weights[client_id]], device=self.device)\n",
    "        criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\n",
    "        X_t = torch.FloatTensor(data[\"X_train\"]).to(self.device)\n",
    "        y_t = torch.FloatTensor(data[\"y_train\"]).to(self.device)\n",
    "        loader = DataLoader(TensorDataset(X_t, y_t), batch_size=OPTUNA_CONFIG[\"batch_size\"], shuffle=True)\n",
    "\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            local_model.parameters(),\n",
    "            lr=OPTUNA_CONFIG[\"lr\"],\n",
    "            weight_decay=OPTUNA_CONFIG[\"weight_decay\"],\n",
    "        )\n",
    "\n",
    "        for _ in range(local_epochs):\n",
    "            for batch_X, batch_y in loader:\n",
    "                optimizer.zero_grad()\n",
    "                loss = criterion(local_model(batch_X), batch_y.unsqueeze(1))\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(local_model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "\n",
    "        return local_model.state_dict(), len(data[\"y_train\"])\n",
    "\n",
    "    def _local_train_personalized(self, client_id: str, local_epochs: int):\n",
    "        data = self.client_data[client_id]\n",
    "        pers_model = self.personalized_models[client_id]\n",
    "        pers_model.train()\n",
    "\n",
    "        global_weights = {n: p.clone().detach() for n, p in self.global_model.named_parameters()}\n",
    "\n",
    "        pos_weight = torch.tensor([self.class_weights[client_id]], device=self.device)\n",
    "        criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\n",
    "        X_t = torch.FloatTensor(data[\"X_train\"]).to(self.device)\n",
    "        y_t = torch.FloatTensor(data[\"y_train\"]).to(self.device)\n",
    "        loader = DataLoader(TensorDataset(X_t, y_t), batch_size=OPTUNA_CONFIG[\"batch_size\"], shuffle=True)\n",
    "\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            pers_model.parameters(),\n",
    "            lr=OPTUNA_CONFIG[\"lr\"],\n",
    "            weight_decay=OPTUNA_CONFIG[\"weight_decay\"],\n",
    "        )\n",
    "\n",
    "        for _ in range(local_epochs):\n",
    "            for batch_X, batch_y in loader:\n",
    "                optimizer.zero_grad()\n",
    "                loss = criterion(pers_model(batch_X), batch_y.unsqueeze(1))\n",
    "                reg = sum(torch.norm(p - global_weights[n]) ** 2 for n, p in pers_model.named_parameters())\n",
    "                loss = loss + (self.lambda_reg / 2) * reg\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(pers_model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "\n",
    "    def _aggregate(self, states, counts):\n",
    "        total = sum(counts)\n",
    "        weights = [c / total for c in counts]\n",
    "        global_state = self.global_model.state_dict()\n",
    "        for key in global_state:\n",
    "            global_state[key] = sum(weights[i] * states[i][key].float() for i in range(len(states)))\n",
    "        self.global_model.load_state_dict(global_state)\n",
    "\n",
    "    def _evaluate_personalized(self, dataset: str = \"test\"):\n",
    "        all_probs, all_labels = [], []\n",
    "        for cid in self.client_ids:\n",
    "            data = self.client_data[cid]\n",
    "            X = data[\"X_test\"] if dataset == \"test\" else data[\"X_val\"]\n",
    "            y = data[\"y_test\"] if dataset == \"test\" else data[\"y_val\"]\n",
    "            probs = self._get_probs(self.personalized_models[cid], X)\n",
    "            all_probs.append(probs)\n",
    "            all_labels.append(y)\n",
    "        return np.concatenate(all_probs), np.concatenate(all_labels)\n",
    "\n",
    "    def train(self, num_rounds: int = 30, local_epochs: int = 5, patience: int = 7, eval_test: bool = True):\n",
    "        no_improve = 0\n",
    "        for r in range(num_rounds):\n",
    "            states, counts = [], []\n",
    "            for cid in self.client_ids:\n",
    "                s, n = self._local_train_global(cid, local_epochs)\n",
    "                states.append(s)\n",
    "                counts.append(n)\n",
    "            self._aggregate(states, counts)\n",
    "\n",
    "            for cid in self.client_ids:\n",
    "                self._local_train_personalized(cid, local_epochs)\n",
    "\n",
    "            val_probs, val_labels = self._evaluate_personalized(\"val\")\n",
    "            self.optimal_threshold, val_f1 = find_optimal_threshold(val_labels, val_probs)\n",
    "            val_auroc = roc_auc_score(val_labels, val_probs)\n",
    "            val_auprc = average_precision_score(val_labels, val_probs)\n",
    "\n",
    "            self.history[\"val_f1\"].append(val_f1)\n",
    "            self.history[\"val_auroc\"].append(val_auroc)\n",
    "            self.history[\"val_auprc\"].append(val_auprc)\n",
    "\n",
    "            if (r + 1) % 5 == 0:\n",
    "                print(f\"  Round {r+1}: Val F1={val_f1:.4f}, AUROC={val_auroc:.4f}, AUPRC={val_auprc:.4f}\")\n",
    "\n",
    "            if val_f1 > self.best_val_f1:\n",
    "                self.best_val_f1 = val_f1\n",
    "                self.best_threshold = self.optimal_threshold\n",
    "                self.best_model_state = copy.deepcopy(self.global_model.state_dict())\n",
    "                self.best_personal_states = {cid: copy.deepcopy(model.state_dict()) for cid, model in self.personalized_models.items()}\n",
    "                no_improve = 0\n",
    "            else:\n",
    "                no_improve += 1\n",
    "                if no_improve >= patience:\n",
    "                    print(f\"  Early stopping at round {r+1}\")\n",
    "                    break\n",
    "\n",
    "        if self.best_personal_states:\n",
    "            for cid in self.client_ids:\n",
    "                self.personalized_models[cid].load_state_dict(self.best_personal_states[cid])\n",
    "        self.global_model.load_state_dict(self.best_model_state)\n",
    "        self.optimal_threshold = self.best_threshold\n",
    "\n",
    "        if not eval_test:\n",
    "            return {\"val_f1\": self.best_val_f1, \"test\": None}\n",
    "\n",
    "        test_probs, test_labels = self._evaluate_personalized(\"test\")\n",
    "        test_preds = (test_probs >= self.optimal_threshold).astype(int)\n",
    "        m = {\n",
    "            \"auroc\": roc_auc_score(test_labels, test_probs),\n",
    "            \"auprc\": average_precision_score(test_labels, test_probs),\n",
    "            \"f1\": f1_score(test_labels, test_preds),\n",
    "        }\n",
    "        return {\"val_f1\": self.best_val_f1, \"test\": m}\n",
    "\n",
    "    def evaluate_per_client(self):\n",
    "        print(\"\\n  Per-client results:\")\n",
    "        for cid in self.client_ids:\n",
    "            data = self.client_data[cid]\n",
    "            probs = self._get_probs(self.personalized_models[cid], data[\"X_test\"])\n",
    "            preds = (probs >= self.optimal_threshold).astype(int)\n",
    "            m = {\n",
    "                \"auroc\": roc_auc_score(data[\"y_test\"], probs),\n",
    "                \"auprc\": average_precision_score(data[\"y_test\"], probs),\n",
    "                \"f1\": f1_score(data[\"y_test\"], preds),\n",
    "            }\n",
    "            print(f\"    {cid}: AUROC={m['auroc']:.4f}, AUPRC={m['auprc']:.4f}, F1={m['f1']:.4f}\")\n",
    "\n",
    "class SCAFFOLDTrainer(BaseFLTrainer):\n",
    "    def __init__(self, client_data: Dict[str, Dict]):\n",
    "        super().__init__(client_data, use_bn=False)\n",
    "        self.name = \"SCAFFOLD\"\n",
    "        self.c_global = {n: torch.zeros_like(p) for n, p in self.global_model.named_parameters()}\n",
    "        self.c_local = {\n",
    "            cid: {n: torch.zeros_like(p) for n, p in self.global_model.named_parameters()} for cid in self.client_ids\n",
    "        }\n",
    "\n",
    "    def _local_train(self, client_id: str, local_epochs: int):\n",
    "        data = self.client_data[client_id]\n",
    "        local_model = copy.deepcopy(self.global_model)\n",
    "        local_model.train()\n",
    "\n",
    "        pos_weight = torch.tensor([self.class_weights[client_id]], device=self.device)\n",
    "        criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\n",
    "        X_t = torch.FloatTensor(data[\"X_train\"]).to(self.device)\n",
    "        y_t = torch.FloatTensor(data[\"y_train\"]).to(self.device)\n",
    "        loader = DataLoader(TensorDataset(X_t, y_t), batch_size=OPTUNA_CONFIG[\"batch_size\"], shuffle=True)\n",
    "\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            local_model.parameters(),\n",
    "            lr=OPTUNA_CONFIG[\"lr\"],\n",
    "            weight_decay=OPTUNA_CONFIG[\"weight_decay\"],\n",
    "        )\n",
    "\n",
    "        c_global = self.c_global\n",
    "        old_c_local = {n: p.clone().detach() for n, p in self.c_local[client_id].items()}\n",
    "\n",
    "        steps = 0\n",
    "        for _ in range(local_epochs):\n",
    "            for batch_X, batch_y in loader:\n",
    "                optimizer.zero_grad()\n",
    "                loss = criterion(local_model(batch_X), batch_y.unsqueeze(1))\n",
    "                loss.backward()\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    for n, p in local_model.named_parameters():\n",
    "                        if p.grad is not None:\n",
    "                            p.grad.data += c_global[n].to(self.device) - old_c_local[n].to(self.device)\n",
    "\n",
    "                torch.nn.utils.clip_grad_norm_(local_model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "                steps += 1\n",
    "\n",
    "        new_c_local = {}\n",
    "        with torch.no_grad():\n",
    "            for n, p in local_model.named_parameters():\n",
    "                global_p = dict(self.global_model.named_parameters())[n]\n",
    "                new_c_local[n] = old_c_local[n] - c_global[n] + (global_p - p) / (steps * OPTUNA_CONFIG[\"lr\"])\n",
    "\n",
    "        delta_c = {n: new_c_local[n] - old_c_local[n] for n in new_c_local}\n",
    "        self.c_local[client_id] = new_c_local\n",
    "\n",
    "        return local_model.state_dict(), len(data[\"y_train\"]), delta_c\n",
    "\n",
    "    def _aggregate(self, states, counts, delta_cs):\n",
    "        total = sum(counts)\n",
    "        weights = [c / total for c in counts]\n",
    "\n",
    "        global_state = self.global_model.state_dict()\n",
    "        for key in global_state:\n",
    "            global_state[key] = sum(weights[i] * states[i][key].float() for i in range(len(states)))\n",
    "        self.global_model.load_state_dict(global_state)\n",
    "\n",
    "        for n in self.c_global:\n",
    "            self.c_global[n] = self.c_global[n] + sum(weights[i] * delta_cs[i][n] for i in range(len(delta_cs)))\n",
    "\n",
    "    def train(self, num_rounds: int = 30, local_epochs: int = 5, patience: int = 7, eval_test: bool = True):\n",
    "        no_improve = 0\n",
    "        for r in range(num_rounds):\n",
    "            states, counts, delta_cs = [], [], []\n",
    "            for cid in self.client_ids:\n",
    "                s, n, dc = self._local_train(cid, local_epochs)\n",
    "                states.append(s)\n",
    "                counts.append(n)\n",
    "                delta_cs.append(dc)\n",
    "            self._aggregate(states, counts, delta_cs)\n",
    "\n",
    "            val_f1, val_auroc, val_auprc = self._update_threshold(self.global_model)\n",
    "            self.history[\"val_f1\"].append(val_f1)\n",
    "            self.history[\"val_auroc\"].append(val_auroc)\n",
    "            self.history[\"val_auprc\"].append(val_auprc)\n",
    "\n",
    "            if (r + 1) % 5 == 0:\n",
    "                print(f\"  Round {r+1}: Val F1={val_f1:.4f}, AUROC={val_auroc:.4f}, AUPRC={val_auprc:.4f}\")\n",
    "\n",
    "            if val_f1 > self.best_val_f1:\n",
    "                self.best_val_f1 = val_f1\n",
    "                self.best_model_state = copy.deepcopy(self.global_model.state_dict())\n",
    "                self.best_threshold = self.optimal_threshold\n",
    "                no_improve = 0\n",
    "            else:\n",
    "                no_improve += 1\n",
    "                if no_improve >= patience:\n",
    "                    print(f\"  Early stopping at round {r+1}\")\n",
    "                    break\n",
    "\n",
    "        test_metrics = self._finalize_and_eval(eval_test)\n",
    "        return {\"val_f1\": self.best_val_f1, \"test\": test_metrics}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        _ = client_data\n",
    "    except NameError as exc:\n",
    "        raise SystemExit(\"client_data must be defined before running this script.\") from exc\n",
    "\n",
    "    results = {}\n",
    "    trainers = {}\n",
    "\n",
    "    config = {\n",
    "        \"num_rounds\": 30,\n",
    "        \"local_epochs\": 5,\n",
    "        \"patience\": 7,\n",
    "    }\n",
    "\n",
    "    trainers[\"FedAvg\"] = FedAvgTrainer(client_data)\n",
    "    results[\"FedAvg\"] = trainers[\"FedAvg\"].train(**config, eval_test=True)\n",
    "\n",
    "    trainers[\"FedProx\"] = FedProxTrainer(client_data, mu=0.01)\n",
    "    results[\"FedProx\"] = trainers[\"FedProx\"].train(**config, eval_test=True)\n",
    "\n",
    "    trainers[\"FedBN\"] = FedBNTrainer(client_data)\n",
    "    results[\"FedBN\"] = trainers[\"FedBN\"].train(**config, eval_test=True)\n",
    "\n",
    "    trainers[\"SCAFFOLD\"] = SCAFFOLDTrainer(client_data)\n",
    "    results[\"SCAFFOLD\"] = trainers[\"SCAFFOLD\"].train(**config, eval_test=True)\n",
    "\n",
    "    trainers[\"Ditto\"] = DittoTrainer(client_data, lambda_reg=0.1)\n",
    "    results[\"Ditto\"] = trainers[\"Ditto\"].train(**config, eval_test=True)\n",
    "\n",
    "    print(f\"{'Method':<20} {'Val_F1':>10} {'Test_AUROC':>12} {'Test_AUPRC':>12} {'Test_F1':>10}\")\n",
    "    for method, m in results.items():\n",
    "        test = m[\"test\"] or {\"auroc\": float(\"nan\"), \"auprc\": float(\"nan\"), \"f1\": float(\"nan\")}\n",
    "        print(f\"{method:<20} {m['val_f1']:>10.4f} {test['auroc']:>12.4f} {test['auprc']:>12.4f} {test['f1']:>10.4f}\")\n",
    "\n",
    "    best_method = max(results.keys(), key=lambda x: results[x][\"val_f1\"])\n",
    "    print(f\"\\nBest method by validation F1: {best_method}\")\n",
    "\n",
    "    if best_method == \"Ditto\":\n",
    "        lambda_values = [0.01, 0.05, 0.1, 0.2, 0.5]\n",
    "        val_scores = {}\n",
    "        for lam in lambda_values:\n",
    "            trainer = DittoTrainer(client_data, lambda_reg=lam)\n",
    "            res = trainer.train(num_rounds=30, local_epochs=5, patience=7, eval_test=False)\n",
    "            val_scores[lam] = res[\"val_f1\"]\n",
    "        best_lambda = max(val_scores.keys(), key=lambda x: val_scores[x])\n",
    "        print(f\"Best lambda (by val F1): {best_lambda}\")\n",
    "\n",
    "        final_trainer = DittoTrainer(client_data, lambda_reg=best_lambda)\n",
    "        final_metrics = final_trainer.train(num_rounds=40, local_epochs=5, patience=10, eval_test=True)\n",
    "        final_trainer.evaluate_per_client()\n",
    "    elif best_method == \"FedProx\":\n",
    "        mu_values = [0.001, 0.01, 0.05, 0.1]\n",
    "        val_scores = {}\n",
    "        for mu in mu_values:\n",
    "            trainer = FedProxTrainer(client_data, mu=mu)\n",
    "            res = trainer.train(num_rounds=30, local_epochs=5, patience=7, eval_test=False)\n",
    "            val_scores[mu] = res[\"val_f1\"]\n",
    "        best_mu = max(val_scores.keys(), key=lambda x: val_scores[x])\n",
    "        print(f\"Best mu (by val F1): {best_mu}\")\n",
    "\n",
    "        final_trainer = FedProxTrainer(client_data, mu=best_mu)\n",
    "        final_metrics = final_trainer.train(num_rounds=40, local_epochs=5, patience=10, eval_test=True)\n",
    "    else:\n",
    "        if best_method == \"FedAvg\":\n",
    "            final_trainer = FedAvgTrainer(client_data)\n",
    "        elif best_method == \"FedBN\":\n",
    "            final_trainer = FedBNTrainer(client_data)\n",
    "        elif best_method == \"SCAFFOLD\":\n",
    "            final_trainer = SCAFFOLDTrainer(client_data)\n",
    "        final_metrics = final_trainer.train(num_rounds=40, local_epochs=5, patience=10, eval_test=True)\n",
    "\n",
    "    print(f\"\\nFinal test metrics for selected method ({best_method}): {final_metrics['test']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "99pHO-LYXuuE"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "FIGURES_DIR = Path(\"/kaggle/working/figures\")\n",
    "RESULTS_DIR = Path(\"/kaggle/working/results\")\n",
    "\n",
    "fl_results_data = {\n",
    "    'FedAvg': {'Val_F1': 0.4722, 'Test_AUROC': 0.8508, 'Test_AUPRC': 0.4452, 'Test_F1': 0.4463},\n",
    "    'FedProx': {'Val_F1': 0.4711, 'Test_AUROC': 0.8538, 'Test_AUPRC': 0.4470, 'Test_F1': 0.4495},\n",
    "    'FedBN': {'Val_F1': 0.4451, 'Test_AUROC': 0.8231, 'Test_AUPRC': 0.3995, 'Test_F1': 0.4222},\n",
    "    'SCAFFOLD': {'Val_F1': 0.3969, 'Test_AUROC': 0.7882, 'Test_AUPRC': 0.3071, 'Test_F1': 0.3762},\n",
    "    'Ditto': {'Val_F1': 0.4879, 'Test_AUROC': 0.8655, 'Test_AUPRC': 0.4619, 'Test_F1': 0.4725},\n",
    "}\n",
    "\n",
    "per_client_results = {\n",
    "    'CCU': {'AUROC': 0.8651, 'AUPRC': 0.5198, 'F1': 0.5047},\n",
    "    'CVICU': {'AUROC': 0.8870, 'AUPRC': 0.2918, 'F1': 0.3248},\n",
    "    'MICU': {'AUROC': 0.8283, 'AUPRC': 0.4769, 'F1': 0.4957},\n",
    "    'Neuro': {'AUROC': 0.8565, 'AUPRC': 0.0892, 'F1': 0.1463},\n",
    "    'SICU': {'AUROC': 0.8473, 'AUPRC': 0.4772, 'F1': 0.4587},\n",
    "}\n",
    "\n",
    "centralized_auroc = 0.8718\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "ax1 = axes[0, 0]\n",
    "methods = list(fl_results_data.keys())\n",
    "aurocs = [fl_results_data[m]['Test_AUROC'] for m in methods]\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd']\n",
    "\n",
    "bars = ax1.bar(methods, aurocs, color=colors, edgecolor='black', alpha=0.8)\n",
    "ax1.axhline(y=centralized_auroc, color='red', linestyle='--', linewidth=2, label=f'Centralized ({centralized_auroc:.4f})')\n",
    "ax1.set_ylabel('AUROC', fontsize=11)\n",
    "ax1.set_title('FL Methods: Test AUROC Comparison', fontsize=12)\n",
    "ax1.set_ylim([0.75, 0.90])\n",
    "ax1.legend(loc='lower right')\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "for bar, val in zip(bars, aurocs):\n",
    "    ax1.annotate(f'{val:.4f}', xy=(bar.get_x() + bar.get_width()/2, bar.get_height()),\n",
    "                ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "\n",
    "ax2 = axes[0, 1]\n",
    "x = np.arange(len(methods))\n",
    "width = 0.25\n",
    "\n",
    "aurocs = [fl_results_data[m]['Test_AUROC'] for m in methods]\n",
    "auprcs = [fl_results_data[m]['Test_AUPRC'] for m in methods]\n",
    "f1s = [fl_results_data[m]['Test_F1'] for m in methods]\n",
    "\n",
    "bars1 = ax2.bar(x - width, aurocs, width, label='AUROC', color='steelblue', edgecolor='black')\n",
    "bars2 = ax2.bar(x, auprcs, width, label='AUPRC', color='coral', edgecolor='black')\n",
    "bars3 = ax2.bar(x + width, f1s, width, label='F1', color='mediumseagreen', edgecolor='black')\n",
    "\n",
    "ax2.set_ylabel('Score', fontsize=11)\n",
    "ax2.set_title('FL Methods: All Test Metrics', fontsize=12)\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels(methods, rotation=45, ha='right')\n",
    "ax2.legend(loc='lower right')\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "ax2.set_ylim([0, 1.0])\n",
    "\n",
    "ax3 = axes[1, 0]\n",
    "clients = list(per_client_results.keys())\n",
    "client_aurocs = [per_client_results[c]['AUROC'] for c in clients]\n",
    "client_colors = ['#e41a1c', '#377eb8', '#4daf4a', '#984ea3', '#ff7f00']\n",
    "\n",
    "bars = ax3.bar(clients, client_aurocs, color=client_colors, edgecolor='black', alpha=0.8)\n",
    "ax3.axhline(y=0.8655, color='black', linestyle='--', linewidth=2, label='Ditto Global (0.8655)')\n",
    "ax3.set_ylabel('AUROC', fontsize=11)\n",
    "ax3.set_title('Ditto: Per-Client Test AUROC', fontsize=12)\n",
    "ax3.set_ylim([0.75, 0.95])\n",
    "ax3.legend(loc='lower right')\n",
    "ax3.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "for bar, val in zip(bars, client_aurocs):\n",
    "    ax3.annotate(f'{val:.3f}', xy=(bar.get_x() + bar.get_width()/2, bar.get_height()),\n",
    "                ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "ax4 = axes[1, 1]\n",
    "client_auprcs = [per_client_results[c]['AUPRC'] for c in clients]\n",
    "\n",
    "bars = ax4.bar(clients, client_auprcs, color=client_colors, edgecolor='black', alpha=0.8)\n",
    "ax4.set_ylabel('AUPRC', fontsize=11)\n",
    "ax4.set_title('Ditto: Per-Client Test AUPRC (Shows Data Heterogeneity)', fontsize=12)\n",
    "ax4.set_ylim([0, 0.6])\n",
    "ax4.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "for bar, val in zip(bars, client_auprcs):\n",
    "    ax4.annotate(f'{val:.3f}', xy=(bar.get_x() + bar.get_width()/2, bar.get_height()),\n",
    "                ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / \"fl_training_results.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "fl_df = pd.DataFrame([\n",
    "    {'Method': m, **fl_results_data[m]} for m in methods\n",
    "]).sort_values('Test_AUROC', ascending=False)\n",
    "\n",
    "print(fl_df.to_string(index=False))\n",
    "\n",
    "print(f\"{'Method':<12} {'AUROC':>10} {'Gap vs Centralized':>20}\")\n",
    "for m in methods:\n",
    "    gap = centralized_auroc - fl_results_data[m]['Test_AUROC']\n",
    "    gap_pct = (gap / centralized_auroc) * 100\n",
    "    print(f\"{m:<12} {fl_results_data[m]['Test_AUROC']:>10.4f} {gap:>10.4f} ({gap_pct:>5.2f}%)\")\n",
    "\n",
    "client_df = pd.DataFrame([\n",
    "    {'Client': c, **per_client_results[c]} for c in clients\n",
    "]).sort_values('AUROC', ascending=False)\n",
    "\n",
    "print(client_df.to_string(index=False))\n",
    "\n",
    "fl_df.to_csv(RESULTS_DIR / \"fl_training_results.csv\", index=False)\n",
    "client_df.to_csv(RESULTS_DIR / \"fl_per_client_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-nBilhAMXuuD"
   },
   "outputs": [],
   "source": [
    "from sklearn.calibration import calibration_curve\n",
    "from sklearn.metrics import brier_score_loss\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "RESULTS_DIR = Path(\"/kaggle/working/results\")\n",
    "FIGURES_DIR = Path(\"/kaggle/working/figures\")\n",
    "\n",
    "def compute_ece(y_true, y_prob, n_bins=10):\n",
    "    y_true = np.asarray(y_true).ravel()\n",
    "    y_prob = np.asarray(y_prob).ravel()\n",
    "    bins = np.linspace(0, 1, n_bins + 1)\n",
    "    ece = 0.0\n",
    "    for i in range(n_bins):\n",
    "        mask = (y_prob >= bins[i]) & (y_prob < bins[i + 1])\n",
    "        if i == n_bins - 1:\n",
    "            mask = (y_prob >= bins[i]) & (y_prob <= bins[i + 1])\n",
    "        if mask.sum() > 0:\n",
    "            bin_acc = y_true[mask].mean()\n",
    "            bin_conf = y_prob[mask].mean()\n",
    "            ece += mask.sum() * np.abs(bin_acc - bin_conf)\n",
    "    return ece / len(y_true)\n",
    "\n",
    "def compute_mce(y_true, y_prob, n_bins=10):\n",
    "    y_true = np.asarray(y_true).ravel()\n",
    "    y_prob = np.asarray(y_prob).ravel()\n",
    "    bins = np.linspace(0, 1, n_bins + 1)\n",
    "    mce = 0.0\n",
    "    for i in range(n_bins):\n",
    "        mask = (y_prob >= bins[i]) & (y_prob < bins[i + 1])\n",
    "        if i == n_bins - 1:\n",
    "            mask = (y_prob >= bins[i]) & (y_prob <= bins[i + 1])\n",
    "        if mask.sum() > 0:\n",
    "            bin_acc = y_true[mask].mean()\n",
    "            bin_conf = y_prob[mask].mean()\n",
    "            mce = max(mce, np.abs(bin_acc - bin_conf))\n",
    "    return mce\n",
    "\n",
    "class TemperatureScaling:\n",
    "    def __init__(self):\n",
    "        self.temperature = 1.0\n",
    "    \n",
    "    def fit(self, y_true, y_prob):\n",
    "        from scipy.optimize import minimize_scalar\n",
    "        y_true = np.asarray(y_true).ravel()\n",
    "        y_prob = np.asarray(y_prob).ravel()\n",
    "        y_prob = np.clip(y_prob, 1e-10, 1 - 1e-10)\n",
    "        logits = np.log(y_prob / (1 - y_prob))\n",
    "        \n",
    "        def nll(T):\n",
    "            scaled = 1 / (1 + np.exp(-logits / T))\n",
    "            scaled = np.clip(scaled, 1e-10, 1 - 1e-10)\n",
    "            return -np.mean(y_true * np.log(scaled) + (1 - y_true) * np.log(1 - scaled))\n",
    "        \n",
    "        result = minimize_scalar(nll, bounds=(0.1, 10.0), method='bounded')\n",
    "        self.temperature = result.x\n",
    "        return self\n",
    "    \n",
    "    def transform(self, y_prob):\n",
    "        y_prob = np.asarray(y_prob).ravel()\n",
    "        y_prob = np.clip(y_prob, 1e-10, 1 - 1e-10)\n",
    "        logits = np.log(y_prob / (1 - y_prob))\n",
    "        return 1 / (1 + np.exp(-logits / self.temperature))\n",
    "\n",
    "fl_probs_test = {}\n",
    "fl_probs_val = {}\n",
    "\n",
    "X_val = trainers['FedAvg'].X_val\n",
    "y_val = trainers['FedAvg'].y_val\n",
    "X_test = trainers['FedAvg'].X_test\n",
    "y_test = trainers['FedAvg'].y_test\n",
    "\n",
    "for method_name, trainer in trainers.items():\n",
    "    if method_name == 'Ditto':\n",
    "        test_probs_list = []\n",
    "        val_probs_list = []\n",
    "        for cid in trainer.client_ids:\n",
    "            data = trainer.client_data[cid]\n",
    "            test_probs_list.append(trainer._get_probs(trainer.personalized_models[cid], data['X_test']))\n",
    "            val_probs_list.append(trainer._get_probs(trainer.personalized_models[cid], data['X_val']))\n",
    "        fl_probs_test[method_name] = np.concatenate(test_probs_list)\n",
    "        fl_probs_val[method_name] = np.concatenate(val_probs_list)\n",
    "    else:\n",
    "        fl_probs_test[method_name] = trainer._get_probs(trainer.global_model, X_test)\n",
    "        fl_probs_val[method_name] = trainer._get_probs(trainer.global_model, X_val)\n",
    "\n",
    "fl_calibration_results = {}\n",
    "\n",
    "print(f\"{'Method':<12} {'ECE_Pre':>10} {'ECE_Post':>10} {'MCE_Pre':>10} {'MCE_Post':>10} {'Brier_Pre':>10} {'Brier_Post':>10} {'Temp':>8}\")\n",
    "\n",
    "for method_name in fl_probs_test.keys():\n",
    "    y_prob_test = fl_probs_test[method_name]\n",
    "    y_prob_val = fl_probs_val[method_name]\n",
    "    \n",
    "    ece_pre = compute_ece(y_test, y_prob_test)\n",
    "    mce_pre = compute_mce(y_test, y_prob_test)\n",
    "    brier_pre = brier_score_loss(y_test, y_prob_test)\n",
    "    \n",
    "    ts = TemperatureScaling()\n",
    "    ts.fit(y_val, y_prob_val)\n",
    "    y_prob_cal = ts.transform(y_prob_test)\n",
    "    \n",
    "    ece_post = compute_ece(y_test, y_prob_cal)\n",
    "    mce_post = compute_mce(y_test, y_prob_cal)\n",
    "    brier_post = brier_score_loss(y_test, y_prob_cal)\n",
    "    \n",
    "    fl_calibration_results[method_name] = {\n",
    "        'probs_uncal': y_prob_test,\n",
    "        'probs_cal': y_prob_cal,\n",
    "        'ece_pre': ece_pre,\n",
    "        'ece_post': ece_post,\n",
    "        'mce_pre': mce_pre,\n",
    "        'mce_post': mce_post,\n",
    "        'brier_pre': brier_pre,\n",
    "        'brier_post': brier_post,\n",
    "        'temperature': ts.temperature\n",
    "    }\n",
    "    \n",
    "    print(f\"{method_name:<12} {ece_pre:>10.4f} {ece_post:>10.4f} {mce_pre:>10.4f} {mce_post:>10.4f} {brier_pre:>10.4f} {brier_post:>10.4f} {ts.temperature:>8.3f}\")\n",
    "\n",
    "fl_cal_df = pd.DataFrame([\n",
    "    {\n",
    "        'Method': name,\n",
    "        'ECE_Pre': r['ece_pre'],\n",
    "        'ECE_Post': r['ece_post'],\n",
    "        'MCE_Pre': r['mce_pre'],\n",
    "        'MCE_Post': r['mce_post'],\n",
    "        'Brier_Pre': r['brier_pre'],\n",
    "        'Brier_Post': r['brier_post'],\n",
    "        'Temperature': r['temperature'],\n",
    "    }\n",
    "    for name, r in fl_calibration_results.items()\n",
    "]).sort_values('Brier_Post')\n",
    "\n",
    "print(fl_cal_df.to_string(index=False))\n",
    "fl_cal_df.to_csv(RESULTS_DIR / \"fl_calibration_results.csv\", index=False)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "ax1 = axes[0]\n",
    "ax1.plot([0, 1], [0, 1], \"k:\", label=\"Perfect\", linewidth=2)\n",
    "\n",
    "colors = {'FedAvg': '#1f77b4', 'FedProx': '#ff7f0e', 'FedBN': '#2ca02c', 'SCAFFOLD': '#d62728', 'Ditto': '#9467bd'}\n",
    "\n",
    "for method_name, res in fl_calibration_results.items():\n",
    "    color = colors.get(method_name, 'gray')\n",
    "    prob_true, prob_pred = calibration_curve(y_test, res['probs_uncal'], n_bins=10, strategy='uniform')\n",
    "    ax1.plot(prob_pred, prob_true, linestyle=\"--\", color=color, alpha=0.4)\n",
    "    prob_true_cal, prob_pred_cal = calibration_curve(y_test, res['probs_cal'], n_bins=10, strategy='uniform')\n",
    "    ax1.plot(prob_pred_cal, prob_true_cal, marker='o', linestyle=\"-\", color=color, linewidth=2,\n",
    "             label=f\"{method_name} (ECE={res['ece_post']:.3f})\")\n",
    "\n",
    "ax1.set_xlabel(\"Mean Predicted Probability\", fontsize=11)\n",
    "ax1.set_ylabel(\"Fraction of Positives\", fontsize=11)\n",
    "ax1.set_title(\"FL Methods: Calibration Curves (After Temp Scaling)\", fontsize=12)\n",
    "ax1.legend(loc=\"lower right\", fontsize=9)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_xlim([0, 1])\n",
    "ax1.set_ylim([0, 1])\n",
    "\n",
    "ax2 = axes[1]\n",
    "methods = list(fl_calibration_results.keys())\n",
    "ece_pre = [fl_calibration_results[m]['ece_pre'] for m in methods]\n",
    "ece_post = [fl_calibration_results[m]['ece_post'] for m in methods]\n",
    "\n",
    "x = np.arange(len(methods))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax2.bar(x - width/2, ece_pre, width, label='Before', color='salmon', edgecolor='black')\n",
    "bars2 = ax2.bar(x + width/2, ece_post, width, label='After', color='steelblue', edgecolor='black')\n",
    "\n",
    "ax2.set_xlabel(\"FL Method\", fontsize=11)\n",
    "ax2.set_ylabel(\"ECE\", fontsize=11)\n",
    "ax2.set_title(\"ECE Before vs After Temperature Scaling\", fontsize=12)\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels(methods, rotation=45, ha='right')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "for bar in bars1:\n",
    "    ax2.annotate(f'{bar.get_height():.3f}', xy=(bar.get_x() + bar.get_width()/2, bar.get_height()),\n",
    "                ha='center', va='bottom', fontsize=8)\n",
    "for bar in bars2:\n",
    "    ax2.annotate(f'{bar.get_height():.3f}', xy=(bar.get_x() + bar.get_width()/2, bar.get_height()),\n",
    "                ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / \"fl_calibration_analysis.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"{'Setting':<30} {'ECE':>10} {'Brier':>10}\")\n",
    "\n",
    "if 'calibrated_results' in dir() and 'StackedEnsemble' in calibrated_results:\n",
    "    print(f\"{'StackedEnsemble (Centralized)':<30} {calibrated_results['StackedEnsemble']['ece_post']:>10.4f} {calibrated_results['StackedEnsemble']['brier_post']:>10.4f}\")\n",
    "\n",
    "for method in ['Ditto', 'FedAvg', 'FedProx']:\n",
    "    if method in fl_calibration_results:\n",
    "        print(f\"{method + ' (FL)':<30} {fl_calibration_results[method]['ece_post']:>10.4f} {fl_calibration_results[method]['brier_post']:>10.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.calibration import calibration_curve\n",
    "from pathlib import Path\n",
    "\n",
    "FIGURES_DIR = Path(\"/kaggle/working/figures\")\n",
    "\n",
    "y_test_fl = trainers['FedAvg'].y_test\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "ax1 = axes[0]\n",
    "ax1.plot([0, 1], [0, 1], \"k:\", label=\"Perfect\", linewidth=2)\n",
    "\n",
    "fl_colors = {'Ditto': '#9467bd', 'FedAvg': '#1f77b4', 'FedBN': '#2ca02c'}\n",
    "for method, color in fl_colors.items():\n",
    "    if method in fl_calibration_results:\n",
    "        probs = fl_calibration_results[method]['probs_cal']\n",
    "        prob_true, prob_pred = calibration_curve(y_test_fl, probs, n_bins=10, strategy='uniform')\n",
    "        ece = fl_calibration_results[method]['ece_post']\n",
    "        ax1.plot(prob_pred, prob_true, '-o', color=color, linewidth=2, label=f'{method} (ECE={ece:.3f})')\n",
    "\n",
    "ax1.plot([0.05, 0.95], [0.05, 0.95], 'g--', linewidth=2, alpha=0.7, label='Centralized (~Perfect)')\n",
    "\n",
    "ax1.set_xlabel(\"Mean Predicted Probability\", fontsize=11)\n",
    "ax1.set_ylabel(\"Fraction of Positives\", fontsize=11)\n",
    "ax1.set_title(\"FL Methods: Calibration Curves\", fontsize=12)\n",
    "ax1.legend(loc=\"lower right\", fontsize=9)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_xlim([0, 1])\n",
    "ax1.set_ylim([0, 1])\n",
    "\n",
    "ax2 = axes[1]\n",
    "methods = ['Centralized', 'FedBN', 'Ditto', 'FedAvg', 'FedProx', 'SCAFFOLD']\n",
    "ece_values = [\n",
    "    0.0050,\n",
    "    fl_calibration_results['FedBN']['ece_post'],\n",
    "    fl_calibration_results['Ditto']['ece_post'],\n",
    "    fl_calibration_results['FedAvg']['ece_post'],\n",
    "    fl_calibration_results['FedProx']['ece_post'],\n",
    "    fl_calibration_results['SCAFFOLD']['ece_post'],\n",
    "]\n",
    "colors = ['green', '#2ca02c', '#9467bd', '#1f77b4', '#ff7f0e', '#d62728']\n",
    "\n",
    "bars = ax2.bar(methods, ece_values, color=colors, edgecolor='black', alpha=0.8)\n",
    "ax2.set_ylabel('ECE (After Calibration)', fontsize=11)\n",
    "ax2.set_title('Calibration Quality: Centralized vs FL', fontsize=12)\n",
    "ax2.set_xticklabels(methods, rotation=45, ha='right')\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "for bar, val in zip(bars, ece_values):\n",
    "    ax2.annotate(f'{val:.4f}', xy=(bar.get_x() + bar.get_width()/2, bar.get_height()),\n",
    "                ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "ax3 = axes[2]\n",
    "\n",
    "ax3.scatter([0.8718], [0.005], s=200, c='green', marker='*', label='Centralized', zorder=5, edgecolors='black')\n",
    "\n",
    "fl_aurocs = {\n",
    "    'FedAvg': 0.8508, 'FedProx': 0.8538, 'FedBN': 0.8231, \n",
    "    'SCAFFOLD': 0.7882, 'Ditto': 0.8655\n",
    "}\n",
    "fl_eces = {m: fl_calibration_results[m]['ece_post'] for m in fl_aurocs.keys()}\n",
    "\n",
    "for method in fl_aurocs.keys():\n",
    "    color = {'FedAvg': '#1f77b4', 'FedProx': '#ff7f0e', 'FedBN': '#2ca02c', \n",
    "             'SCAFFOLD': '#d62728', 'Ditto': '#9467bd'}[method]\n",
    "    ax3.scatter([fl_aurocs[method]], [fl_eces[method]], s=150, c=color, \n",
    "                label=method, edgecolors='black', zorder=4)\n",
    "\n",
    "ax3.set_xlabel('AUROC', fontsize=11)\n",
    "ax3.set_ylabel('ECE (Lower is Better)', fontsize=11)\n",
    "ax3.set_title('Discrimination vs Calibration Tradeoff', fontsize=12)\n",
    "ax3.legend(loc='upper left', fontsize=9)\n",
    "ax3.grid(True, alpha=0.3)\n",
    "ax3.set_xlim([0.75, 0.90])\n",
    "ax3.set_ylim([0, 0.35])\n",
    "\n",
    "ax3.annotate('', xy=(0.88, 0.02), xytext=(0.80, 0.25),\n",
    "            arrowprops=dict(arrowstyle='->', color='gray', lw=2))\n",
    "ax3.text(0.82, 0.12, 'Better', fontsize=10, color='gray', rotation=-55)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / \"centralized_vs_fl_calibration.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"{'Method':<20} {'AUROC':>10} {'ECE':>10} {'Brier':>10}\")\n",
    "print(f\"{'Centralized':<20} {0.8718:>10.4f} {0.0050:>10.4f} {0.0724:>10.4f}\")\n",
    "for method in ['Ditto', 'FedAvg', 'FedProx', 'FedBN', 'SCAFFOLD']:\n",
    "    auroc = fl_aurocs[method]\n",
    "    ece = fl_calibration_results[method]['ece_post']\n",
    "    brier = fl_calibration_results[method]['brier_post']\n",
    "    print(f\"{method:<20} {auroc:>10.4f} {ece:>10.4f} {brier:>10.4f}\")\n",
    "\n",
    "print(f\"AUROC Gap (Ditto vs Centralized): {0.8718 - 0.8655:.4f} ({(0.8718 - 0.8655)/0.8718*100:.1f}%)\")\n",
    "print(f\"ECE Gap (Ditto vs Centralized):    {0.2251 - 0.0050:.4f} ({0.2251/0.0050:.0f}x worse)\")\n",
    "print(\"FL preserves discrimination but severely degrades calibration.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IsZjGhlrXuuF"
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q opacus \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import warnings\n",
    "from typing import Dict, Any, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from opacus import PrivacyEngine\n",
    "from opacus.validators import ModuleValidator\n",
    "from opacus.accountants.utils import get_noise_multiplier\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score,\n",
    "    average_precision_score,\n",
    "    brier_score_loss,\n",
    "    precision_recall_curve,\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.getLogger(\"opacus\").setLevel(logging.ERROR)\n",
    "\n",
    "\n",
    "class Config:\n",
    "    # Algorithms to benchmark\n",
    "    ALGORITHMS = [\"FedBN\", \"FedProx\", \"Ditto\", \"FedAvg\"]\n",
    "    EPSILONS = [2.0, 4.0, 6.0, 8.0]\n",
    "    SEEDS = [0, 1, 2]\n",
    "\n",
    "    # Training hyperparameters\n",
    "    BATCH_SIZE = 512\n",
    "    LEARNING_RATE = 0.002\n",
    "    N_ROUNDS = 40\n",
    "    LOCAL_EPOCHS = 4\n",
    "    FINE_TUNE_EPOCHS = 5\n",
    "\n",
    "    # Algorithm hyperparameters\n",
    "    FEDPROX_MU = 0.1\n",
    "    DITTO_LAMBDA = 0.8\n",
    "\n",
    "    # Privacy settings\n",
    "    TARGET_DELTA = 1e-5\n",
    "    MAX_GRAD_NORM = 1.2\n",
    "\n",
    "    # Model architecture\n",
    "    HIDDEN_DIMS = (256, 128, 64)\n",
    "    DROPOUT = 0.2\n",
    "\n",
    "    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    OUTPUT_FILE = \"thesis_final_metrics_085_fixed.csv\"\n",
    "\n",
    "\n",
    "conf = Config()\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# UTILITIES\n",
    "# ============================================================\n",
    "\n",
    "def set_seed(seed: int) -> None:\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "def compute_ece(probs: np.ndarray, labels: np.ndarray, n_bins: int = 10) -> float:\n",
    "    bins = np.linspace(0.0, 1.0, n_bins + 1)\n",
    "    ece = 0.0\n",
    "    for i in range(n_bins):\n",
    "        mask = (probs >= bins[i]) & (probs < bins[i + 1])\n",
    "        if np.any(mask):\n",
    "            bin_acc = labels[mask].mean()\n",
    "            bin_conf = probs[mask].mean()\n",
    "            ece += np.abs(bin_acc - bin_conf) * mask.mean()\n",
    "    return float(ece)\n",
    "\n",
    "\n",
    "def fit_temperature(val_logits: np.ndarray, val_labels: np.ndarray) -> float:\n",
    "    # Simple 1D search for temperature using NLL\n",
    "    val_logits = np.clip(val_logits, -20.0, 20.0)\n",
    "    val_labels = val_labels.astype(np.float32)\n",
    "\n",
    "    def nll(temp: float) -> float:\n",
    "        scaled = val_logits / temp\n",
    "        probs = 1.0 / (1.0 + np.exp(-scaled))\n",
    "        probs = np.clip(probs, 1e-7, 1.0 - 1e-7)\n",
    "        loss = -np.mean(\n",
    "            val_labels * np.log(probs) + (1.0 - val_labels) * np.log(1.0 - probs)\n",
    "        )\n",
    "        return float(loss)\n",
    "\n",
    "    best_t = 1.0\n",
    "    best_loss = nll(best_t)\n",
    "    for t in np.linspace(0.5, 3.0, 26):\n",
    "        loss = nll(t)\n",
    "        if loss < best_loss:\n",
    "            best_loss = loss\n",
    "            best_t = t\n",
    "    return float(best_t)\n",
    "\n",
    "\n",
    "def apply_temperature(logits: np.ndarray, temperature: float) -> np.ndarray:\n",
    "    logits = np.clip(logits / temperature, -20.0, 20.0)\n",
    "    probs = 1.0 / (1.0 + np.exp(-logits))\n",
    "    return probs\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# MODEL\n",
    "# ============================================================\n",
    "\n",
    "class MortalityMLP(nn.Module):\n",
    "    def __init__(self, input_dim: int) -> None:\n",
    "        super().__init__()\n",
    "        layers: List[nn.Module] = []\n",
    "        in_dim = input_dim\n",
    "        for h_dim in conf.HIDDEN_DIMS:\n",
    "            layers.append(nn.Linear(in_dim, h_dim))\n",
    "            layers.append(nn.GroupNorm(1, h_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(conf.DROPOUT))\n",
    "            in_dim = h_dim\n",
    "        layers.append(nn.Linear(in_dim, 1))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# FEDERATED MANAGER\n",
    "# ============================================================\n",
    "\n",
    "class FederatedManager:\n",
    "    def __init__(self, client_data: Dict[str, Dict[str, np.ndarray]]) -> None:\n",
    "        self.client_ids = list(client_data.keys())\n",
    "        self.input_dim = client_data[self.client_ids[0]][\"X_train\"].shape[1]\n",
    "        self.raw_data = client_data\n",
    "        self.results: List[Dict[str, Any]] = []\n",
    "\n",
    "    def _make_loader(\n",
    "        self,\n",
    "        X: np.ndarray,\n",
    "        y: np.ndarray,\n",
    "        shuffle: bool = True,\n",
    "    ) -> DataLoader:\n",
    "        ds = TensorDataset(\n",
    "            torch.from_numpy(X.astype(np.float32)),\n",
    "            torch.from_numpy(y.astype(np.float32)),\n",
    "        )\n",
    "        loader = DataLoader(\n",
    "            ds,\n",
    "            batch_size=conf.BATCH_SIZE,\n",
    "            shuffle=shuffle,\n",
    "            drop_last=False,\n",
    "        )\n",
    "        return loader\n",
    "\n",
    "    def _init_client(\n",
    "        self,\n",
    "        data: Dict[str, np.ndarray],\n",
    "        global_state: Dict[str, torch.Tensor],\n",
    "        noise_mult: float,\n",
    "    ) -> Dict[str, Any]:\n",
    "        train_loader = self._make_loader(data[\"X_train\"], data[\"y_train\"], shuffle=True)\n",
    "\n",
    "        model = MortalityMLP(self.input_dim)\n",
    "        model = ModuleValidator.fix(model)\n",
    "        model.to(conf.DEVICE)\n",
    "        model.load_state_dict(global_state, strict=True)\n",
    "\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=conf.LEARNING_RATE)\n",
    "\n",
    "        privacy_engine = PrivacyEngine()\n",
    "        model, optimizer, train_loader = privacy_engine.make_private(\n",
    "            module=model,\n",
    "            optimizer=optimizer,\n",
    "            data_loader=train_loader,\n",
    "            noise_multiplier=noise_mult,\n",
    "            max_grad_norm=conf.MAX_GRAD_NORM,\n",
    "        )\n",
    "\n",
    "        val_X = torch.from_numpy(data[\"X_val\"].astype(np.float32)).to(conf.DEVICE)\n",
    "        val_y = data[\"y_val\"].astype(np.float32)\n",
    "\n",
    "        test_X = torch.from_numpy(data[\"X_test\"].astype(np.float32)).to(conf.DEVICE)\n",
    "        test_y = data[\"y_test\"].astype(np.float32)\n",
    "\n",
    "        return {\n",
    "            \"model\": model,\n",
    "            \"opt\": optimizer,\n",
    "            \"loader\": train_loader,\n",
    "            \"privacy_engine\": privacy_engine,\n",
    "            \"val_X\": val_X,\n",
    "            \"val_y\": val_y,\n",
    "            \"test_X\": test_X,\n",
    "            \"test_y\": test_y,\n",
    "        }\n",
    "\n",
    "    def _get_clean_state(self, model: nn.Module) -> Dict[str, torch.Tensor]:\n",
    "        if hasattr(model, \"_module\"):\n",
    "            return {k: v.detach().clone() for k, v in model._module.state_dict().items()}\n",
    "        return {k: v.detach().clone() for k, v in model.state_dict().items()}\n",
    "\n",
    "    def _load_safe(\n",
    "        self,\n",
    "        model: nn.Module,\n",
    "        state: Dict[str, torch.Tensor],\n",
    "        strict: bool = True,\n",
    "    ) -> None:\n",
    "        if hasattr(model, \"_module\"):\n",
    "            model._module.load_state_dict(state, strict=strict)\n",
    "        else:\n",
    "            model.load_state_dict(state, strict=strict)\n",
    "\n",
    "    def _build_param_ref(\n",
    "        self,\n",
    "        global_state: Dict[str, torch.Tensor],\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        ref_model = MortalityMLP(self.input_dim)\n",
    "        ref_model = ModuleValidator.fix(ref_model)\n",
    "        ref_model.to(conf.DEVICE)\n",
    "        ref_model.load_state_dict(global_state, strict=True)\n",
    "\n",
    "        ref: Dict[str, torch.Tensor] = {}\n",
    "        for name, param in ref_model.named_parameters():\n",
    "            ref[name] = param.detach().clone()\n",
    "        return ref\n",
    "\n",
    "    def _train_round(\n",
    "        self,\n",
    "        client: Dict[str, Any],\n",
    "        global_state: Dict[str, torch.Tensor],\n",
    "        algo: str,\n",
    "    ) -> None:\n",
    "        model = client[\"model\"]\n",
    "        model.train()\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "        optimizer = client[\"opt\"]\n",
    "\n",
    "        global_ref = None\n",
    "        if algo == \"FedProx\":\n",
    "            global_ref = self._build_param_ref(global_state)\n",
    "\n",
    "        for _ in range(conf.LOCAL_EPOCHS):\n",
    "            for X, y in client[\"loader\"]:\n",
    "                X = X.to(conf.DEVICE)\n",
    "                y = y.to(conf.DEVICE)\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                logits = model(X).squeeze()\n",
    "                loss = criterion(logits, y)\n",
    "\n",
    "                if algo == \"FedProx\":\n",
    "                    prox = 0.0\n",
    "                    base_model = model._module if hasattr(model, \"_module\") else model\n",
    "                    for name, param in base_model.named_parameters():\n",
    "                        if name in global_ref:\n",
    "                            prox = prox + (param - global_ref[name]).pow(2).sum()\n",
    "                    loss = loss + (conf.FEDPROX_MU / 2.0) * prox\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "    def _aggregate(\n",
    "        self,\n",
    "        updates: List[Tuple[Dict[str, torch.Tensor], int]],\n",
    "        algo: str,\n",
    "        global_state: Dict[str, torch.Tensor],\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        total_weight = float(sum(n for _, n in updates))\n",
    "        keys = list(updates[0][0].keys())\n",
    "        new_state: Dict[str, torch.Tensor] = {}\n",
    "\n",
    "        for k in keys:\n",
    "            if algo == \"FedBN\" and (\"norm\" in k or \"bn\" in k):\n",
    "                new_state[k] = global_state[k]\n",
    "                continue\n",
    "            agg = None\n",
    "            for state, n in updates:\n",
    "                w = n / total_weight\n",
    "                tensor = state[k].float() * w\n",
    "                if agg is None:\n",
    "                    agg = tensor\n",
    "                else:\n",
    "                    agg = agg + tensor\n",
    "            new_state[k] = agg\n",
    "\n",
    "        return new_state\n",
    "\n",
    "    def _evaluate(\n",
    "        self,\n",
    "        clients: Dict[str, Dict[str, Any]],\n",
    "    ) -> Dict[str, float]:\n",
    "        all_val_logits: List[np.ndarray] = []\n",
    "        all_val_labels: List[np.ndarray] = []\n",
    "        all_test_logits: List[np.ndarray] = []\n",
    "        all_test_labels: List[np.ndarray] = []\n",
    "        eps_spent: List[float] = []\n",
    "\n",
    "        for client in clients.values():\n",
    "            model = client[\"model\"]\n",
    "            model.eval()\n",
    "            eps_spent.append(client[\"privacy_engine\"].get_epsilon(conf.TARGET_DELTA))\n",
    "\n",
    "            with torch.no_grad():\n",
    "                val_logits = model(client[\"val_X\"]).squeeze().cpu().numpy()\n",
    "                test_logits = model(client[\"test_X\"]).squeeze().cpu().numpy()\n",
    "\n",
    "            all_val_logits.append(val_logits)\n",
    "            all_val_labels.append(client[\"val_y\"])\n",
    "            all_test_logits.append(test_logits)\n",
    "            all_test_labels.append(client[\"test_y\"])\n",
    "\n",
    "        val_logits = np.concatenate(all_val_logits)\n",
    "        val_labels = np.concatenate(all_val_labels)\n",
    "        test_logits = np.concatenate(all_test_logits)\n",
    "        test_labels = np.concatenate(all_test_labels)\n",
    "\n",
    "        test_logits = np.clip(test_logits, -20.0, 20.0)\n",
    "        test_probs = 1.0 / (1.0 + np.exp(-test_logits))\n",
    "\n",
    "        if np.any(np.isnan(test_probs)):\n",
    "            test_probs = np.nan_to_num(test_probs)\n",
    "\n",
    "        auroc = roc_auc_score(test_labels, test_probs)\n",
    "        auprc = average_precision_score(test_labels, test_probs)\n",
    "        brier_pre = brier_score_loss(test_labels, test_probs)\n",
    "        ece_pre = compute_ece(test_probs, test_labels)\n",
    "\n",
    "        precision, recall, thresholds = precision_recall_curve(test_labels, test_probs)\n",
    "        f1_scores = 2.0 * precision * recall / (precision + recall + 1e-10)\n",
    "        best_f1 = float(np.max(f1_scores))\n",
    "\n",
    "        temperature = fit_temperature(val_logits, val_labels)\n",
    "        cal_probs = apply_temperature(test_logits, temperature)\n",
    "        cal_probs = np.clip(cal_probs, 1e-7, 1.0 - 1e-7)\n",
    "\n",
    "        brier_post = brier_score_loss(test_labels, cal_probs)\n",
    "        ece_post = compute_ece(cal_probs, test_labels)\n",
    "\n",
    "        eps_max = float(max(eps_spent)) if eps_spent else 0.0\n",
    "\n",
    "        return {\n",
    "            \"AUROC\": float(auroc),\n",
    "            \"AUPRC\": float(auprc),\n",
    "            \"ECE_pre\": float(ece_pre),\n",
    "            \"ECE_post\": float(ece_post),\n",
    "            \"Brier_pre\": float(brier_pre),\n",
    "            \"Brier_post\": float(brier_post),\n",
    "            \"F1\": best_f1,\n",
    "            \"Total_epsilon_spent\": eps_max,\n",
    "        }\n",
    "\n",
    "    def run(self) -> pd.DataFrame:\n",
    "        print(\"Starting DP-FL benchmark for thesis\")\n",
    "        print(f\"Algorithms: {conf.ALGORITHMS}\")\n",
    "        print(f\"Epsilons: {conf.EPSILONS}\")\n",
    "        print(f\"Seeds: {conf.SEEDS}\")\n",
    "        print(f\"Device: {conf.DEVICE}\")\n",
    "\n",
    "        for eps in conf.EPSILONS:\n",
    "            max_sr = max(\n",
    "                conf.BATCH_SIZE / len(d[\"X_train\"]) for d in self.raw_data.values()\n",
    "            )\n",
    "            noise = get_noise_multiplier(\n",
    "                target_epsilon=eps,\n",
    "                target_delta=conf.TARGET_DELTA,\n",
    "                sample_rate=max_sr,\n",
    "                epochs=conf.N_ROUNDS * conf.LOCAL_EPOCHS,\n",
    "            )\n",
    "\n",
    "            print(\"\\n\" + \"=\" * 60)\n",
    "            print(f\"Epsilon={eps} | noise_multiplier={noise:.4f}\")\n",
    "            print(\"=\" * 60)\n",
    "\n",
    "            for algo in conf.ALGORITHMS:\n",
    "                for seed in conf.SEEDS:\n",
    "                    set_seed(seed)\n",
    "                    print(f\"Running {algo} | seed={seed}...\")\n",
    "\n",
    "                    base_model = MortalityMLP(self.input_dim)\n",
    "                    base_model = ModuleValidator.fix(base_model)\n",
    "                    base_model.to(conf.DEVICE)\n",
    "                    global_state = base_model.state_dict()\n",
    "\n",
    "                    clients: Dict[str, Dict[str, Any]] = {\n",
    "                        cid: self._init_client(data, global_state, noise)\n",
    "                        for cid, data in self.raw_data.items()\n",
    "                    }\n",
    "\n",
    "                    for r in range(conf.N_ROUNDS):\n",
    "                        updates: List[Tuple[Dict[str, torch.Tensor], int]] = []\n",
    "\n",
    "                        for cid, client in clients.items():\n",
    "                            if algo == \"FedBN\":\n",
    "                                local_state = self._get_clean_state(client[\"model\"])\n",
    "                                mixed_state: Dict[str, torch.Tensor] = {}\n",
    "                                for k, v in global_state.items():\n",
    "                                    if \"norm\" in k or \"bn\" in k:\n",
    "                                        mixed_state[k] = local_state[k]\n",
    "                                    else:\n",
    "                                        mixed_state[k] = v\n",
    "                                self._load_safe(client[\"model\"], mixed_state, strict=True)\n",
    "                            else:\n",
    "                                self._load_safe(client[\"model\"], global_state, strict=True)\n",
    "\n",
    "                            self._train_round(client, global_state, algo)\n",
    "                            local_clean = self._get_clean_state(client[\"model\"])\n",
    "                            local_clean_cpu = {k: v.cpu().clone() for k, v in local_clean.items()}\n",
    "\n",
    "                            n_weight = len(self.raw_data[cid][\"y_train\"])\n",
    "                            updates.append((local_clean_cpu, n_weight))\n",
    "\n",
    "                        global_state = self._aggregate(updates, algo, global_state)\n",
    "\n",
    "                    for cid, client in clients.items():\n",
    "                        if algo != \"FedBN\":\n",
    "                            self._load_safe(client[\"model\"], global_state, strict=True)\n",
    "\n",
    "                        client[\"model\"].train()\n",
    "                        opt = client[\"opt\"]\n",
    "                        crit = nn.BCEWithLogitsLoss()\n",
    "\n",
    "                        if algo in [\"Ditto\", \"FedProx\"]:\n",
    "                            param_ref = self._build_param_ref(global_state)\n",
    "                        else:\n",
    "                            param_ref = None\n",
    "\n",
    "                        for _ in range(conf.FINE_TUNE_EPOCHS):\n",
    "                            for X, y in client[\"loader\"]:\n",
    "                                X = X.to(conf.DEVICE)\n",
    "                                y = y.to(conf.DEVICE)\n",
    "                                opt.zero_grad()\n",
    "\n",
    "                                logits = client[\"model\"](X).squeeze()\n",
    "                                loss = crit(logits, y)\n",
    "\n",
    "                                if algo in [\"Ditto\", \"FedProx\"]:\n",
    "                                    reg = 0.0\n",
    "                                    base_model = (\n",
    "                                        client[\"model\"]._module\n",
    "                                        if hasattr(client[\"model\"], \"_module\")\n",
    "                                        else client[\"model\"]\n",
    "                                    )\n",
    "                                    for name, param in base_model.named_parameters():\n",
    "                                        if name in param_ref:\n",
    "                                            reg = reg + (\n",
    "                                                param - param_ref[name]\n",
    "                                            ).pow(2).sum()\n",
    "                                    coef = (\n",
    "                                        conf.DITTO_LAMBDA\n",
    "                                        if algo == \"Ditto\"\n",
    "                                        else conf.FEDPROX_MU\n",
    "                                    )\n",
    "                                    loss = loss + (coef / 2.0) * reg\n",
    "\n",
    "                                loss.backward()\n",
    "                                opt.step()\n",
    "\n",
    "                    metrics = self._evaluate(clients)\n",
    "                    print(\n",
    "                        f\"  AUROC={metrics['AUROC']:.4f}, \"\n",
    "                        f\"AUPRC={metrics['AUPRC']:.4f}, \"\n",
    "                        f\"ECE_pre={metrics['ECE_pre']:.4f}, \"\n",
    "                        f\"ECE_post={metrics['ECE_post']:.4f}, \"\n",
    "                        f\"Eps_spent={metrics['Total_epsilon_spent']:.4f}\"\n",
    "                    )\n",
    "\n",
    "                    row = {\n",
    "                        \"Algorithm\": algo,\n",
    "                        \"Epsilon\": eps,\n",
    "                        \"Seed\": seed,\n",
    "                        **metrics,\n",
    "                    }\n",
    "                    self.results.append(row)\n",
    "\n",
    "                    df = pd.DataFrame(self.results)\n",
    "                    df.to_csv(conf.OUTPUT_FILE, index=False)\n",
    "\n",
    "        df_final = pd.DataFrame(self.results)\n",
    "        return df_final\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# MAIN\n",
    "# ============================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if \"client_data\" in globals():\n",
    "        manager = FederatedManager(client_data)\n",
    "        df_results = manager.run()\n",
    "        print(\"\\nSaved results to:\", conf.OUTPUT_FILE)\n",
    "    else:\n",
    "        print(\"client_data not found in globals(). Load it before running this script.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Configuration\n",
    "CSV_PATH = \"thesis_final_metrics_085_fixed.csv\"\n",
    "OUT_DIR = \"figures_dp_fl\"\n",
    "\n",
    "def load_results(csv_path: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(csv_path)\n",
    "    # Ensure types\n",
    "    df[\"Epsilon\"] = df[\"Epsilon\"].astype(float)\n",
    "    df[\"Seed\"] = df[\"Seed\"].astype(int)\n",
    "    return df\n",
    "\n",
    "def setup_style() -> None:\n",
    "    sns.set(style=\"whitegrid\", context=\"talk\")\n",
    "    plt.rcParams[\"figure.figsize\"] = (8, 5)\n",
    "    plt.rcParams[\"axes.titlesize\"] = 18\n",
    "    plt.rcParams[\"axes.labelsize\"] = 14\n",
    "    plt.rcParams[\"legend.fontsize\"] = 12\n",
    "    plt.rcParams[\"xtick.labelsize\"] = 12\n",
    "    plt.rcParams[\"ytick.labelsize\"] = 12\n",
    "\n",
    "def fig1_auroc_vs_epsilon(df: pd.DataFrame, out_dir: str) -> None:\n",
    "    methods = sorted(df[\"Algorithm\"].unique())\n",
    "    plt.figure()\n",
    "    for algo in methods:\n",
    "        sub = df[df[\"Algorithm\"] == algo]\n",
    "        agg = sub.groupby(\"Epsilon\")[\"AUROC\"].agg([\"mean\", \"std\"]).reset_index()\n",
    "        plt.errorbar(\n",
    "            agg[\"Epsilon\"],\n",
    "            agg[\"mean\"],\n",
    "            yerr=agg[\"std\"],\n",
    "            marker=\"o\",\n",
    "            linewidth=2,\n",
    "            capsize=4,\n",
    "            label=algo,\n",
    "        )\n",
    "    plt.xlabel(\"Privacy budget ()\")\n",
    "    plt.ylabel(\"AUROC\")\n",
    "    plt.title(\"Privacyutility tradeoff (AUROC vs )\")\n",
    "    plt.xticks(sorted(df[\"Epsilon\"].unique()))\n",
    "    plt.ylim(0.8, 0.9)\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(out_dir, \"fig1_auroc_vs_epsilon.png\"), dpi=300)\n",
    "    plt.savefig(os.path.join(out_dir, \"fig1_auroc_vs_epsilon.pdf\"))\n",
    "    plt.close()\n",
    "\n",
    "def fig2_auprc_vs_epsilon(df: pd.DataFrame, out_dir: str) -> None:\n",
    "    methods = sorted(df[\"Algorithm\"].unique())\n",
    "    plt.figure()\n",
    "    for algo in methods:\n",
    "        sub = df[df[\"Algorithm\"] == algo]\n",
    "        agg = sub.groupby(\"Epsilon\")[\"AUPRC\"].agg([\"mean\", \"std\"]).reset_index()\n",
    "        plt.errorbar(\n",
    "            agg[\"Epsilon\"],\n",
    "            agg[\"mean\"],\n",
    "            yerr=agg[\"std\"],\n",
    "            marker=\"o\",\n",
    "            linewidth=2,\n",
    "            capsize=4,\n",
    "            label=algo,\n",
    "        )\n",
    "    plt.xlabel(\"Privacy budget ()\")\n",
    "    plt.ylabel(\"AUPRC\")\n",
    "    plt.title(\"Precisionrecall performance (AUPRC vs )\")\n",
    "    plt.xticks(sorted(df[\"Epsilon\"].unique()))\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(out_dir, \"fig2_auprc_vs_epsilon.png\"), dpi=300)\n",
    "    plt.savefig(os.path.join(out_dir, \"fig2_auprc_vs_epsilon.pdf\"))\n",
    "    plt.close()\n",
    "\n",
    "def fig3_calibration_bars(df: pd.DataFrame, out_dir: str) -> None:\n",
    "    # Aggregate ECE over seeds for each algorithm and epsilon\n",
    "    agg = (\n",
    "        df.groupby([\"Algorithm\", \"Epsilon\"])\n",
    "        .agg({\"ECE_pre\": \"mean\", \"ECE_post\": \"mean\"})\n",
    "        .reset_index()\n",
    "    )\n",
    "    agg_melt = agg.melt(\n",
    "        id_vars=[\"Algorithm\", \"Epsilon\"],\n",
    "        value_vars=[\"ECE_pre\", \"ECE_post\"],\n",
    "        var_name=\"Phase\",\n",
    "        value_name=\"ECE\",\n",
    "    )\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(\n",
    "        data=agg_melt,\n",
    "        x=\"Epsilon\",\n",
    "        y=\"ECE\",\n",
    "        hue=\"Phase\",\n",
    "        ci=None,\n",
    "    )\n",
    "    plt.xlabel(\"Privacy budget ()\")\n",
    "    plt.ylabel(\"ECE\")\n",
    "    plt.title(\"Calibration error before and after temperature scaling\")\n",
    "    plt.legend(title=\"\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(out_dir, \"fig3_ece_pre_post.png\"), dpi=300)\n",
    "    plt.savefig(os.path.join(out_dir, \"fig3_ece_pre_post.pdf\"))\n",
    "    plt.close()\n",
    "\n",
    "def fig4_auroc_heatmap(df: pd.DataFrame, out_dir: str) -> None:\n",
    "    pivot = (\n",
    "        df.groupby([\"Algorithm\", \"Epsilon\"])[\"AUROC\"]\n",
    "        .mean()\n",
    "        .reset_index()\n",
    "        .pivot(index=\"Algorithm\", columns=\"Epsilon\", values=\"AUROC\")\n",
    "    )\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    sns.heatmap(\n",
    "        pivot,\n",
    "        annot=True,\n",
    "        fmt=\".3f\",\n",
    "        cmap=\"RdYlGn\",\n",
    "        vmin=0.80,\n",
    "        vmax=0.88,\n",
    "        cbar_kws={\"label\": \"AUROC\"},\n",
    "    )\n",
    "    plt.title(\"Mean AUROC by method and privacy budget\")\n",
    "    plt.xlabel(\"Privacy budget ()\")\n",
    "    plt.ylabel(\"Algorithm\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(out_dir, \"fig4_auroc_heatmap.png\"), dpi=300)\n",
    "    plt.savefig(os.path.join(out_dir, \"fig4_auroc_heatmap.pdf\"))\n",
    "    plt.close()\n",
    "\n",
    "def run_significance_tests(df: pd.DataFrame) -> None:\n",
    "    from scipy.stats import ttest_ind\n",
    "\n",
    "    methods = sorted(df[\"Algorithm\"].unique())\n",
    "    epsilons = sorted(df[\"Epsilon\"].unique())\n",
    "\n",
    "    print(\"Pairwise AUROC comparisons per  (Welch t-test, 3 seeds per method):\")\n",
    "    for eps in epsilons:\n",
    "        sub = df[df[\"Epsilon\"] == eps]\n",
    "        print(f\"\\nEpsilon = {eps}\")\n",
    "        for i in range(len(methods)):\n",
    "            for j in range(i + 1, len(methods)):\n",
    "                m1 = methods[i]\n",
    "                m2 = methods[j]\n",
    "                x = sub[sub[\"Algorithm\"] == m1][\"AUROC\"].values\n",
    "                y = sub[sub[\"Algorithm\"] == m2][\"AUROC\"].values\n",
    "                if len(x) >= 2 and len(y) >= 2:\n",
    "                    stat, p = ttest_ind(x, y, equal_var=False)\n",
    "                    print(f\"  {m1} vs {m2}: p = {p:.4f}\")\n",
    "\n",
    "    # Global test: ECE_pre vs ECE_post across all runs\n",
    "    from scipy.stats import ttest_rel\n",
    "    if \"ECE_pre\" in df.columns and \"ECE_post\" in df.columns:\n",
    "        stat, p = ttest_rel(df[\"ECE_pre\"].values, df[\"ECE_post\"].values)\n",
    "        print(\"\\nGlobal paired t-test for ECE_pre vs ECE_post (all runs combined):\")\n",
    "        print(f\"  p = {p:.4e}\")\n",
    "\n",
    "def main() -> None:\n",
    "    if not os.path.exists(CSV_PATH):\n",
    "        print(f\"CSV file not found at {CSV_PATH}\")\n",
    "        return\n",
    "\n",
    "    os.makedirs(OUT_DIR, exist_ok=True)\n",
    "    setup_style()\n",
    "    df = load_results(CSV_PATH)\n",
    "\n",
    "    fig1_auroc_vs_epsilon(df, OUT_DIR)\n",
    "    fig2_auprc_vs_epsilon(df, OUT_DIR)\n",
    "    fig3_calibration_bars(df, OUT_DIR)\n",
    "    fig4_auroc_heatmap(df, OUT_DIR)\n",
    "    run_significance_tests(df)\n",
    "\n",
    "    print(f\"\\nFigures saved in directory: {OUT_DIR}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 8383840,
     "sourceId": 13226665,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8886384,
     "sourceId": 13943108,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
